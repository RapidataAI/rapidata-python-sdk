# Leaderboards

## Overview

Leaderboards in Rapidata provide a powerful way to compare and rank different AI models based on their performance on specific tasks. They allow you to create standardized evaluation environments where multiple models can be tested against the same prompts and ranked based on user feedback.

## What are Leaderboards?

A leaderboard is a competitive evaluation system that:

- **Creates standardized testing environments** with specific instructions and prompts
- **Enables model comparison** by having different models generate content for the same prompts
- **Provides ranking systems** based on user evaluations and feedback
- **Offers great insights** through the Rapidata web interface at [app.rapidata.ai/leaderboards](https://app.rapidata.ai/leaderboards)

## How Leaderboards Work

### 1. Leaderboard Creation
You start by creating a leaderboard with specific settings:

- **Name**: Identifies your leaderboard in the overview
- **Instruction**: Determines how models will be evaluated
- **Prompts**: A set of registered prompts that will be used by all models for model evaluation
- **Show Prompt**: Whether to display the prompt to evaluators

### 2. Model Evaluation
Once your leaderboard is set up, you can evaluate models by:

- **Adding media**: Images, videos, or audio files generated by your model
- **Providing corresponding prompts**: Each media file must be paired with the exact prompt used to generate it
- **Using registered prompts only**: All prompts must be from the leaderboard's registered prompt set

### 3. Matchmaking and Ranking
The system creates fair comparisons by:

- **Prompt-based matching**: Only media generated from the same prompt are compared against each other
- **Mixed evaluation**: New models are integrated with existing models for comprehensive ranking
- **User-driven assessment**: Human evaluators compare model outputs based on the instruction to determine rankings

### 4. Results and Visibility
Your leaderboard results are:

- **Directly viewable** on the Rapidata website at [app.rapidata.ai/leaderboards](https://app.rapidata.ai/leaderboards)
- **Continuously updated** as new models are added and evaluated
- **Provides deeper insights** into model performances over time

## Getting Started

### Creating a Leaderboard

Use the `RapidataClient` to authenticate yourself and create a new leaderboard:

```python
from rapidata import RapidataClient

# Initialize the client
# Running this the first time will open a browser window and ask you to login
client = RapidataClient() 

# Create a new leaderboard
leaderboard = client.leaderboard.create_new_leaderboard(
    name="AI Art Competition",
    instruction="Which image do you prefer?",
    prompts=[
        "A serene mountain landscape at sunset",
        "A futuristic city with flying cars",
        "A portrait of a wise old wizard"
    ],
    show_prompt=False
)
```

### Retrieving Existing Leaderboards

You can retrieve leaderboards by ID or search for them:

```python
# Get a specific leaderboard by ID
leaderboard = client.leaderboard.get_leaderboard_by_id("leaderboard_id_here")

# Find leaderboards by name
recent_leaderboards = client.leaderboard.find_leaderboards(
    name="AI Art",
    amount=5
)
```

### Evaluating Models

Add your model's outputs to the leaderboard:

```python
# Evaluate a model
leaderboard.evaluate_model(
    name="MyAIModel_v2.1",
    media=[
        "path/to/mountain_sunset.jpg",
        "path/to/futuristic_city.jpg", 
        "path/to/wizard_portrait.jpg"
    ],
    prompts=[
        "A serene mountain landscape at sunset",
        "A futuristic city with flying cars",
        "A portrait of a wise old wizard"
    ]
)
```

## Related Classes
- [RapidataLeaderboardManager](/reference/rapidata/rapidata_client/leaderboard/rapidata_leaderboard_manager/)
- [RapidataLeaderboard](/reference/rapidata/rapidata_client/leaderboard/rapidata_leaderboard/)

