# coding: utf-8

"""
    Rapidata.Dataset

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from rapidata.api_client.models.compare_workflow_config_model_pair_maker_config import CompareWorkflowConfigModelPairMakerConfig
from rapidata.api_client.models.compare_workflow_model1_referee import CompareWorkflowModel1Referee
from typing import Optional, Set
from typing_extensions import Self

class CompareWorkflowConfigModel(BaseModel):
    """
    The configuration for creating a compare workflow.  A compare workflow is a workflow that continuously matches datapoints against each other and updates their  respective ELO scores. The ELO scores are used to determine the relative strength of the datapoints,  and datapoints are matched against other datapoints with similar ELO scores.  The end goal is a ranking of the datapoints based on their relative strength.
    """ # noqa: E501
    t: StrictStr = Field(description="Discriminator value for CompareWorkflowConfig", alias="_t")
    criteria: StrictStr = Field(description="The criteria to add to each compare rapid.")
    starting_elo: Optional[StrictInt] = Field(default=None, description="The starting ELO score for each datapoint.", alias="startingElo")
    k_factor: Optional[StrictInt] = Field(default=None, description="The K-factor to use in the ELO calculation.  Determines the maximum possible change in a player's Elo rating after a single match.  Higher K-Factor values result in larger rating changes.", alias="kFactor")
    scaling_factor: Optional[StrictInt] = Field(default=None, description="Scaling factor to use in the ELO calculation.  Adjusts the sensitivity of the Elo rating system to differences in player ratings.  It affects how much the rating changes based on the expected outcome versus the actual outcome.", alias="scalingFactor")
    pair_maker_config: CompareWorkflowConfigModelPairMakerConfig = Field(alias="pairMakerConfig")
    matches_until_completed: Optional[StrictInt] = Field(default=None, description="The number of matches to run before each datapoint is considered \"completed\".", alias="matchesUntilCompleted")
    referee: CompareWorkflowModel1Referee
    target_country_codes: List[StrictStr] = Field(description="A list of country codes that this workflow is targeting.", alias="targetCountryCodes")
    __properties: ClassVar[List[str]] = ["_t", "criteria", "startingElo", "kFactor", "scalingFactor", "pairMakerConfig", "matchesUntilCompleted", "referee", "targetCountryCodes"]

    @field_validator('t')
    def t_validate_enum(cls, value):
        """Validates the enum"""
        if value not in set(['CompareWorkflowConfig']):
            raise ValueError("must be one of enum values ('CompareWorkflowConfig')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CompareWorkflowConfigModel from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of pair_maker_config
        if self.pair_maker_config:
            _dict['pairMakerConfig'] = self.pair_maker_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of referee
        if self.referee:
            _dict['referee'] = self.referee.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CompareWorkflowConfigModel from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "_t": obj.get("_t") if obj.get("_t") is not None else 'CompareWorkflowConfig',
            "criteria": obj.get("criteria"),
            "startingElo": obj.get("startingElo"),
            "kFactor": obj.get("kFactor"),
            "scalingFactor": obj.get("scalingFactor"),
            "pairMakerConfig": CompareWorkflowConfigModelPairMakerConfig.from_dict(obj["pairMakerConfig"]) if obj.get("pairMakerConfig") is not None else None,
            "matchesUntilCompleted": obj.get("matchesUntilCompleted"),
            "referee": CompareWorkflowModel1Referee.from_dict(obj["referee"]) if obj.get("referee") is not None else None,
            "targetCountryCodes": obj.get("targetCountryCodes")
        })
        return _obj


