{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/","title":"Api","text":"<pre><code>classDiagram\n    class RapidataClient {\n        +RapidataOrderManager order\n        +RapidataValidationManager validation\n    }\n\n    class RapidataOrderManager {\n        +RapidataFilters filter\n        +RapidataSettings settings\n        +RapidataSelections selections\n        +create_****_order()\n        +get_order_by_id()\n        +find_orders()\n    }\n\n    class RapidataValidationManager {\n        +RapidsManager rapid\n        +create_****_set()\n        +get_validation_set_by_id()\n        +find_validation_sets()\n    }\n\n    class RapidataFilters {\n        +user_score\n        +age\n        +country\n        +gender\n        +language\n    }\n\n    class RapidataSettings {\n        +alert_on_fast_response\n        +translation_behaviour\n        +free_text_minimum_characters\n        +no_shuffle\n        +play_video_until_the_end\n    }\n\n    class RapidataSelections {\n        +demographic\n        +labeling\n        +validation\n        +conditionl_validation\n        +capped\n    }\n\n    class RapidsManager {\n        +****_rapid()\n    }\n\n    RapidataClient --* RapidataOrderManager\n    RapidataClient --* RapidataValidationManager\n    RapidataOrderManager --* RapidataFilters\n    RapidataOrderManager --* RapidataSettings\n    RapidataOrderManager --* RapidataSelections\n    RapidataValidationManager --* RapidsManager\n\n    link RapidataClient \"../reference/rapidata/rapidata_client/rapidata_client/\" \"\"\n    link RapidataOrderManager \"../reference/rapidata/rapidata_client/order/rapidata_order_manager/\" \"\"\n    link RapidataValidationManager \"../reference/rapidata/rapidata_client/validation/validation_set_manager/\" \"\"\n    link RapidataFilters \"../reference/rapidata/rapidata_client/filter/rapidata_filters/\" \"\"\n    link RapidataSettings \"../reference/rapidata/rapidata_client/settings/rapidata_settings/\" \"\"\n    link RapidataSelections \"../reference/rapidata/rapidata_client/selection/rapidata_selections/\" \"\"\n    link RapidsManager \"../reference/rapidata/rapidata_client/validation/rapids/rapids_manager/\" \"\"\n</code></pre>"},{"location":"api/#rapidata-api","title":"Rapidata API","text":"<p>The Rapidata API builds on the RapidataClient class. This class is the entry point for all operations. The RapidataClient class has two main properties, order and validation, which are used to manage orders and validation sets respectively.</p>"},{"location":"api/#order-related-classes","title":"Order related classes","text":"<p>RapidataOrderManger - accessible through the RapidataClient(rapi) under rapi.order</p> <p>RapidataFilters - accessible through the RapidataClient(rapi) under rapi.order</p> <p>RapidataSettings - accessible through the RapidataClient(rapi) under rapi.order</p> <p>RapidataSelections - accessible through the RapidataClient(rapi) under rapi.order</p>"},{"location":"api/#validation-related-classes","title":"Validation related classes","text":"<p>RapidataValidationManger - accessible through the RapidataClient(rapi) under rapi.validation</p> <p>RapidsManager - accessible through the RapidataClient(rapi) under rapi.validation.rapid. Used to create specific rapids to be added to a validation set.</p>"},{"location":"confidence_stopping/","title":"Early Stopping Based on Confidence","text":"<p>To improve the efficiency and cost-effectiveness of your data labeling tasks, Rapidata offers an Early Stopping feature based on confidence thresholds. This feature allows you to automatically stop collecting responses for a datapoint once a specified confidence level is reached, saving time and resources without compromising quality.</p>"},{"location":"confidence_stopping/#why-use-early-stopping","title":"Why Use Early Stopping?","text":"<p>In traditional data labeling workflows, you might request a fixed number of responses per datapoint to ensure accuracy. However, once a consensus is reached with high confidence, continuing to collect more responses becomes redundant and incurs unnecessary costs.</p> <p>Early Stopping addresses this by:</p> <ul> <li>Reducing Costs: Stop collecting responses when sufficient confidence is achieved.</li> <li>Improving Efficiency: Accelerate the labeling process by focusing resources where they are most needed.</li> <li>Maintaining Quality: Ensure that each datapoint meets your specified confidence level before stopping.</li> </ul>"},{"location":"confidence_stopping/#how-it-works","title":"How it Works","text":"<p>The Early Stopping feature leverages the trustworthiness, quantified through their <code>userScores</code>, to calculate the confidence level of each category for any given datapoint.</p>"},{"location":"confidence_stopping/#confidence-calculation","title":"Confidence Calculation","text":"<ul> <li>UserScores: Each annotator has a <code>userScore</code> between 0 and 1, representing their reliability. More information</li> <li>Aggregated Confidence: By combining the userScores of annotators who selected a particular category, the system computes the probability that this category is the correct one.</li> <li>Threshold Comparison: If the calculated confidence exceeds your specified threshold, the system stops collecting further responses for that datapoint.</li> </ul>"},{"location":"confidence_stopping/#understanding-the-confidence-threshold","title":"Understanding the Confidence Threshold","text":"<p>We've created a plot based on empirical data aided by simulations to give you an estimate of the number of responses required to reach a certain confidence level.</p> <p>There are a few things to keep in mind when interpreting the results:</p> <ul> <li>Unambiguous Scenario: The graph represents an ideal situation such as in the example below with no ambiguity which category is the correct one. A counter-example would be subjective tasks like \"Which image do you prefer?\", where there's no clear correct answer.</li> <li>Real-World Variability: Actual required responses may vary based on task complexity.</li> <li>Guidance Tool: Use the graph as a reference to set realistic expectations for your orders. </li> <li>Response Overflow: The number of responses per datapoint may exceed the specified amount due to multiple users answering simultaneously.</li> </ul> <p>Note: The Early Stopping feature is supported for the Classification and Comparison workflows. The number of categories is the number of options in the Classification task. For the Comparison task, the number of categories is always 2.</p>"},{"location":"confidence_stopping/#using-early-stopping-in-your-order","title":"Using Early Stopping in Your Order","text":"<p>Implementing Early Stopping is straightforward. You simply add the confidence threshold as a parameter when creating the order.</p>"},{"location":"confidence_stopping/#example-classification-order-with-early-stopping","title":"Example: Classification Order with Early Stopping","text":"<pre><code>order = rapi.order.create_classification_order(\n    name=\"Test Classification Order with Early Stopping\",\n    instruction=\"What do you see in the image?\",\n    answer_options=[\"Cat\", \"Dog\"],\n    datapoints=[\"https://assets.rapidata.ai/dog.jpeg\"],\n    responses_per_datapoint=50,\n    confidence_threshold=0.99,\n).run()\n\norder.display_progress_bar()\nresult = order.get_results()\nprint(result)\n</code></pre> <p>In this example:</p> <ul> <li>responses_per_datapoint=50: Sets the maximum number of responses per datapoint.</li> <li>confidence_threshold=0.99: Specifies that data collection for a datapoint should stop once a 99% confidence level is reached.</li> </ul> <p>We'd expect this to take roughtly 4 responses to reach the 99% confidence level.</p>"},{"location":"confidence_stopping/#when-to-use-early-stopping","title":"When to Use Early Stopping","text":"<p>We recommend using Early Stopping when:</p> <ul> <li>Cost Efficiency: You want to optimize costs by reducing the number of responses per datapoint.</li> <li>Clear Correct Answer: The task has a clear correct answer, and you're not interested in a distribution.</li> </ul>"},{"location":"confidence_stopping/#analyzing-early-stopping-results","title":"Analyzing Early Stopping Results","text":"<p>When using Early Stopping, the results will additionally include a <code>confidencePerCategory</code> field for each datapoint. This field shows the confidence level for each of the categories in the task.</p> <p>Example: <pre><code>{\n    \"info\": {\n        \"createdAt\": \"2099-12-30T00:00:00.000000+00:00\",\n        \"version\": \"3.0.0\"\n    },\n    \"results\": {\n        \"globalAggregatedData\": {\n            \"Dog\": 4,\n            \"Cat\": 0\n        },\n        \"data\": [\n            {\n                \"originalFileName\": \"dog.jpeg\",\n                \"aggregatedResults\": {\n                    \"Dog\": 4,\n                    \"Cat\": 0\n                },\n                \"aggregatedResultsRatios\": {\n                    \"Dog\": 1.0,\n                    \"Cat\": 0.0\n                },\n                \"summedUserScores\": {\n                    \"Dog\": 2.0865,\n                    \"Cat\": 0.0\n                },\n                \"summedUserScoresRatios\": {\n                    \"Dog\": 1.0,\n                    \"Cat\": 0.0\n                },\n                # this only appears when using early stopping\n                \"confidencePerCategory\": { \n                    \"Dog\": 0.9943,\n                    \"Cat\": 0.0057\n                },\n                \"detailedResults\": [\n                    {\n                        \"selectedCategory\": \"Dog\",\n                        \"userDetails\": {\n                            \"country\": \"PT\",\n                            \"language\": \"pt\",\n                            \"userScore\": 0.3\n                        }\n                    },\n                    {\n                        \"selectedCategory\": \"Dog\",\n                        \"userDetails\": {\n                            \"country\": \"RS\",\n                            \"language\": \"sr\",\n                            \"userScore\": 0.8486\n                        }\n                    },\n                    {\n                        \"selectedCategory\": \"Dog\",\n                        \"userDetails\": {\n                            \"country\": \"SG\",\n                            \"language\": \"en\",\n                            \"userScore\": 0.4469\n                        }\n                    },\n                    {\n                        \"selectedCategory\": \"Dog\",\n                        \"userDetails\": {\n                            \"country\": \"IN\",\n                            \"language\": \"en\",\n                            \"userScore\": 0.4911\n                        }\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"config/","title":"Configuration and Logging","text":"<p>The Rapidata SDK provides a centralized configuration system through the global <code>rapidata_config</code> object that controls all aspects of the SDK's behavior including logging, output management, upload settings, and data sharing.</p>"},{"location":"config/#rapidata-configuration-system","title":"Rapidata Configuration System","text":"<p>All configuration is managed through the global <code>rapidata_config</code> object, which provides a unified way to configure:</p> <ol> <li>Logging Configuration: Log levels, file output, formatting, silent mode and OpenTelemetry integration</li> <li>Upload Configuration: Worker threads and retry settings</li> </ol>"},{"location":"config/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidata import rapidata_config, logger\n\nlogger.info(\"This will not be shown\")\nrapidata_config.logging.level = \"INFO\"\nlogger.info(\"This will be shown\")\n</code></pre> <p>Note: The logging system is now fully managed through <code>rapidata_config.logging</code>. Changes to the configuration are automatically applied to the logger in real-time.</p>"},{"location":"config/#logging-configuration-options","title":"Logging Configuration Options","text":"Parameter Type Default Description <code>level</code> <code>str</code> <code>\"WARNING\"</code> Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) <code>log_file</code> <code>Optional[str]</code> <code>None</code> Optional file path for log output <code>format</code> <code>str</code> <code>\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"</code> Log message format <code>silent_mode</code> <code>bool</code> <code>False</code> Suppress prints and progress bars (doesn't affect logging) <code>enable_otlp</code> <code>bool</code> <code>True</code> Enable OpenTelemetry trace logs to Rapidata <p>Note: Rapidata SDK tracking is limited exclusively to SDK-generated logs and traces. No other data is collected.</p>"},{"location":"human_prompting/","title":"Effective Instruction Design for Rapidata Tasks","text":"<p>When creating tasks for human annotators using the Rapidata API, phrasing your instructions well can significantly improve quality and consistency of the responses you receive. This guide provides best practices for designing effective instructions for your Rapidata tasks.</p>"},{"location":"human_prompting/#time-constraints","title":"Time Constraints","text":"<p>Each annotator session (specified in the selections) has a limited time window of 25 seconds to complete all tasks. With this in mind:</p> <ul> <li>Be concise: Keep instructions as brief as possible while maintaining clarity</li> <li>Use simple language: Avoid complex terminology or jargon</li> <li>Focus on the essentials: Include only what is needed to complete the task</li> </ul>"},{"location":"human_prompting/#language-clarity","title":"Language Clarity","text":"<p>Since Rapidata tasks are presented to a diverse audience of annotators:</p> <ul> <li>Use accessible language: The average person should be able to understand your instructions clearly</li> <li>Avoid ambiguity: Ensure there's only one way to interpret your instructions</li> <li>Be specific: Clearly state what you're looking for in the responses</li> </ul>"},{"location":"human_prompting/#question-framing","title":"Question Framing","text":"<p>The way you frame questions significantly impacts response quality:</p>"},{"location":"human_prompting/#use-positive-framing","title":"Use Positive Framing","text":"<p>Frame questions in the positive rather than negative. Positive questions are easier to process quickly.</p> <p>Better: <pre><code>\"Which image looks more realistic?\"\n</code></pre></p> <p>Avoid: <pre><code>\"Which image looks less AI-generated?\"\n</code></pre></p>"},{"location":"human_prompting/#limit-decision-criteria","title":"Limit Decision Criteria","text":"<p>Don't overload annotators with multiple criteria in a single question.</p> <p>Better: <pre><code>\"What animal is in the image? - rabbit/dog/cat/other\"\n</code></pre></p> <p>Avoid: <pre><code>\"Does this image contain a rabbit, a dog, or a cat? - yes/no\"\n</code></pre></p>"},{"location":"human_prompting/#use-clear-response-options","title":"Use Clear Response Options","text":"<p>Provide distinct, non-overlapping response options.</p> <p>Better: <pre><code>\"Rate the image quality: poor/acceptable/excellent\"\n</code></pre></p> <p>Avoid: <pre><code>\"Rate the image quality: bad/not good/fine/good/great\"\n</code></pre></p>"},{"location":"human_prompting/#task-sequence-considerations","title":"Task Sequence Considerations","text":"<p>When using multiple selections in a session, consider the cognitive load and time constraints:</p> <pre><code>from rapidata import LabelingSelection, ValidationSelection\n\n# Keep the total number of tasks manageable for the 25-second window\nselections=[\n    ValidationSelection(\"67cafc95bc71604b08d8aa62\", 1),  # Start with one validation task (id is the validation set id)\n    LabelingSelection(2)  # Follow with one labeling task\n]\n</code></pre>"},{"location":"human_prompting/#example-implementation","title":"Example Implementation","text":"<p>When creating a Rapidata order, implement these principles as follows:</p> <pre><code>order = rapi.order.create_compare_order(\n    name=\"Image Coherence Comparison\",\n    instruction=\"Which images has more glitches and is more likely to be AI generated?\",  # Clear, positive framing\n    datapoints=[[\"https://assets.rapidata.ai/flux-1.1-pro/33_2.jpg\", \n    \"https://assets.rapidata.ai/stable-diffusion-3/33_0.jpg\"]],\n    selections=selections  # Include the above-defined selections\n)\n</code></pre>"},{"location":"human_prompting/#common-task-types-and-recommended-instructions","title":"Common Task Types and Recommended Instructions","text":""},{"location":"human_prompting/#image-comparison-tasks","title":"Image Comparison Tasks","text":"<pre><code># Comparing image preference\ninstruction=\"Which image do you prefer?\"\n\n# Comparing prompt adherence\ninstruction=\"Which image matches the description better?\"\n\n# Comparing image coherence\ninstruction=\"Which images has more glitches and is more likely to be AI generated?\"\n\n# Comparing two texts\ninstruction=\"Which of these sentences makes more sense?\"\n</code></pre>"},{"location":"human_prompting/#classification-tasks","title":"Classification Tasks","text":"<pre><code># Simple classification\ninstruction=\"What object is in the image?\"\n\n# Likert classification (add no shuffling setting)\ninstruction=\"How well does the video match the description?\nanswer_options=[\"1: Perfectly\", \n                \"2: Very well\", \n                \"3: Moderately\", \n                \"4: A little\", \n                \"5: Not at all\"]\n</code></pre>"},{"location":"human_prompting/#monitoring-and-iteration","title":"Monitoring and Iteration","text":"<p>After launching your order, monitor the initial responses to see if annotators are understanding your instructions as intended.</p> <p>You can see how the users will be presented with the task by calling the <code>.preview()</code> method on the order object: <pre><code>order.preview()\n</code></pre></p> <p>If you see that annotators are giving inconsistent or incorrect answers:</p> <ol> <li>Pause your order</li> <li>Review and simplify your instructions </li> <li>Update your selections if needed</li> <li>Start a new order with the improved settings</li> </ol> <p>This helps ensure you get high quality results from annotators.</p> <p>For more information on creating and managing orders, refer to the Rapidata API documentation and Understanding the Results guide.</p>"},{"location":"improve_order_quality/","title":"Improve Response Quality","text":"<p>This guide builds on the Quickstart and focuses on improving the quality of responses received for your orders. By creating a Validation Set, you can provide clear guidance to labelers, helping them better understand your expectations. While using a validation set is optional, it significantly enhances response accuracy and consistency, especially for more complex or unintuitive tasks.</p>"},{"location":"improve_order_quality/#why-use-a-validation-set","title":"Why Use a Validation Set?","text":"<p>A validation set is a collection of tasks with known answers. It provides labelers with examples of how you want your data annotated, offering immediate feedback to align their work with your requirements.</p>"},{"location":"improve_order_quality/#how-does-it-work","title":"How Does it Work?","text":"<p>The validation task has a known truth and will be shown in front of the actual labeling task. Think of it as an interview question before the work starts. The labelers are only allowed to proceed to the actual task if they correctly solve one validation task.</p>"},{"location":"improve_order_quality/#example-validation-set","title":"Example Validation Set","text":"<p>Let\u2019s walk through how to create and use a validation set using the Compare Order as an example. The principles apply to any order type and the created validation set can be reused indefinitely.</p> <p>Here\u2019s what labelers will see during validation for this example:</p> <p></p>"},{"location":"improve_order_quality/#creating-a-validation-set","title":"Creating a Validation Set","text":"<p>As always, we start by creating a <code>RapidataClient</code>:</p> <pre><code>from rapidata import RapidataClient\n\nrapi = RapidataClient()\n</code></pre> <p>All the validation-related operations are performed using <code>rapi.validation</code>. Here we create a compare validation set to be used in any future orders:</p> <p>The creation is structured in the same way as the order creation, except here we have to supply the correct answers as \"truth\" for each task.</p> <pre><code>validation_set = rapi.validation.create_compare_set(\n     name=\"Example Compare Validation Set\",\n     instruction=\"Which of the AI generated images looks more realistic?\",\n     datapoints=[[\"https://assets.rapidata.ai/bad_ai_generated_image.png\", \n         \"https://assets.rapidata.ai/good_ai_generated_image.png\"]], \n     truths=[\"https://assets.rapidata.ai/good_ai_generated_image.png\"] \n)\n</code></pre> <p>The parameters are as follows:</p> <ul> <li><code>name</code>: The name of the validation set. This is used to identify the validation set and to find it again later.</li> <li><code>criteria</code>: The criteria for the comparison. This is the question that the labeler will answer.</li> <li><code>datapoints</code>: The datapoints, each containing 2 images to compare.</li> <li><code>truths</code>: The truth, which image is the correct answer.</li> </ul> <p>The truths must be the same length as the datapoints and contain the correct answer for each datapoint.</p>"},{"location":"improve_order_quality/#usage","title":"Usage","text":"<ol> <li>We can now use the validation set in any order we create. We first need to find the validation set we created:</li> </ol> <pre><code># find the validation set by name\nvalidation_set = rapi.validation.find_validation_sets(\"Example Compare Validation Set\")[0] \n\n# or by id\nvalidation_set = rapi.validation.get_validation_set_by_id(\"validation_set_id\")\n</code></pre> <ol> <li>Now we can create a new order and add the validation set to it. It will automatically be shown in front of the datapoints we want to label. Ideally the criteria is the same or very closely related to the one in the validation set:</li> </ol> <pre><code>order = rapi.order.create_compare_order(\n     name=\"Example Compare Validation Set\",\n     instruction=\"Which of the AI generated images looks more realistic?\", \n     datapoints=[[\"https://assets.rapidata.ai/dalle-3_human.jpg\", \n        \"https://assets.rapidata.ai/flux_human.jpg\"]],\n     validation_set_id=validation_set.id\n).run()\n</code></pre> <p>If the labeler answers the validation task incorrectly, they will get warned and have to answer it correctly before they can proceed to the actual task.</p> <ol> <li>Finally we wait for the order to complete and look at the results:</li> </ol> <pre><code>order.display_progress_bar()\nresults = order.get_results()\n</code></pre> <p>The validation results will not be included in the final results, you'll only see the results of the actual tasks you wanted to have labeled. Likewise you'll only be charged credits for the actual tasks and not the validation tasks.</p>"},{"location":"improve_order_quality/#next-steps","title":"Next Steps","text":"<p>You now know how to create and use a validation set to improve the quality of your orders. Check out our example for the benchmark described in this paper where we collected over 2 million responses! You may also have a look at the freely available Huggingface dataset. </p> <p>Happy labeling!</p>"},{"location":"mri/","title":"Model Ranking Insights","text":""},{"location":"mri/#overview","title":"Overview","text":"<p>Model Ranking Insights (MRI) provides a powerful way to compare and rank different AI models based on their performance on specific tasks. They allow you to create standardized evaluation environments where multiple models can be tested against each other and ranked based on human feedback.</p> <p></p>"},{"location":"mri/#how-to-use-mri","title":"How to use MRI","text":""},{"location":"mri/#1-benchmark-creation","title":"1. Benchmark Creation","text":"<p>You start by creating a benchmark with specific settings:</p> <ul> <li>Name: Identifies your benchmark in the overview</li> <li>Prompts: A list of prompts that will be used to generate the media to evaluate the models.</li> </ul> <p>Use the <code>RapidataClient</code> to authenticate yourself and create a new leaderboard:</p> <pre><code>from rapidata import RapidataClient\n\n# Initialize the client\n# Running this the first time will open a browser window and ask you to login\nclient = RapidataClient() \n\n# Create a new benchmark\nbenchmark = client.mri.create_new_benchmark(\n    name=\"AI Art Competition\",\n    prompts=[\n        \"A serene mountain landscape at sunset\",\n        \"A futuristic city with flying cars\",\n        \"A portrait of a wise old wizard\"\n    ]\n)\n</code></pre>"},{"location":"mri/#2-leaderboard-creation","title":"2. Leaderboard Creation","text":"<p>Once your benchmark is set up, you can create leaderboards for it.</p> <ul> <li>Name: Identifies your leaderboard in the overview</li> <li>Instruction: The criteria upon which labelers choose the better model</li> <li>Show Prompt: Whether to display the prompt to evaluators. Including this option adds complexity and cost, so it is advised to only include it in settings where the prompt is necessary for the labelers to follow the instruction (e.g., prompt alignment).</li> </ul> <p>Note: You can find all leaderboards for a benchmark by using the <code>leaderboards</code> attribute of the benchmark.</p> <pre><code># Create a new leaderboard on a benchmark\nleaderboard = benchmark.create_leaderboard(\n    name=\"Realism\", \n    instruction=\"Which image is more realistic?\", \n    show_prompt=False\n)\n</code></pre>"},{"location":"mri/#3-model-evaluation","title":"3. Model Evaluation","text":"<p>Once your benchmark and leaderboard are set up, you can evaluate models by the following:</p> <ul> <li>Media: Images, videos, or audio files generated by your model</li> <li>Prompts: Each media file must be paired with a prompt</li> </ul> <p>All prompts must be from the benchmark's registered prompt set (available through the <code>prompts</code> attribute of the benchmark)</p> <p>Note: You are not limited to one media per prompt; you can supply the same prompt multiple times.</p> <pre><code># Evaluate a model\nbenchmark.evaluate_model(\n    name=\"MyAIModel_v2.1\",\n    media=[\n        \"https://assets.rapidata.ai/mountain_sunset1.png\",\n        \"https://assets.rapidata.ai/mountain_sunset2.png\",\n        \"https://assets.rapidata.ai/futuristic_city.png\", \n        \"https://assets.rapidata.ai/wizard_portrait.png\"\n    ],\n    prompts=[\n        \"A serene mountain landscape at sunset\",\n        \"A serene mountain landscape at sunset\",\n        \"A futuristic city with flying cars\",\n        \"A portrait of a wise old wizard\"\n    ]\n)\n</code></pre>"},{"location":"mri/#4-matchmaking-and-ranking","title":"4. Matchmaking and Ranking","text":"<p>MRI creates fair comparisons by:</p> <ul> <li>Prompt-based matching: Only media with the same prompt are compared against each other</li> <li>Mixed evaluation: New models are matched up with existing models to maximize the information gained</li> <li>User-driven assessment: Human evaluators compare model outputs based on the instruction to determine rankings</li> </ul>"},{"location":"mri/#5-results-and-visibility","title":"5. Results and Visibility","text":"<p>Your leaderboard results are:</p> <ul> <li>Directly viewable on the Rapidata dashboard at app.rapidata.ai/mri/benchmarks</li> <li>Continuously updated as new models are added and evaluated</li> <li>Provides deeper insights into model performances over time</li> </ul>"},{"location":"mri/#retrieving-existing-benchmarks","title":"Retrieving Existing Benchmarks","text":"<p>You can retrieve benchmarks by ID or search for them:</p> <pre><code># Get a specific benchmark by ID\nbenchmark = client.mri.get_benchmark_by_id(\"benchmark_id_here\")\n\n# Find benchmarks by name\nrecent_benchmarks = client.mri.find_benchmarks(\n    name=\"AI Art\",\n    amount=10\n)\n</code></pre>"},{"location":"mri/#retrieving-results","title":"Retrieving Results","text":"<pre><code># Get the leaderboard\nleaderboard = benchmark.leaderboards[0]\n\n# Get the standings\nstandings = leaderboard.get_standings() # Returns a pandas dataframe\n</code></pre>"},{"location":"mri_advanced/","title":"Model Ranking Insights Advanced","text":""},{"location":"mri_advanced/#overview","title":"Overview","text":"<p>To unlock the full potential of Model Ranking Insights (MRI), you can use the advanced features. These include sophisticated configuration options for benchmarks, leaderboards, and evaluation settings that give you fine-grained control over your model evaluation process.</p>"},{"location":"mri_advanced/#benchmark-configuration","title":"Benchmark Configuration","text":""},{"location":"mri_advanced/#using-identifiers","title":"Using Identifiers","text":"<p>In the MRI quickstart we used the prompts to identify the media and create the appropriate matchups. However, more generally you might not have an exact 1-to-1 relationship between prompts and media (e.g., you may have different settings or inputs for the same prompt - for example input images for image-to-video models. More about this below). To handle this case, we allow you to supply your own identifiers, which will then be used when creating the matchups.</p> <pre><code># Example 1: Explicit identifiers\nbenchmark = benchmark_manager.create_new_benchmark(\n    name=\"Preference Benchmark\",\n    identifiers=[\"scene_1\", \"scene_2\", \"scene_3\"],\n    prompts=[\n        \"A serene mountain landscape at sunset\",\n        \"A futuristic city with flying cars\",\n        \"A portrait of a wise old wizard\"\n    ],\n    prompt_assets=[\n        \"https://assets.rapidata.ai/mountain_sunset.png\",\n        \"https://assets.rapidata.ai/futuristic_city.png\", \n        \"https://assets.rapidata.ai/wizard_portrait.png\"\n    ]\n)\n\n# Example 2: Identifiers used for the same prompts but different seeding\nbenchmark = benchmark_manager.create_new_benchmark(\n    name=\"Preference Benchmark\",\n    identifiers=[\"seed_1\", \"seed_2\", \"seed_3\"],\n    prompts=[\"prompt_1\", \"prompt_1\", \"prompt_1\"],\n    prompt_assets=[\"https://example.com/asset1.jpg\", \"https://example.com/asset1.jpg\", \"https://example.com/asset1.jpg\"]\n)\n\n# Example 3: Using only prompt assets\nbenchmark = benchmark_manager.create_new_benchmark(\n    name=\"Preference Benchmark\",\n    identifiers=[\"image_1\", \"image_2\", \"image_3\"],   \n    prompt_assets=[\"https://example.com/asset1.jpg\", \"https://example.com/asset2.jpg\", \"https://example.com/asset3.jpg\"]\n)\n</code></pre> <p>Note: Media assets are images, videos, or audio files that provide visual or auditory context for your evaluation prompts. For example when evaluating image to video models.</p>"},{"location":"mri_advanced/#tagging-system","title":"Tagging System","text":"<p>Tags provide metadata for filtering and organizing benchmark results without showing them to evaluators. These tags can also be set and used in the frontend. To view the frontend, you can use the <code>view</code> method of the benchmark or leaderboard.</p> <pre><code># Tags for filtering leaderboard results\ntags = [\n    [\"landscape\", \"outdoor\", \"beach\"],\n    [\"landscape\", \"outdoor\", \"mountain\"],\n    [\"outdoor\", \"city\"],\n    [\"indoor\", \"vehicle\"]\n]\n\nbenchmark = benchmark_manager.create_new_benchmark(\n    name=\"Tagged Benchmark\",\n    identifiers=[\"scene_1\", \"scene_2\", \"scene_3\", \"scene_4\"],\n    prompts=[\"A sunny beach\", \"A mountain landscape\", \"A city skyline\", \"A car in a garage\"],\n    tags=tags\n)\n\n# Filter leaderboard results by tags\nstandings = leaderboard.get_standings(tags=[\"landscape\", \"outdoor\"])\n</code></pre>"},{"location":"mri_advanced/#adding-prompts-and-assets-after-benchmark-creation","title":"Adding prompts and assets after benchmark creation","text":"<p>If you have already created a benchmark and want to add new prompts and assets after the fact. Note however that these will only take effect for new models.</p> <pre><code># Adding individual prompts with assets\nbenchmark.add_prompt(\n    identifier=\"new_style\",\n    prompt=\"Generate artwork in this new style\",\n    prompt_asset=\"https://assets.rapidata.ai/new_style_ref.jpg\",\n    tags=[\"abstract\", \"modern\"]\n)\n</code></pre>"},{"location":"mri_advanced/#leaderboard-configuration","title":"Leaderboard Configuration","text":""},{"location":"mri_advanced/#inverse-ranking","title":"Inverse Ranking","text":"<p>For evaluation questions where lower scores are better (e.g., \"Which image is worse?\"), use inverse ranking.</p> <pre><code>leaderboard = benchmark.create_leaderboard(\n    name=\"Quality Assessment\",\n    instruction=\"Which image has lower quality?\",\n    inverse_ranking=True,  # Lower scores = better performance\n    show_prompt=True,\n    show_prompt_asset=True\n)\n</code></pre>"},{"location":"mri_advanced/#level-of-detail","title":"Level of Detail","text":"<p>Controls the number of comparisons performed, affecting accuracy vs. speed.</p> <pre><code># Different detail levels\nleaderboard_fast = benchmark.create_leaderboard(\n    name=\"Quick Evaluation\", \n    instruction=\"Which image do you prefer?\",\n    level_of_detail=\"low\"      # Fewer comparisons, faster results\n)\n\nleaderboard_precise = benchmark.create_leaderboard(\n    name=\"Precise Evaluation\",\n    instruction=\"Which image do you prefer?\", \n    level_of_detail=\"very high\"  # More comparisons, higher accuracy\n)\n</code></pre>"},{"location":"mri_advanced/#prompt-and-asset-display","title":"Prompt and Asset Display","text":"<p>Control what evaluators see during comparison.</p> <pre><code>leaderboard = benchmark.create_leaderboard(\n    name=\"Context-Aware Evaluation\",\n    instruction=\"Which generated image better matches the prompt?\",\n    show_prompt=True,           # Show the original text prompt\n    show_prompt_asset=True,     # Show reference images/videos\n    level_of_detail=\"medium\"\n)\n</code></pre>"},{"location":"mri_advanced/#references","title":"References","text":"<ul> <li>RapidataBenchmarkManager</li> <li>RapidataBenchmark</li> <li>RapidataLeaderboard</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>Directly ask real humans to compare your data. This guide will show you how to create a compare order using the Rapidata API.</p> <p>There are many other types of orders you can create which you can find in the examples on the Overview.</p> <p>We will create an order assessing image-prompt-alignment, using 2 AI generated images and compare them against each other based on which image followed the prompt more accurately.</p> <p>Our annotators will then label the data according to the instruction we provided.</p> <p>They see the following screen:</p> <p></p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install Rapidata using pip:</p> <pre><code>pip install -U rapidata\n</code></pre>"},{"location":"quickstart/#usage","title":"Usage","text":"<p>Orders are managed through the <code>RapidataClient</code>.</p> <p>Create a client as follows, this will save your credentials in your <code>~/.config/rapidata/credentials.json</code> file so you don't have to log in again on that machine:</p> <pre><code>from rapidata import RapidataClient\n\n#The first time executing it on a machine will require you to log in\nrapi = RapidataClient()\n</code></pre> <p>Alternatively you can generate a Client ID and Secret in the Rapidata Settings and pass them to the <code>RapidataClient</code> constructor:</p> <pre><code>from rapidata import RapidataClient\nrapi = RapidataClient(client_id=\"Your client ID\", client_secret=\"Your client secret\")\n</code></pre>"},{"location":"quickstart/#creating-an-order","title":"Creating an Order","text":"<p>All order-related operations are performed using rapi.order.</p> <p>Here we create a compare order with a name and the instruction / question we want to ask. Additionally, we provide the prompt as context:</p> <pre><code>order = rapi.order.create_compare_order(\n    name=\"Example Alignment Order\",\n    instruction=\"Which image matches the description better?\",\n    contexts=[\"A small blue book sitting on a large red book.\"],\n    datapoints=[[\"https://assets.rapidata.ai/midjourney-5.2_37_3.jpg\", \n                \"https://assets.rapidata.ai/flux-1-pro_37_0.jpg\"]],\n)\n</code></pre> <p>Note: When calling this function the data gets uploaded and prepared, but no annotators will start working on it yet.</p> <p>The parameters are as follows:</p> <ul> <li><code>name</code>: The name of the order. This is used to identify the order in the Rapidata Dashboard. This name is also be used to find the order again later.</li> <li><code>instruction</code>: The instruction you want to show the annotators to select the image by.</li> <li><code>contexts</code>: The prompt that will be shown along side the two images and the instruction. (optional parameter)</li> <li><code>datapoints</code>: The image pairs we want to compare (order is randomized for every annotator). This can be any public URL (that points to an image, video or audio) or a local file path. This is a list of all datapoints you want to compare. Each datapoint consists of 2 files that are compared, as well as an optional context (which in this case is the prompt). The same instruction will be shown for each datapoint. There is a limit of 100 datapoints per order. If you need more than that, you can reach out to us at info@rapidata.ai.</li> </ul> <p>Optionally you may add additional specifications with the other parameters. As an example, the <code>responses_per_datapoint</code> that specifies how many responses you want per datapoint<sup>1</sup>.</p> <p>Further more you can customize to whom, how and in what sequence the tasks are shown:</p> <ul> <li>Filters to specify who should work on the order</li> <li>Settings to specify how the order should be shown</li> </ul> <p>These customizations can be added to the order through the <code>filters</code> and <code>settings</code> parameters respectively.</p>"},{"location":"quickstart/#preview-the-order","title":"Preview the Order","text":"<p>You can see how the users will be presented with the task by calling the <code>.preview()</code> method on the order object to make sure everything looks as expected:</p> <pre><code>order.preview()\n</code></pre>"},{"location":"quickstart/#start-collecting-responses","title":"Start Collecting Responses","text":"<p>To start the order and collect responses, call the <code>run</code> method:</p> <pre><code>order.run()\n</code></pre> <p>Once you call this method, annotators will start working on your order immediately.</p>"},{"location":"quickstart/#retrieve-orders","title":"Retrieve Orders","text":"<p>To retrieve old orders, you can use the <code>find_orders</code> method. This method allows you to filter by name and amount of orders to retrieve:</p> <pre><code>example_orders = rapi.order.find_orders(\"Example Alignment Order\")\n\n# if no name is provided it will just return the most recent one\nmost_recent_order = rapi.order.find_orders()[0]\n</code></pre> <p>Optionally you can also retrieve a specific order using the order ID:</p> <pre><code>order = rapi.order.get_order_by_id(\"order_id\")\n</code></pre>"},{"location":"quickstart/#monitoring-order-progress","title":"Monitoring Order Progress","text":"<p>You can monitor the progress of the order on the Rapidata Dashboard or by checking how many datapoints are already done with labeling:</p> <pre><code>order.display_progress_bar()\n</code></pre>"},{"location":"quickstart/#downloading-results","title":"Downloading Results","text":"<p>To download the results simply call the <code>get_results</code> method on the order:</p> <pre><code>results = order.get_results()\n</code></pre> <p>To better understand the results you can check out the Understanding the Results guide.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>This is just the beginning. You can create many different types of orders and customize them to your needs. Check out the Overview for more examples and information or check out how to improve the quality of your responses in the Improve Quality .</p> <p><sup>1</sup> Due to the possibility of multiple people answering at the same time, this number is treated as a minimum. The actual number of responses may be higher. The overshoot per datapoint will be lower the more datapoints are added.</p>"},{"location":"starting_page/","title":"\ud83d\udc40 Overview","text":"Rapidata developer documentation"},{"location":"starting_page/#developer-quickstart","title":"Developer Quickstart","text":"<p>Set up your environment and make your first API request in minutes.</p> <ul> <li> <p>Developer Quickstart</p> <p>Compare two items and determine which one better matches your criteria.</p> <p>Estimated time: 5 minutes</p> <pre><code>pip install -U rapidata\n</code></pre> ImageVideoAudioText <pre><code>from rapidata import RapidataClient\n\nrapi = RapidataClient()\n\norder = rapi.order.create_compare_order(\n    name=\"Example Image Comparison\",\n    instruction=\"Which image matches the description better?\",\n    contexts=[\"A small blue book sitting on a large red book.\"],\n    datapoints=[[\"https://assets.rapidata.ai/midjourney-5.2_37_3.jpg\", \n                \"https://assets.rapidata.ai/flux-1-pro_37_0.jpg\"]],\n).run()\n\norder.display_progress_bar()\n\nresults = order.get_results()\nprint(results)\n</code></pre> <pre><code>from rapidata import RapidataClient\n\nrapi = RapidataClient()\n\norder = rapi.order.create_compare_order(\n    name=\"Example Video Comparison\",\n    instruction=\"Which video fits the description better?\",\n    contexts=[\"A group of elephants painting vibrant murals on a city wall during a lively street festival.\"],\n    datapoints=[[\"https://assets.rapidata.ai/0074_sora_1.mp4\", \n                \"https://assets.rapidata.ai/0074_hunyuan_1724.mp4\"]],\n).run()\n\norder.display_progress_bar()\n\nresults = order.get_results()\nprint(results)\n</code></pre> <pre><code>from rapidata import RapidataClient, LanguageFilter\n\nrapi = RapidataClient()\n\norder = rapi.order.create_compare_order(\n    name=\"Example Audio Comparison\",\n    instruction=\"Which audio clip sounds more natural?\",\n    datapoints=[[\"https://assets.rapidata.ai/Chat_gpt.mp3\", \n                \"https://assets.rapidata.ai/ElevenLabs.mp3\"]],\n    filters=[LanguageFilter([\"en\"])]\n).run()\n\norder.display_progress_bar()\n\nresults = order.get_results()\nprint(results)\n</code></pre> <pre><code>from rapidata import RapidataClient, LanguageFilter\n\nrapi = RapidataClient()\n\norder = rapi.order.create_compare_order(\n    name=\"Example Text Comparison\",\n    instruction=\"Which sentence is grammatically more correct?\",\n    datapoints=[[\"The children were amazed by the magician\u2019s tricks\", \n                \"The children were amusing by the magician\u2019s tricks.\"]],\n    data_type=\"text\",\n    filters=[LanguageFilter([\"en\"])]\n).run()\n\norder.display_progress_bar()\n\nresults = order.get_results()\nprint(results)\n</code></pre> <p> Let's go</p> </li> </ul>"},{"location":"understanding_the_results/","title":"Interpreting the Results","text":"<p>After running your comparison order and collecting responses, you'll receive a structured result containing valuable insights from the annotators. Understanding each component of this result is crucial for analyzing and utilizing the data effectively.</p> <p>Here's an example of the results you might receive when running a COMPARE task (for simplicity, this example uses 3 responses):</p> <pre><code>{\n  \"info\": {\n    \"createdAt\": \"2025-02-11T07:31:59.353232+00:00\",\n    \"version\": \"3.0.0\"\n  },\n  \"summary\": {\n    \"A_wins_total\": 0,\n    \"B_wins_total\": 1\n  },\n  \"results\": [\n    {\n      \"context\": \"A small blue book sitting on a large red book.\",\n      \"winner_index\": 1,\n      \"winner\": \"dalle-3_37_2.jpg\",\n      \"aggregatedResults\": {\n        \"aurora-20-1-25_37_4.png\": 0,\n        \"dalle-3_37_2.jpg\": 3\n      },\n      \"aggregatedResultsRatios\": {\n        \"aurora-20-1-25_37_4.png\": 0.0,\n        \"dalle-3_37_2.jpg\": 1.0\n      },\n      \"summedUserScores\": {\n        \"aurora-20-1-25_37_4.png\": 0.0,\n        \"dalle-3_37_2.jpg\": 1.196\n      },\n      \"summedUserScoresRatios\": {\n        \"aurora-20-1-25_37_4.png\": 0.0,\n        \"dalle-3_37_2.jpg\": 1.0\n      },\n      \"detailedResults\": [\n          {\n              \"votedFor\": \"dalle-3_37_2.jpg\",\n              \"userDetails\": {\n                  \"country\": \"BY\",\n                  \"language\": \"ru\",\n                  \"userScores\": {\n                      \"global\": 0.4469\n                  },\n                  \"age\": \"Unknown\",\n                  \"gender\": \"Unknown\",\n                  \"occupation\": \"Unknown\"\n              }\n          },\n          {\n              \"votedFor\": \"dalle-3_37_2.jpg\",\n              \"userDetails\": {\n                  \"country\": \"LY\",\n                  \"language\": \"ar\",\n                  \"userScores\": {\n                      \"global\": 0.3923\n                  },\n                  \"age\": \"0-17\",\n                  \"gender\": \"Other\",\n                  \"occupation\": \"Other Employment\"\n              }\n          },\n          {\n              \"votedFor\": \"dalle-3_37_2.jpg\",\n              \"userDetails\": {\n                  \"country\": \"BY\",\n                  \"language\": \"ru\",\n                  \"userScores\": {\n                      \"global\": 0.3568\n                  },\n                  \"age\": \"0-17\",\n                  \"gender\": \"Other\",\n                  \"occupation\": \"Healthcare\"\n              }\n          }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"understanding_the_results/#breakdown-of-the-results","title":"Breakdown of the Results","text":"<ol> <li> <p><code>info</code></p> <ul> <li><code>createdAt</code>: The timestamp indicating when the results overview was generated, in UTC time.</li> <li><code>version</code>: The version of the aggregator system that produced the results.</li> </ul> </li> <li> <p><code>summary</code></p> <ul> <li><code>A_wins_total</code>: The total number of comparisons won by option A (index 0) across all pairs</li> <li><code>B_wins_total</code>: The total number of comparisons won by option B (index 1) across all pairs</li> </ul> </li> <li> <p><code>results</code>: This section contains the actual comparison data collected from the annotators. For comparison orders, each item includes:</p> <ul> <li><code>context</code>: The prompt or description provided for the comparison task</li> <li><code>winner_index</code>: Index of the winning option (0 for first option, 1 for second option)</li> <li> <p><code>winner</code>: Filename or identifier of the winning option</p> </li> <li> <p><code>aggregatedResults</code>: The total number of responses each option received for this specific comparison.     <pre><code>\"aggregatedResults\": {\n    \"aurora-20-1-25_37_4.png\": 0,\n    \"dalle-3_37_2.jpg\": 3\n}\n</code></pre></p> </li> <li> <p><code>aggregatedResultsRatios</code>: The proportion of responses each option received, calculated as the number of responses for the option divided by the total number of responses.     <pre><code>\"aggregatedResultsRatios\": {\n    \"aurora-20-1-25_37_4.png\": 0.0,\n    \"dalle-3_37_2.jpg\": 1.0\n}\n</code></pre></p> </li> <li> <p><code>summedUserScores</code>: The sum of the annotators' global userScore values for each option. This metric accounts for the reliability of each annotator's response.     <pre><code>\"summedUserScores\": {\n    \"aurora-20-1-25_37_4.png\": 0.0,\n    \"dalle-3_37_2.jpg\": 1.196\n}\n</code></pre></p> </li> <li> <p><code>summedUserScoresRatios</code>: The proportion of the summed global userScores for each option, providing a weighted ratio based on annotator reliability.     <pre><code>\"summedUserScoresRatios\": {\n    \"aurora-20-1-25_37_4.png\": 0.0,\n    \"dalle-3_37_2.jpg\": 1.0\n}\n</code></pre></p> </li> <li> <p><code>detailedResults</code>: A list of individual responses from each annotator, including:</p> <ul> <li><code>votedFor</code>: The option chosen by the annotator</li> <li><code>userDetails</code>: Information about the annotator<ul> <li><code>country</code>: Country code of the annotator</li> <li><code>language</code>: Language in which the annotator viewed the task</li> <li><code>userScores</code>: A score representing the annotator's reliability across different dimensions<ul> <li><code>global</code>: The global userScore of the annotator, which is a measure of their overall reliability</li> </ul> </li> <li><code>age</code>: Age group of the annotator</li> <li><code>gender</code>: The gender of the annotator</li> <li><code>occupation</code>: The occupation of the annotator</li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"understanding_the_results/#understanding-the-user-scores","title":"Understanding the User Scores","text":"<p>The <code>userScore</code> is a value between 0 and 1 (1 can never be reached, but can appear because of rounding) that indicates the reliability or trustworthiness of an annotator's responses. A higher score suggests that the annotator consistently provides accurate and reliable answers.</p>"},{"location":"understanding_the_results/#how-is-it-calculated","title":"How is it Calculated?","text":"<p>The <code>userScore</code> is derived from the annotator's performance on Validation Tasks\u2014tasks with known correct answers. By evaluating how accurately an annotator completes these tasks, we assign a score that reflects their understanding and adherence to the task requirements. It is not simply the accuracy, as it also takes into account the difficulties of the tasks, but strongly related to it.</p> <p>For most tasks, the <code>global</code> userScore is the most relevant and can be used per default. If you need more specific information, you may contact us directly at info@rapidata.ai.</p> <p>To know more about the Validation Tasks have a look at the Improve Order Quality guide.</p>"},{"location":"understanding_the_results/#why-is-it-important","title":"Why is it Important?","text":"<ul> <li>Weighted Analysis: Responses from annotators with higher <code>userScores</code> can be given more weight, improving the overall quality of the aggregated results.</li> <li>Quality Control: It helps in identifying and filtering for the most reliable responses.</li> <li>Insight into Annotator Performance: Provides transparency into who is contributing to your data and how reliably.</li> </ul>"},{"location":"understanding_the_results/#utilizing-the-results","title":"Utilizing the Results","text":"<ul> <li>Clear Winners: Use <code>winner</code> and <code>winner_index</code> to quickly identify which option was preferred. It is calculated based on the global userScores.</li> <li>Aggregated Insights: Use <code>aggregatedResults</code> and <code>aggregatedResultsRatios</code> to understand the strength of preference between options</li> <li>Weighted Decisions: Consider <code>summedUserScores</code> and <code>summedUserScoresRatios</code> to make decisions based on annotator reliability</li> <li>Detailed Analysis: Explore <code>detailedResults</code> to see individual responses and gather insights about annotator demographics and performance</li> </ul>"},{"location":"understanding_the_results/#conclusion","title":"Conclusion","text":"<p>By thoroughly understanding each component of the results, you can effectively interpret the data and make informed decisions. Leveraging the userScore and validation sets ensures high-quality, reliable data for your projects.</p>"},{"location":"examples/classify_order/","title":"Example Classify Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> BasicAdvanced <p>With this order, we want to rate different images based on a Likert scale to better understand how well the model generated the images we intended. We have various images that we want to evaluate, and we will assess how accurately they represent the desired concepts. When you run this with your own examples, you may use local paths to your images instead of the URLs.</p> <p>The <code>NoShuffle</code> setting is used to ensure that the answer options remain in a fixed order, because the answer options are ordered.</p> <pre><code>from rapidata import RapidataClient, NoShuffle\n\n# List of image URLs representing different emotions\nIMAGE_URLS: list[str] = [\n    \"https://assets.rapidata.ai/tshirt-4o.png\",   # Related T-Shirt with the text \"Running on caffeine &amp; dreams\"\n    \"https://assets.rapidata.ai/tshirt-aurora.jpg\",   # Related T-shirt with the text \"Running on caffeine &amp; dreams\"\n    \"https://assets.rapidata.ai/teamleader-aurora.jpg\",   # Unrelated image\n]\n\nCONTEXTS: list[str] = [\n    \"A t-shirt with the text 'Running on caffeine &amp; dreams'\"\n] * len(IMAGE_URLS) # Each image will have the same context to be rated by\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n\n    # Create a classification order for emotions based on the images\n    order = rapi.order.create_classification_order(\n        name=\"Likert Scale Example\",  \n        instruction=\"How well does the image match the description?\",\n        answer_options=[\"1: Not at all\", \n                        \"2: A little\", \n                        \"3: Moderately\", \n                        \"4: Very well\", \n                        \"5: Perfectly\"], \n        contexts=CONTEXTS,\n        datapoints=IMAGE_URLS,\n        responses_per_datapoint=25,\n        settings=[NoShuffle()]  # Do not shuffle the answer options\n    ).run()  # Execute the order\n\n    # Display a progress bar for the order\n    order.display_progress_bar()\n\n    # Retrieve and print the results of the classification\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre> <p>In the advanced example we will first create a validation set to give the annotators a reference how they should rate the images.</p> <p>To get a better understanding of validation sets, please refer to the Improve Quality guide.</p> <pre><code>from rapidata import RapidataClient, NoShuffle\n\n# ===== TASK CONFIGURATION =====\n# Likert scale options (from lowest to highest agreement)\nANSWER_OPTIONS: list[str] = [\n    \"1: Not at all\",\n    \"2: A little\",\n    \"3: Moderately\", \n    \"4: Very well\",\n    \"5: Perfectly\"\n]\n\nINSTRUCTION: str = \"How well does the image match the description?\"\n\n# ===== VALIDATION DATA =====\n# This validation set helps ensure quality responses by providing known examples\nVALIDATION_IMAGE_URLS: list[str] = [\n    \"https://assets.rapidata.ai/email-4o.png\",\n    \"https://assets.rapidata.ai/email-aurora.jpg\",\n    \"https://assets.rapidata.ai/teacher-aurora.jpg\",\n]\n\nVALIDATION_CONTEXT: str = \"A laptop screen with clearly readable text, addressed to the marketing team about an upcoming meeting\"\n\nVALIDATION_CONTEXTS: list[str] = [VALIDATION_CONTEXT] * len(VALIDATION_IMAGE_URLS)\n\n# Expected correct answers for each validation image (multiple acceptable answers possible)\nVALIDATION_TRUTHS: list[list[str]] = [\n    [\"5: Perfectly\", \"4: Very well\"],  # First image matches very well\n    [\"3: Moderately\"],                 # Second image matches moderately\n    [\"1: Not at all\"]                  # Third image doesn't match at all\n]\n\n# ===== REAL TASK DATA =====\n# Images to be classified\nIMAGE_URLS: list[str] = [\n    \"https://assets.rapidata.ai/tshirt-4o.png\",       # Related T-Shirt with text\n    \"https://assets.rapidata.ai/tshirt-aurora.jpg\",   # Related T-shirt with text\n    \"https://assets.rapidata.ai/teamleader-aurora.jpg\", # Unrelated image\n]\n\n# Description that workers will compare against the images\nT_SHIRT_DESCRIPTION: str = \"A t-shirt with the text 'Running on caffeine &amp; dreams'\"\nCONTEXTS: list[str] = [T_SHIRT_DESCRIPTION] * len(IMAGE_URLS)\n\ndef create_validation_set(client: RapidataClient) -&gt; str:\n    \"\"\"\n    Create a validation set to ensure quality responses.\n\n    Args:\n        client: The RapidataClient instance\n\n    Returns:\n        The validation set ID\n    \"\"\"\n    validation_set = client.validation.create_classification_set(\n        name=\"Example Likert Scale Validation Set\",\n        instruction=INSTRUCTION,\n        answer_options=ANSWER_OPTIONS,\n        contexts=VALIDATION_CONTEXTS,\n        datapoints=VALIDATION_IMAGE_URLS,\n        truths=VALIDATION_TRUTHS\n    )\n    return validation_set.id\n\n\ndef main():\n    \"\"\"Run the complete example workflow\"\"\"\n    # Initialize the client\n    client = RapidataClient()\n\n    # Step 1: Create validation set\n    validation_set_id = create_validation_set(client)\n    print(f\"Created validation set with ID: {validation_set_id}\")\n\n    # Step 2: Create and run the classification order\n    print(\"Creating and running classification order...\")\n    order = client.order.create_classification_order(\n        name=\"Likert Scale Example\",  \n        instruction=INSTRUCTION,\n        answer_options=ANSWER_OPTIONS, \n        contexts=CONTEXTS,\n        datapoints=IMAGE_URLS,\n        validation_set_id=validation_set_id,  # Using our validation set\n        responses_per_datapoint=25,\n        settings=[NoShuffle()]               # Keep Likert scale in order\n    ).run()  # Start the order\n\n    order.display_progress_bar()\n\n    results = order.get_results()\n    print(results)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/compare_order/","title":"Example Compare Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> BasicAdvanced <p>In this example we want to compare two image-to-text models, Flux and Midjourney, that have generated images based on a description, aka prompt. Those images have been saved to a public URL in order to be able to run the example anywhere. When you run this with your own examples, you may use local paths to your images instead of the URLs.</p> <p>We now want to find out which of the two images more closely aligns with the prompt - for every prompt.</p> <pre><code>from rapidata import RapidataClient\n\nPROMPTS: list[str] = [\n    \"A sign that says 'Diffusion'.\",\n    \"A yellow flower sticking out of a green pot.\",\n    \"hyperrealism render of a surreal alien humanoid.\",\n    \"psychedelic duck\",\n    \"A small blue book sitting on a large red book.\"\n]\n\n# Image pairs to be matched with prompts (flux vs midjourney versions)\nIMAGE_PAIRS: list[list[str]] = [\n    [\"https://assets.rapidata.ai/flux_sign_diffusion.jpg\", \"https://assets.rapidata.ai/mj_sign_diffusion.jpg\"],\n    [\"https://assets.rapidata.ai/flux_flower.jpg\", \"https://assets.rapidata.ai/mj_flower.jpg\"],\n    [\"https://assets.rapidata.ai/flux_alien.jpg\", \"https://assets.rapidata.ai/mj_alien.jpg\"],\n    [\"https://assets.rapidata.ai/flux_duck.jpg\", \"https://assets.rapidata.ai/mj_duck.jpg\"],\n    [\"https://assets.rapidata.ai/flux_book.jpg\", \"https://assets.rapidata.ai/mj_book.jpg\"]\n]\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n    order = rapi.order.create_compare_order(\n        name=\"Example Image Prompt Alignment Order\",\n        instruction=\"Which image follows the prompt more accurately?\",\n        datapoints=IMAGE_PAIRS,\n        responses_per_datapoint=25,\n        contexts=PROMPTS, # prompt is the context for each image pair\n    ).run()\n    order.display_progress_bar()\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre> <p>In the advanced example we will first create a validation set to give the annotators a reference how they should rate the images.</p> <p>To get a better understanding of validation sets, please refer to the Improve Quality guide.</p> <pre><code>from rapidata import RapidataClient\n\n# ===== TASK CONFIGURATION =====\nINSTRUCTION: str = \"Which image follows the prompt more accurately?\"\n\n# ===== VALIDATION DATA =====\n# This validation set helps ensure quality responses by providing known examples\nVALIDATION_PROMPTS: list[str] = [\n    \"2 cats sitting on both sides of a dog\",\n    \"girl wearing a futuristic costume without her face being covered by a mask\",\n    \"a train traveling fast through a forest\",\n]\n\nVALIDATION_IMAGE_PAIRS: list[list[str]] = [\n    [\"https://assets.rapidata.ai/2_cats_1_dog.jpg\", \"https://assets.rapidata.ai/2_dogs_1_cat.jpg\"],\n    [\"https://assets.rapidata.ai/girl_without_mask.jpg\", \"https://assets.rapidata.ai/girl_with_mask.jpg\"],\n    [\"https://assets.rapidata.ai/train_normal.jpg\", \"https://assets.rapidata.ai/train_surfing.jpg\"],\n]\n\nVALIDATION_TRUTHS: list[str] = [\n    \"https://assets.rapidata.ai/2_cats_1_dog.jpg\",     # First image: 2 cats, 1 dog\n    \"https://assets.rapidata.ai/girl_without_mask.jpg\", # First image: girl without mask\n    \"https://assets.rapidata.ai/train_normal.jpg\",     # First image: normal train through forest\n]\n\n# ===== REAL TASK DATA =====\n# Prompts to be matched with images\nPROMPTS: list[str] = [\n    \"A sign that says 'Diffusion'.\",\n    \"A yellow flower sticking out of a green pot.\",\n    \"hyperrealism render of a surreal alien humanoid.\",\n    \"psychedelic duck\",\n    \"A small blue book sitting on a large red book.\"\n]\n\n# Image pairs to be matched with prompts (flux vs midjourney versions)\nIMAGE_PAIRS: list[list[str]] = [\n    [\"https://assets.rapidata.ai/flux_sign_diffusion.jpg\", \"https://assets.rapidata.ai/mj_sign_diffusion.jpg\"],\n    [\"https://assets.rapidata.ai/flux_flower.jpg\", \"https://assets.rapidata.ai/mj_flower.jpg\"],\n    [\"https://assets.rapidata.ai/flux_alien.jpg\", \"https://assets.rapidata.ai/mj_alien.jpg\"],\n    [\"https://assets.rapidata.ai/flux_duck.jpg\", \"https://assets.rapidata.ai/mj_duck.jpg\"],\n    [\"https://assets.rapidata.ai/flux_book.jpg\", \"https://assets.rapidata.ai/mj_book.jpg\"]\n]\n\n\ndef create_validation_set(client: RapidataClient) -&gt; str:\n    \"\"\"\n    Create a validation set to ensure quality responses.\n\n    Args:\n        client: The RapidataClient instance\n\n    Returns:\n        The validation set ID\n    \"\"\"\n    validation_set = client.validation.create_compare_set(\n        name=\"Example Image Prompt Alignment Validation Set\",\n        instruction=INSTRUCTION,\n        contexts=VALIDATION_PROMPTS,\n        datapoints=VALIDATION_IMAGE_PAIRS,\n        truths=VALIDATION_TRUTHS\n    )\n    return validation_set.id\n\n\ndef main():\n    client = RapidataClient()\n\n    validation_set_id = create_validation_set(client)\n\n    order = client.order.create_compare_order(\n        name=\"Example Image Prompt Alignment Order\",\n        instruction=INSTRUCTION,\n        datapoints=IMAGE_PAIRS,\n        contexts=PROMPTS,\n        responses_per_datapoint=25,\n        validation_set_id=validation_set_id\n    ).run()\n\n    order.display_progress_bar()\n\n    results = order.get_results()\n    print(results)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/draw_order/","title":"Example Draw Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> <p>A powerful new capability of image generation models is their ability to edit images. But how do these models understand the location and size of objects they need to edit? In this example, we will create an order where annotators color in the object in images, providing training data that helps the model learn where to apply its edits.</p> <pre><code>from rapidata import RapidataClient\n\nIMAGE_URLS = [\"https://assets.rapidata.ai/midjourney-5.2_37_3.jpg\", \n              \"https://assets.rapidata.ai/flux-1-pro_37_0.jpg\",\n              \"https://assets.rapidata.ai/frames-23-1-25_37_4.png\",\n              \"https://assets.rapidata.ai/aurora-20-1-25_37_3.png\"]\n\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n\n    order = rapi.order.create_draw_order(\n        name=\"Example Image Comparison\",\n        instruction=\"Color in all the blue books\",\n        datapoints=IMAGE_URLS,\n        responses_per_datapoint=35,\n    ).run()\n\n    order.display_progress_bar()\n\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/free_text_order/","title":"Example Free Text Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> BasicAdvanced <p>Let's assume you want to build a new LLM chatbot and you want to know what people might want to ask the bot. You can create a free-text order to gather the questions people might have.</p> <p>The free-text order will take longer to complete than the others, as the response process is slightly more involved from the perspective of the annotator.</p> <pre><code>from rapidata import RapidataClient\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n    order = rapi.order.create_free_text_order(\n        name=\"Example prompt generation\",\n        instruction=\"What would you like to ask an AI? please spell out the question\",\n        datapoints=[\"https://assets.rapidata.ai/ai_question.png\"],\n    ).run()\n    order.display_progress_bar()\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre> <p>In the advanced example we will add some filters to the order to make sure only annotators that have their phone set to english are able to answer. Additionally we will set a minimum length of 5 characters for the questions.</p> <pre><code>from rapidata import (\n    RapidataClient,\n    FreeTextMinimumCharacters,\n    LanguageFilter,\n)\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n    order = rapi.order.create_free_text_order(\n        name=\"Example prompt generation\",\n        instruction=\"What would you like to ask an AI? please spell out the question\",\n        datapoints=[\"https://assets.rapidata.ai/ai_question.png\"],\n        settings=[FreeTextMinimumCharacters(5)],\n        filters=[LanguageFilter([\"en\"])],\n    ).run()\n    order.display_progress_bar()\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/locate_order/","title":"Example Locate Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> <p>A big part of image generation is to make coherent images without any visual glitches. Rapidata makes it easy to get feedback on what parts of the image the model messed up. In this example, we will create an order where annotators are asked to select the parts of the image where there are visual glitches.</p> <pre><code>from rapidata import RapidataClient, RapidataSettings\n\nIMAGE_URLS = [\"https://assets.rapidata.ai/eac11c3e-ad57-402b-90ed-23378d2ff869.jpg\",\n              \"https://assets.rapidata.ai/04e7e3c6-5554-47ca-bdb2-950e48ac3e6c.jpg\",\n              \"https://assets.rapidata.ai/91d9913c-b399-47f8-ad19-767798cc951c.jpg\",\n              \"https://assets.rapidata.ai/d1cbf51d-7712-4819-96b4-20e030c573de.jpg\"]\n\n\nif __name__ == \"__main__\":\n\n    rapi = RapidataClient()\n\n    order = rapi.order.create_locate_order(\n        name=\"Artifact detection example\",\n        instruction=\"Look close, find incoherent errors, like senseless or malformed objects, incomprehensible details, or visual glitches? Tap to select.\",\n        datapoints=IMAGE_URLS,\n        responses_per_datapoint=35,\n        settings=[RapidataSettings.alert_on_fast_response(2500)], # This is optional, it will alert you if the annotators are responding before 2.5 seconds\n        validation_set_id=\"6768a557026456ec851f51f9\" # in this example, the validation set has already been created\n    ).run()\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/ranking_order/","title":"Example Ranking Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> <p>In this example, we will create an order to rank various images of rabbits. The matchups will be generated automatically, comparing two images at a time. The ranking system is based on an Elo rating system, which updates rankings based on the results of these matchups.</p> <p>The instruction is designed to focus on the comparison between the two images rather than the overall ranking. For example, instead of asking \"Which rabbit looks the coolest?\", we ask \"Which rabbit looks cooler?\" to emphasize the specific matchup.</p> <pre><code>from rapidata import RapidataClient\n\nDATAPOINTS = [\n    \"https://assets.rapidata.ai/f9d92460-a362-493c-af91-bf50046453ae.webp\",\n    \"https://assets.rapidata.ai/9bcd8b18-e9ad-4449-84d4-b3d72e200e9c.webp\",\n    \"https://assets.rapidata.ai/266f6446-3ca8-4c2d-b070-13558b35a4e0.webp\",\n    \"https://assets.rapidata.ai/f787f02c-e5d0-43ca-aa6e-aea747845cf3.webp\",\n    \"https://assets.rapidata.ai/7e518a1b-4d1c-4a86-9109-26646684cc02.webp\",\n    \"https://assets.rapidata.ai/10af47bd-3502-4534-b917-73dba5feaf76.webp\",\n    \"https://assets.rapidata.ai/59725ca0-1fd5-4850-a15c-4221e191e293.webp\",\n    \"https://assets.rapidata.ai/65d3939d-c1b8-433c-b180-13dae80f0519.webp\",\n    \"https://assets.rapidata.ai/c13b8feb-fb97-4646-8dfc-97f05d37a637.webp\",\n    \"https://assets.rapidata.ai/586dc517-c987-4d06-8a6f-553508b86356.webp\",\n    \"https://assets.rapidata.ai/f4884ecd-cacb-4387-ab18-3b6e7dcdf10c.webp\",\n    \"https://assets.rapidata.ai/79076f76-a432-4ef9-9007-6d09a218417a.webp\"\n]\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n\n    order = rapi.order.create_ranking_order(\n        name=\"Example Ranking Order\",\n        instruction=\"Which rabbit looks cooler?\",\n        datapoints=DATAPOINTS,\n        total_comparison_budget=50, #Make 50 comparisons, each comparison containing 2 datapoints\n        random_comparisons_ratio=0.5 #First half of the comparisons are random, the second half are close matchups\n    ).run()\n\n    order.display_progress_bar()\n    results = order.get_results()\n    print(results)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"examples/select_words_order/","title":"Example Select Words Order","text":"<p>To learn about the basics of creating an order, please refer to the quickstart guide.</p> <p>A big part of image generation is following the prompt accurately. Rapidata makes it easy to get feedback on what parts of the prompt the model is struggling with. In this example, we will create an order where annotators are asked to select the words that are not correctly depicted in the image.</p> <pre><code>from rapidata import RapidataClient\n\nIMAGE_URLS = ['https://assets.rapidata.ai/dalle-3_244_0.jpg',\n        'https://assets.rapidata.ai/dalle-3_30_1.jpg',\n        'https://assets.rapidata.ai/dalle-3_268_2.jpg',\n        'https://assets.rapidata.ai/dalle-3_26_2.jpg']\n\nPROMPTS = ['The black camera was next to the white tripod.',\n        'Four cars on the street.',\n        'Car is bigger than the airplane.',\n        'One cat and two dogs sitting on the grass.']\n\nPROMPTS_WITH_NO_MISTAKES = [prompt + \" [No_mistakes]\" for prompt in PROMPTS] # selection is split based on spaces\n\nif __name__ == \"__main__\":\n    rapi = RapidataClient()\n    order = rapi.order.create_select_words_order(\n        name=\"Detect Mistakes in Image-Text Alignment\",\n        instruction=\"The image is based on the text below. Select mistakes, i.e., words that are not aligned with the image.\",\n        datapoints=IMAGE_URLS,\n        responses_per_datapoint=15,\n        sentences=PROMPTS_WITH_NO_MISTAKES,\n        filters=[\n            rapi.order.filters.language([\"en\"]),\n        ],\n        validation_set_id=\"6761a86eef7af86285630ea8\" # in this example, the validation set has already been created\n    ).run()\n    order.display_progress_bar()\n    result = order.get_results()\n    print(result)\n</code></pre> <p>To preview the order and see what the annotators see, you can run the following code:</p> <pre><code>order.preview()\n</code></pre> <p>To open the order in the browser, you can run the following code:</p> <pre><code>order.view()\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>rapidata<ul> <li>rapidata_client<ul> <li>api<ul> <li>rapidata_api_client</li> </ul> </li> <li>benchmark<ul> <li>leaderboard<ul> <li>rapidata_leaderboard</li> </ul> </li> <li>rapidata_benchmark</li> <li>rapidata_benchmark_manager</li> </ul> </li> <li>config<ul> <li>logger</li> <li>logging_config</li> <li>managed_print</li> <li>order_config</li> <li>rapidata_config</li> <li>tracer</li> <li>upload_config</li> </ul> </li> <li>datapoints<ul> <li>assets<ul> <li>constants</li> </ul> </li> </ul> </li> <li>demographic<ul> <li>demographic_manager</li> </ul> </li> <li>exceptions<ul> <li>failed_upload_exception</li> </ul> </li> <li>filter<ul> <li>age_filter</li> <li>and_filter</li> <li>campaign_filter</li> <li>country_filter</li> <li>custom_filter</li> <li>gender_filter</li> <li>language_filter</li> <li>models<ul> <li>age_group</li> <li>gender</li> </ul> </li> <li>new_user_filter</li> <li>not_filter</li> <li>or_filter</li> <li>rapidata_filters</li> <li>response_count_filter</li> <li>user_score_filter</li> </ul> </li> <li>order<ul> <li>rapidata_order</li> <li>rapidata_order_manager</li> <li>rapidata_results</li> </ul> </li> <li>rapidata_client</li> <li>selection<ul> <li>ab_test_selection</li> <li>capped_selection</li> <li>conditional_validation_selection</li> <li>demographic_selection</li> <li>effort_selection</li> <li>labeling_selection</li> <li>rapidata_retrieval_modes</li> <li>rapidata_selections</li> <li>shuffling_selection</li> <li>static_selection</li> <li>validation_selection</li> </ul> </li> <li>settings<ul> <li>alert_on_fast_response</li> <li>allow_neither_both</li> <li>custom_setting</li> <li>free_text_minimum_characters</li> <li>models<ul> <li>translation_behaviour_options</li> </ul> </li> <li>no_shuffle</li> <li>play_video_until_the_end</li> <li>rapidata_settings</li> <li>swap_context_instruction</li> <li>translation_behaviour</li> </ul> </li> <li>validation<ul> <li>rapidata_validation_set</li> <li>rapids<ul> <li>box</li> <li>rapids</li> <li>rapids_manager</li> </ul> </li> <li>validation_set_manager</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/rapidata/rapidata_client/rapidata_client/","title":"Rapidata client","text":""},{"location":"reference/rapidata/rapidata_client/rapidata_client/#rapidata.rapidata_client.rapidata_client.RapidataClient","title":"RapidataClient","text":"<pre><code>RapidataClient(\n    client_id: str | None = None,\n    client_secret: str | None = None,\n    environment: str = \"rapidata.ai\",\n    oauth_scope: str = \"openid roles\",\n    cert_path: str | None = None,\n    token: dict | None = None,\n    leeway: int = 60,\n)\n</code></pre> <p>The Rapidata client is the main entry point for interacting with the Rapidata API. It allows you to create orders and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>str</code> <p>The client ID for authentication.</p> <code>None</code> <code>client_secret</code> <code>str</code> <p>The client secret for authentication.</p> <code>None</code> <code>environment</code> <code>str</code> <p>The API endpoint.</p> <code>'rapidata.ai'</code> <code>oauth_scope</code> <code>str</code> <p>The scopes to use for authentication. In general this does not need to be changed.</p> <code>'openid roles'</code> <code>cert_path</code> <code>str</code> <p>An optional path to a certificate file useful for development.</p> <code>None</code> <code>token</code> <code>dict</code> <p>If you already have a token that the client should use for authentication. Important, if set, this needs to be the complete token object containing the access token, token type and expiration time.</p> <code>None</code> <code>leeway</code> <code>int</code> <p>An optional leeway to use to determine if a token is expired. Defaults to 60 seconds.</p> <code>60</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>RapidataOrderManager</code> <p>The RapidataOrderManager instance.</p> <code>validation</code> <code>ValidationSetManager</code> <p>The ValidationSetManager instance.</p> <code>demographic</code> <code>DemographicManager</code> <p>The DemographicManager instance.</p> <code>mri</code> <code>RapidataBenchmarkManager</code> <p>The RapidataBenchmarkManager instance.</p> Source code in <code>src/rapidata/rapidata_client/rapidata_client.py</code> <pre><code>def __init__(\n    self,\n    client_id: str | None = None,\n    client_secret: str | None = None,\n    environment: str = \"rapidata.ai\",\n    oauth_scope: str = \"openid roles\",\n    cert_path: str | None = None,\n    token: dict | None = None,\n    leeway: int = 60,\n):\n    \"\"\"Initialize the RapidataClient. If both the client_id and client_secret are None, it will try using your credentials under \"~/.config/rapidata/credentials.json\".\n    If this is not successful, it will open a browser window and ask you to log in, then save your new credentials in said json file.\n\n    Args:\n        client_id (str): The client ID for authentication.\n        client_secret (str): The client secret for authentication.\n        environment (str, optional): The API endpoint.\n        oauth_scope (str, optional): The scopes to use for authentication. In general this does not need to be changed.\n        cert_path (str, optional): An optional path to a certificate file useful for development.\n        token (dict, optional): If you already have a token that the client should use for authentication. Important, if set, this needs to be the complete token object containing the access token, token type and expiration time.\n        leeway (int, optional): An optional leeway to use to determine if a token is expired. Defaults to 60 seconds.\n\n    Attributes:\n        order (RapidataOrderManager): The RapidataOrderManager instance.\n        validation (ValidationSetManager): The ValidationSetManager instance.\n        demographic (DemographicManager): The DemographicManager instance.\n        mri (RapidataBenchmarkManager): The RapidataBenchmarkManager instance.\n    \"\"\"\n    tracer.set_session_id(\n        uuid.UUID(int=random.Random().getrandbits(128), version=4).hex\n    )\n\n    with tracer.start_as_current_span(\"RapidataClient.__init__\"):\n        logger.debug(\"Checking version\")\n        self._check_version()\n        if environment != \"rapidata.ai\":\n            rapidata_config.logging.enable_otlp = False\n\n        logger.debug(\"Initializing OpenAPIService\")\n        self._openapi_service = OpenAPIService(\n            client_id=client_id,\n            client_secret=client_secret,\n            environment=environment,\n            oauth_scope=oauth_scope,\n            cert_path=cert_path,\n            token=token,\n            leeway=leeway,\n        )\n\n        logger.debug(\"Initializing RapidataOrderManager\")\n        self.order = RapidataOrderManager(openapi_service=self._openapi_service)\n\n        logger.debug(\"Initializing ValidationSetManager\")\n        self.validation = ValidationSetManager(\n            openapi_service=self._openapi_service\n        )\n\n        logger.debug(\"Initializing DemographicManager\")\n        self._demographic = DemographicManager(\n            openapi_service=self._openapi_service\n        )\n\n        logger.debug(\"Initializing RapidataBenchmarkManager\")\n        self.mri = RapidataBenchmarkManager(openapi_service=self._openapi_service)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/rapidata_client/#rapidata.rapidata_client.rapidata_client.RapidataClient.reset_credentials","title":"reset_credentials","text":"<pre><code>reset_credentials()\n</code></pre> <p>Reset the credentials saved in the configuration file for the current environment.</p> Source code in <code>src/rapidata/rapidata_client/rapidata_client.py</code> <pre><code>def reset_credentials(self):\n    \"\"\"Reset the credentials saved in the configuration file for the current environment.\"\"\"\n    self._openapi_service.reset_credentials()\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/api/rapidata_api_client/","title":"Rapidata api client","text":""},{"location":"reference/rapidata/rapidata_client/api/rapidata_api_client/#rapidata.rapidata_client.api.rapidata_api_client.RapidataError","title":"RapidataError","text":"<pre><code>RapidataError(\n    status_code: Optional[int] = None,\n    message: str | None = None,\n    original_exception: Exception | None = None,\n    details: Any = None,\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Custom error class for Rapidata API errors.</p> Source code in <code>src/rapidata/rapidata_client/api/rapidata_api_client.py</code> <pre><code>def __init__(\n    self,\n    status_code: Optional[int] = None,\n    message: str | None = None,\n    original_exception: Exception | None = None,\n    details: Any = None,\n):\n    self.status_code = status_code\n    self.message = message\n    self.original_exception = original_exception\n    self.details = details\n\n    # Create a nice error message\n    error_msg = \"Rapidata API Error\"\n    if status_code:\n        error_msg += f\" ({status_code})\"\n    if message:\n        error_msg += f\": {message}\"\n\n    super().__init__(error_msg)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/api/rapidata_api_client/#rapidata.rapidata_client.api.rapidata_api_client.RapidataApiClient","title":"RapidataApiClient","text":"<pre><code>RapidataApiClient(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ApiClient</code></p> <p>Custom API client that wraps errors in RapidataError.</p> Source code in <code>src/rapidata/rapidata_client/api/rapidata_api_client.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.id_generator = RandomIdGenerator()\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/api/rapidata_api_client/#rapidata.rapidata_client.api.rapidata_api_client.RapidataApiClient.response_deserialize","title":"response_deserialize","text":"<pre><code>response_deserialize(\n    response_data: RESTResponse,\n    response_types_map: Optional[\n        dict[str, ApiResponseT]\n    ] = None,\n) -&gt; ApiResponse[ApiResponseT]\n</code></pre> <p>Override the response_deserialize method to catch and convert exceptions.</p> Source code in <code>src/rapidata/rapidata_client/api/rapidata_api_client.py</code> <pre><code>def response_deserialize(\n    self,\n    response_data: rest.RESTResponse,\n    response_types_map: Optional[dict[str, ApiResponseT]] = None,\n) -&gt; ApiResponse[ApiResponseT]:\n    \"\"\"Override the response_deserialize method to catch and convert exceptions.\"\"\"\n    try:\n        return super().response_deserialize(response_data, response_types_map)\n    except ApiException as e:\n        status_code = getattr(e, \"status\", None)\n        message = str(e)\n        details = None\n\n        # Extract more detailed error message from response body if available\n        if hasattr(e, \"body\") and e.body:\n            try:\n                body_json = json.loads(e.body)\n                if isinstance(body_json, dict):\n                    if \"message\" in body_json:\n                        message = body_json[\"message\"]\n                    elif \"error\" in body_json:\n                        message = body_json[\"error\"]\n\n                    # Store the full error details for debugging\n                    details = body_json\n            except (json.JSONDecodeError, AttributeError):\n                # If we can't parse the body as JSON, use the original message\n                pass\n\n        error_formatted = RapidataError(\n            status_code=status_code,\n            message=message,\n            original_exception=e,\n            details=details,\n        )\n\n        # Only log error if not suppressed\n        if not _should_suppress_error_logging():\n            logger.error(\"Error: %s\", error_formatted)\n        else:\n            logger.debug(\"Suppressed Error: %s\", error_formatted)\n\n        raise error_formatted from None\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/api/rapidata_api_client/#rapidata.rapidata_client.api.rapidata_api_client.suppress_rapidata_error_logging","title":"suppress_rapidata_error_logging","text":"<pre><code>suppress_rapidata_error_logging()\n</code></pre> <p>Context manager to suppress error logging for RapidataApiClient calls.</p> Source code in <code>src/rapidata/rapidata_client/api/rapidata_api_client.py</code> <pre><code>@contextmanager\ndef suppress_rapidata_error_logging():\n    \"\"\"Context manager to suppress error logging for RapidataApiClient calls.\"\"\"\n    old_value = getattr(_thread_local, \"suppress_error_logging\", False)\n    _thread_local.suppress_error_logging = True\n    try:\n        yield\n    finally:\n        _thread_local.suppress_error_logging = old_value\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/","title":"Rapidata benchmark","text":""},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark","title":"RapidataBenchmark","text":"<pre><code>RapidataBenchmark(\n    name: str, id: str, openapi_service: OpenAPIService\n)\n</code></pre> <p>An instance of a Rapidata benchmark.</p> <p>Used to interact with a specific benchmark in the Rapidata system, such as retrieving prompts and evaluating models.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name that will be used to identify the benchmark on the overview.</p> required <code>id</code> <code>str</code> <p>The id of the benchmark.</p> required <code>openapi_service</code> <code>OpenAPIService</code> <p>The OpenAPI service to use to interact with the Rapidata API.</p> required Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark.py</code> <pre><code>def __init__(self, name: str, id: str, openapi_service: OpenAPIService):\n    self.name = name\n    self.id = id\n    self._openapi_service = openapi_service\n    self.__prompts: list[str | None] = []\n    self.__prompt_assets: list[str | None] = []\n    self.__leaderboards: list[RapidataLeaderboard] = []\n    self.__identifiers: list[str] = []\n    self.__tags: list[list[str]] = []\n    self.__benchmark_page: str = (\n        f\"https://app.{self._openapi_service.environment}/mri/benchmarks/{self.id}\"\n    )\n    self._asset_uploader = AssetUploader(openapi_service)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.prompts","title":"prompts  <code>property</code>","text":"<pre><code>prompts: list[str | None]\n</code></pre> <p>Returns the prompts that are registered for the leaderboard.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.prompt_assets","title":"prompt_assets  <code>property</code>","text":"<pre><code>prompt_assets: list[str | None]\n</code></pre> <p>Returns the prompt assets that are registered for the benchmark.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.tags","title":"tags  <code>property</code>","text":"<pre><code>tags: list[list[str]]\n</code></pre> <p>Returns the tags that are registered for the benchmark.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.leaderboards","title":"leaderboards  <code>property</code>","text":"<pre><code>leaderboards: list[RapidataLeaderboard]\n</code></pre> <p>Returns the leaderboards that are registered for the benchmark.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.add_prompt","title":"add_prompt","text":"<pre><code>add_prompt(\n    identifier: str | None = None,\n    prompt: str | None = None,\n    prompt_asset: str | None = None,\n    tags: Optional[list[str]] = None,\n)\n</code></pre> <p>Adds a prompt to the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | None</code> <p>The identifier of the prompt/asset/tags that will be used to match up the media. If not provided, it will use the prompt, asset or prompt + asset as the identifier.</p> <code>None</code> <code>prompt</code> <code>str | None</code> <p>The prompt that will be used to evaluate the model.</p> <code>None</code> <code>prompt_asset</code> <code>str | None</code> <p>The prompt asset that will be used to evaluate the model. Provided as a link to the asset.</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>The tags can be used to filter the leaderboard results. They will NOT be shown to the users.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark.py</code> <pre><code>def add_prompt(\n    self,\n    identifier: str | None = None,\n    prompt: str | None = None,\n    prompt_asset: str | None = None,\n    tags: Optional[list[str]] = None,\n):\n    \"\"\"\n    Adds a prompt to the benchmark.\n\n    Args:\n        identifier: The identifier of the prompt/asset/tags that will be used to match up the media. If not provided, it will use the prompt, asset or prompt + asset as the identifier.\n        prompt: The prompt that will be used to evaluate the model.\n        prompt_asset: The prompt asset that will be used to evaluate the model. Provided as a link to the asset.\n        tags: The tags can be used to filter the leaderboard results. They will NOT be shown to the users.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataBenchmark.add_prompt\"):\n        if tags is None:\n            tags = []\n\n        if prompt is None and prompt_asset is None:\n            raise ValueError(\"Prompt or prompt asset must be provided.\")\n\n        if identifier is None and prompt is None:\n            raise ValueError(\"Identifier or prompt must be provided.\")\n\n        if identifier and not isinstance(identifier, str):\n            raise ValueError(\"Identifier must be a string.\")\n\n        if prompt and not isinstance(prompt, str):\n            raise ValueError(\"Prompt must be a string.\")\n\n        if prompt_asset and not isinstance(prompt_asset, str):\n            raise ValueError(\n                \"Asset must be a string. That is the link to the asset.\"\n            )\n\n        if identifier is None:\n            assert prompt is not None\n            if prompt in self.prompts:\n                raise ValueError(\n                    \"Prompts must be unique. Otherwise use identifiers.\"\n                )\n            identifier = prompt\n\n        if identifier in self.identifiers:\n            raise ValueError(\"Identifier already exists in the benchmark.\")\n\n        if tags is not None and (\n            not isinstance(tags, list)\n            or not all(isinstance(tag, str) for tag in tags)\n        ):\n            raise ValueError(\"Tags must be a list of strings.\")\n\n        logger.info(\n            \"Adding identifier %s with prompt %s, prompt asset %s and tags %s to benchmark %s\",\n            identifier,\n            prompt,\n            prompt_asset,\n            tags,\n            self.id,\n        )\n\n        self.__identifiers.append(identifier)\n\n        self.__tags.append(tags)\n        self.__prompts.append(prompt)\n        self.__prompt_assets.append(prompt_asset)\n\n        self._openapi_service.benchmark_api.benchmark_benchmark_id_prompt_post(\n            benchmark_id=self.id,\n            submit_prompt_model=SubmitPromptModel(\n                identifier=identifier,\n                prompt=prompt,\n                promptAsset=(\n                    CreateDemographicRapidModelAsset(\n                        actual_instance=ExistingAssetInput(\n                            _t=\"ExistingAssetInput\",\n                            name=self._asset_uploader.upload_asset(prompt_asset),\n                        )\n                    )\n                    if prompt_asset is not None\n                    else None\n                ),\n                tags=tags,\n            ),\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.create_leaderboard","title":"create_leaderboard","text":"<pre><code>create_leaderboard(\n    name: str,\n    instruction: str,\n    show_prompt: bool = False,\n    show_prompt_asset: bool = False,\n    inverse_ranking: bool = False,\n    level_of_detail: Literal[\n        \"low\", \"medium\", \"high\", \"very high\"\n    ] = \"low\",\n    min_responses_per_matchup: int = 3,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n) -&gt; RapidataLeaderboard\n</code></pre> <p>Creates a new leaderboard for the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the leaderboard. (not shown to the users)</p> required <code>instruction</code> <code>str</code> <p>The instruction decides how the models will be evaluated.</p> required <code>show_prompt</code> <code>bool</code> <p>Whether to show the prompt to the users. (default: False)</p> <code>False</code> <code>show_prompt_asset</code> <code>bool</code> <p>Whether to show the prompt asset to the users. (only works if the prompt asset is a URL) (default: False)</p> <code>False</code> <code>inverse_ranking</code> <code>bool</code> <p>Whether to inverse the ranking of the leaderboard. (if the question is inversed, e.g. \"Which video is worse?\")</p> <code>False</code> <code>level_of_detail</code> <code>Literal['low', 'medium', 'high', 'very high']</code> <p>The level of detail of the leaderboard. This will effect how many comparisons are done per model evaluation. (default: \"low\")</p> <code>'low'</code> <code>min_responses_per_matchup</code> <code>int</code> <p>The minimum number of responses required to be considered for the leaderboard. (default: 3)</p> <code>3</code> <code>validation_set_id</code> <code>str | None</code> <p>The id of the validation set that should be attached to the leaderboard. (default: None)</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The filters that should be applied to the leaderboard. Will determine who can solve answer in the leaderboard. (default: [])</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The settings that should be applied to the leaderboard. Will determine the behavior of the tasks on the leaderboard. (default: [])</p> <code>[]</code> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark.py</code> <pre><code>def create_leaderboard(\n    self,\n    name: str,\n    instruction: str,\n    show_prompt: bool = False,\n    show_prompt_asset: bool = False,\n    inverse_ranking: bool = False,\n    level_of_detail: Literal[\"low\", \"medium\", \"high\", \"very high\"] = \"low\",\n    min_responses_per_matchup: int = 3,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n) -&gt; RapidataLeaderboard:\n    \"\"\"\n    Creates a new leaderboard for the benchmark.\n\n    Args:\n        name: The name of the leaderboard. (not shown to the users)\n        instruction: The instruction decides how the models will be evaluated.\n        show_prompt: Whether to show the prompt to the users. (default: False)\n        show_prompt_asset: Whether to show the prompt asset to the users. (only works if the prompt asset is a URL) (default: False)\n        inverse_ranking: Whether to inverse the ranking of the leaderboard. (if the question is inversed, e.g. \"Which video is worse?\")\n        level_of_detail: The level of detail of the leaderboard. This will effect how many comparisons are done per model evaluation. (default: \"low\")\n        min_responses_per_matchup: The minimum number of responses required to be considered for the leaderboard. (default: 3)\n        validation_set_id: The id of the validation set that should be attached to the leaderboard. (default: None)\n        filters: The filters that should be applied to the leaderboard. Will determine who can solve answer in the leaderboard. (default: [])\n        settings: The settings that should be applied to the leaderboard. Will determine the behavior of the tasks on the leaderboard. (default: [])\n    \"\"\"\n    with tracer.start_as_current_span(\"create_leaderboard\"):\n        if not isinstance(min_responses_per_matchup, int):\n            raise ValueError(\"Min responses per matchup must be an integer\")\n\n        if min_responses_per_matchup &lt; 3:\n            raise ValueError(\"Min responses per matchup must be at least 3\")\n\n        logger.info(\n            \"Creating leaderboard %s with instruction %s, show_prompt %s, show_prompt_asset %s, inverse_ranking %s, level_of_detail %s, min_responses_per_matchup %s, validation_set_id %s, filters %s, settings %s\",\n            name,\n            instruction,\n            show_prompt,\n            show_prompt_asset,\n            inverse_ranking,\n            level_of_detail,\n            min_responses_per_matchup,\n            validation_set_id,\n            filters,\n            settings,\n        )\n\n        leaderboard_result = self._openapi_service.leaderboard_api.leaderboard_post(\n            create_leaderboard_model=CreateLeaderboardModel(\n                benchmarkId=self.id,\n                name=name,\n                instruction=instruction,\n                showPrompt=show_prompt,\n                showPromptAsset=show_prompt_asset,\n                isInversed=inverse_ranking,\n                minResponses=min_responses_per_matchup,\n                responseBudget=DetailMapper.get_budget(level_of_detail),\n                validationSetId=validation_set_id,\n                filters=(\n                    [\n                        AndUserFilterModelFiltersInner(filter._to_model())\n                        for filter in filters\n                    ]\n                    if filters\n                    else None\n                ),\n                featureFlags=(\n                    [setting._to_feature_flag() for setting in settings]\n                    if settings\n                    else None\n                ),\n            )\n        )\n\n        assert (\n            leaderboard_result.benchmark_id == self.id\n        ), \"The leaderboard was not created for the correct benchmark.\"\n\n        logger.info(\"Leaderboard created with id %s\", leaderboard_result.id)\n\n        return RapidataLeaderboard(\n            name,\n            instruction,\n            show_prompt,\n            show_prompt_asset,\n            inverse_ranking,\n            leaderboard_result.response_budget,\n            min_responses_per_matchup,\n            self.id,\n            leaderboard_result.id,\n            self._openapi_service,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    name: str,\n    media: list[str],\n    identifiers: list[str] | None = None,\n    prompts: list[str] | None = None,\n) -&gt; None\n</code></pre> <p>Evaluates a model on the benchmark across all leaderboards.</p> <p>prompts or identifiers must be provided to match the media.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model.</p> required <code>media</code> <code>list[str]</code> <p>The generated images/videos that will be used to evaluate the model.</p> required <code>identifiers</code> <code>list[str] | None</code> <p>The identifiers that correspond to the media. The order of the identifiers must match the order of the media.</p> <p>The identifiers that are used must be registered for the benchmark. To see the registered identifiers, use the identifiers property.</p> <code>None</code> <code>prompts</code> <code>list[str] | None</code> <p>The prompts that correspond to the media. The order of the prompts must match the order of the media.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark.py</code> <pre><code>def evaluate_model(\n    self,\n    name: str,\n    media: list[str],\n    identifiers: list[str] | None = None,\n    prompts: list[str] | None = None,\n) -&gt; None:\n    \"\"\"\n    Evaluates a model on the benchmark across all leaderboards.\n\n    prompts or identifiers must be provided to match the media.\n\n    Args:\n        name: The name of the model.\n        media: The generated images/videos that will be used to evaluate the model.\n        identifiers: The identifiers that correspond to the media. The order of the identifiers must match the order of the media.\\n\n            The identifiers that are used must be registered for the benchmark. To see the registered identifiers, use the identifiers property.\n        prompts: The prompts that correspond to the media. The order of the prompts must match the order of the media.\n    \"\"\"\n    with tracer.start_as_current_span(\"evaluate_model\"):\n        if not media:\n            raise ValueError(\"Media must be a non-empty list of strings\")\n\n        if not identifiers and not prompts:\n            raise ValueError(\"Identifiers or prompts must be provided.\")\n\n        if identifiers and prompts:\n            raise ValueError(\n                \"Identifiers and prompts cannot be provided at the same time. Use one or the other.\"\n            )\n\n        if not identifiers:\n            assert prompts is not None\n            identifiers = prompts\n\n        if len(media) != len(identifiers):\n            raise ValueError(\n                \"Media and identifiers/prompts must have the same length\"\n            )\n\n        if not all(identifier in self.identifiers for identifier in identifiers):\n            raise ValueError(\n                \"All identifiers/prompts must be in the registered identifiers/prompts list. To see the registered identifiers/prompts, use the identifiers/prompts property.\"\n            )\n\n        participant_result = self._openapi_service.benchmark_api.benchmark_benchmark_id_participants_post(\n            benchmark_id=self.id,\n            create_benchmark_participant_model=CreateBenchmarkParticipantModel(\n                name=name,\n            ),\n        )\n\n        logger.info(f\"Participant created: {participant_result.participant_id}\")\n\n        participant = BenchmarkParticipant(\n            name, participant_result.participant_id, self._openapi_service\n        )\n\n        with tracer.start_as_current_span(\"upload_media_for_participant\"):\n            logger.info(\n                f\"Uploading {len(media)} media assets to participant {participant.id}\"\n            )\n\n            successful_uploads, failed_uploads = participant.upload_media(\n                media,\n                identifiers,\n            )\n\n            total_uploads = len(media)\n            success_rate = (\n                (len(successful_uploads) / total_uploads * 100)\n                if total_uploads &gt; 0\n                else 0\n            )\n            logger.info(\n                f\"Upload complete: {len(successful_uploads)} successful, {len(failed_uploads)} failed ({success_rate:.1f}% success rate)\"\n            )\n\n            if failed_uploads:\n                logger.error(f\"Failed uploads for media: {failed_uploads}\")\n                logger.warning(\n                    \"Some uploads failed. The model evaluation may be incomplete.\"\n                )\n\n            if len(successful_uploads) == 0:\n                raise RuntimeError(\n                    \"No uploads were successful. The model evaluation will not be completed.\"\n                )\n\n            self._openapi_service.participant_api.participants_participant_id_submit_post(\n                participant_id=participant_result.participant_id\n            )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark/#rapidata.rapidata_client.benchmark.rapidata_benchmark.RapidataBenchmark.view","title":"view","text":"<pre><code>view() -&gt; None\n</code></pre> <p>Views the benchmark.</p> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark.py</code> <pre><code>def view(self) -&gt; None:\n    \"\"\"\n    Views the benchmark.\n    \"\"\"\n    logger.info(\"Opening benchmark page in browser...\")\n    could_open_browser = webbrowser.open(self.__benchmark_page)\n    if not could_open_browser:\n        encoded_url = urllib.parse.quote(\n            self.__benchmark_page, safe=\"%/:=&amp;?~#+!$,;'@()*[]\"\n        )\n        managed_print(\n            Fore.RED\n            + f\"Please open this URL in your browser: '{encoded_url}'\"\n            + Fore.RESET\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager/","title":"Rapidata benchmark manager","text":""},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager/#rapidata.rapidata_client.benchmark.rapidata_benchmark_manager.RapidataBenchmarkManager","title":"RapidataBenchmarkManager","text":"<pre><code>RapidataBenchmarkManager(openapi_service: OpenAPIService)\n</code></pre> <p>A manager for benchmarks.</p> <p>Used to create and retrieve benchmarks.</p> <p>A benchmark is a collection of leaderboards.</p> <p>Parameters:</p> Name Type Description Default <code>openapi_service</code> <code>OpenAPIService</code> <p>The OpenAPIService instance for API interaction.</p> required Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager.py</code> <pre><code>def __init__(self, openapi_service: OpenAPIService):\n    self.__openapi_service = openapi_service\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager/#rapidata.rapidata_client.benchmark.rapidata_benchmark_manager.RapidataBenchmarkManager.create_new_benchmark","title":"create_new_benchmark","text":"<pre><code>create_new_benchmark(\n    name: str,\n    identifiers: Optional[list[str]] = None,\n    prompts: Optional[list[str | None] | list[str]] = None,\n    prompt_assets: Optional[\n        list[str | None] | list[str]\n    ] = None,\n    tags: Optional[\n        list[list[str] | None] | list[list[str]]\n    ] = None,\n) -&gt; RapidataBenchmark\n</code></pre> <p>Creates a new benchmark with the given name, identifiers, prompts, and media assets. Everything is matched up by the indexes of the lists.</p> <p>prompts or identifiers must be provided, as well as prompts or prompt_assets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the benchmark.</p> required <code>identifiers</code> <code>Optional[list[str]]</code> <p>The identifiers of the prompts/assets/tags that will be used to match up the media. If not provided, it will use the prompts as the identifiers.</p> <code>None</code> <code>prompts</code> <code>Optional[list[str | None] | list[str]]</code> <p>The prompts that will be registered for the benchmark.</p> <code>None</code> <code>prompt_assets</code> <code>Optional[list[str | None] | list[str]]</code> <p>The prompt assets that will be registered for the benchmark.</p> <code>None</code> <code>tags</code> <code>Optional[list[list[str] | None] | list[list[str]]]</code> <p>The tags that will be associated with the prompts to use for filtering the leaderboard results. They will NOT be shown to the users.</p> <code>None</code> Example <pre><code>name = \"Example Benchmark\"\nidentifiers = [\"id1\", \"id2\", \"id3\"]\nprompts = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\nprompt_assets = [\"https://assets.rapidata.ai/prompt_1.jpg\", \"https://assets.rapidata.ai/prompt_2.jpg\", \"https://assets.rapidata.ai/prompt_3.jpg\"]\ntags = [[\"tag1\", \"tag2\"], [\"tag2\"], [\"tag2\", \"tag3\"]]\n\nbenchmark = create_new_benchmark(name=name, identifiers=identifiers, prompts=prompts, prompt_assets=prompt_assets, tags=tags)\n</code></pre> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager.py</code> <pre><code>def create_new_benchmark(\n    self,\n    name: str,\n    identifiers: Optional[list[str]] = None,\n    prompts: Optional[list[str | None] | list[str]] = None,\n    prompt_assets: Optional[list[str | None] | list[str]] = None,\n    tags: Optional[list[list[str] | None] | list[list[str]]] = None,\n) -&gt; RapidataBenchmark:\n    \"\"\"\n    Creates a new benchmark with the given name, identifiers, prompts, and media assets.\n    Everything is matched up by the indexes of the lists.\n\n    prompts or identifiers must be provided, as well as prompts or prompt_assets.\n\n    Args:\n        name: The name of the benchmark.\n        identifiers: The identifiers of the prompts/assets/tags that will be used to match up the media. If not provided, it will use the prompts as the identifiers.\n        prompts: The prompts that will be registered for the benchmark.\n        prompt_assets: The prompt assets that will be registered for the benchmark.\n        tags: The tags that will be associated with the prompts to use for filtering the leaderboard results. They will NOT be shown to the users.\n\n    Example:\n        ```python\n        name = \"Example Benchmark\"\n        identifiers = [\"id1\", \"id2\", \"id3\"]\n        prompts = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\n        prompt_assets = [\"https://assets.rapidata.ai/prompt_1.jpg\", \"https://assets.rapidata.ai/prompt_2.jpg\", \"https://assets.rapidata.ai/prompt_3.jpg\"]\n        tags = [[\"tag1\", \"tag2\"], [\"tag2\"], [\"tag2\", \"tag3\"]]\n\n        benchmark = create_new_benchmark(name=name, identifiers=identifiers, prompts=prompts, prompt_assets=prompt_assets, tags=tags)\n        ```\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"RapidataBenchmarkManager.create_new_benchmark\"\n    ):\n        if not isinstance(name, str):\n            raise ValueError(\"Name must be a string.\")\n\n        if prompts and (\n            not isinstance(prompts, list)\n            or not all(\n                isinstance(prompt, str) or prompt is None for prompt in prompts\n            )\n        ):\n            raise ValueError(\"Prompts must be a list of strings or None.\")\n\n        if prompt_assets and (\n            not isinstance(prompt_assets, list)\n            or not all(\n                isinstance(asset, str) or asset is None for asset in prompt_assets\n            )\n        ):\n            raise ValueError(\"Media assets must be a list of strings or None.\")\n\n        if identifiers and (\n            not isinstance(identifiers, list)\n            or not all(isinstance(identifier, str) for identifier in identifiers)\n        ):\n            raise ValueError(\"Identifiers must be a list of strings.\")\n\n        if identifiers:\n            if not len(set(identifiers)) == len(identifiers):\n                raise ValueError(\"Identifiers must be unique.\")\n\n        if tags is not None:\n            if not isinstance(tags, list):\n                raise ValueError(\"Tags must be a list of lists of strings or None.\")\n\n            for tag in tags:\n                if tag is not None and (\n                    not isinstance(tag, list)\n                    or not all(isinstance(item, str) for item in tag)\n                ):\n                    raise ValueError(\n                        \"Tags must be a list of lists of strings or None.\"\n                    )\n\n        if not identifiers and not prompts:\n            raise ValueError(\n                \"At least one of identifiers or prompts must be provided.\"\n            )\n\n        if not prompts and not prompt_assets:\n            raise ValueError(\n                \"At least one of prompts or media assets must be provided.\"\n            )\n\n        if not identifiers:\n            assert prompts is not None\n            if not len(set(prompts)) == len(prompts):\n                raise ValueError(\n                    \"Prompts must be unique. Otherwise use identifiers.\"\n                )\n            if any(prompt is None for prompt in prompts):\n                raise ValueError(\n                    \"Prompts must not be None. Otherwise use identifiers.\"\n                )\n\n            identifiers = cast(list[str], prompts)\n\n        assert identifiers is not None\n\n        expected_length = len(identifiers)\n\n        if not prompts:\n            prompts = cast(list[str | None], [None] * expected_length)\n\n        if not prompt_assets:\n            prompt_assets = cast(list[str | None], [None] * expected_length)\n\n        if not tags:\n            tags = cast(list[list[str] | None], [None] * expected_length)\n\n        # At this point, all variables are guaranteed to be lists, not None\n        assert prompts is not None\n        assert prompt_assets is not None\n        assert tags is not None\n\n        if not (expected_length == len(prompts) == len(prompt_assets) == len(tags)):\n            raise ValueError(\n                \"Identifiers, prompts, media assets, and tags must have the same length or set to None.\"\n            )\n\n        logger.info(\"Creating new benchmark %s\", name)\n\n        benchmark_result = self.__openapi_service.benchmark_api.benchmark_post(\n            create_benchmark_model=CreateBenchmarkModel(\n                name=name,\n            )\n        )\n\n        logger.info(\"Benchmark created with id %s\", benchmark_result.id)\n\n        benchmark = RapidataBenchmark(\n            name, benchmark_result.id, self.__openapi_service\n        )\n\n        for identifier, prompt, asset, tag in zip(\n            identifiers, prompts, prompt_assets, tags\n        ):\n            benchmark.add_prompt(identifier, prompt, asset, tag)\n\n        return benchmark\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager/#rapidata.rapidata_client.benchmark.rapidata_benchmark_manager.RapidataBenchmarkManager.get_benchmark_by_id","title":"get_benchmark_by_id","text":"<pre><code>get_benchmark_by_id(id: str) -&gt; RapidataBenchmark\n</code></pre> <p>Returns a benchmark by its ID.</p> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager.py</code> <pre><code>def get_benchmark_by_id(self, id: str) -&gt; RapidataBenchmark:\n    \"\"\"\n    Returns a benchmark by its ID.\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"RapidataBenchmarkManager.get_benchmark_by_id\"\n    ):\n        benchmark_result = (\n            self.__openapi_service.benchmark_api.benchmark_benchmark_id_get(\n                benchmark_id=id\n            )\n        )\n        return RapidataBenchmark(\n            benchmark_result.name, benchmark_result.id, self.__openapi_service\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager/#rapidata.rapidata_client.benchmark.rapidata_benchmark_manager.RapidataBenchmarkManager.find_benchmarks","title":"find_benchmarks","text":"<pre><code>find_benchmarks(\n    name: str = \"\", amount: int = 10\n) -&gt; list[RapidataBenchmark]\n</code></pre> <p>Returns a list of benchmarks by their name.</p> Source code in <code>src/rapidata/rapidata_client/benchmark/rapidata_benchmark_manager.py</code> <pre><code>def find_benchmarks(\n    self, name: str = \"\", amount: int = 10\n) -&gt; list[RapidataBenchmark]:\n    \"\"\"\n    Returns a list of benchmarks by their name.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataBenchmarkManager.find_benchmarks\"):\n        benchmark_result = self.__openapi_service.benchmark_api.benchmarks_get(\n            QueryModel(\n                page=PageInfo(index=1, size=amount),\n                filter=RootFilter(\n                    filters=[\n                        Filter(\n                            field=\"Name\",\n                            operator=FilterOperator.CONTAINS,\n                            value=name,\n                        )\n                    ]\n                ),\n                sortCriteria=[\n                    SortCriterion(\n                        direction=SortDirection.DESC, propertyName=\"CreatedAt\"\n                    )\n                ],\n            )\n        )\n        return [\n            RapidataBenchmark(benchmark.name, benchmark.id, self.__openapi_service)\n            for benchmark in benchmark_result.items\n        ]\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/","title":"Rapidata leaderboard","text":""},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard","title":"RapidataLeaderboard","text":"<pre><code>RapidataLeaderboard(\n    name: str,\n    instruction: str,\n    show_prompt: bool,\n    show_prompt_asset: bool,\n    inverse_ranking: bool,\n    response_budget: int,\n    min_responses_per_matchup: int,\n    benchmark_id: str,\n    id: str,\n    openapi_service: OpenAPIService,\n)\n</code></pre> <p>An instance of a Rapidata leaderboard.</p> <p>Used to interact with a specific leaderboard in the Rapidata system, such as retrieving prompts and evaluating models.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name that will be used to identify the leaderboard on the overview.</p> required <code>instruction</code> <code>str</code> <p>The instruction that will determine what how the models will be evaluated.</p> required <code>show_prompt</code> <code>bool</code> <p>Whether to show the prompt to the users.</p> required <code>id</code> <code>str</code> <p>The ID of the leaderboard.</p> required <code>openapi_service</code> <code>OpenAPIService</code> <p>The OpenAPIService instance for API interaction.</p> required Source code in <code>src/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    instruction: str,\n    show_prompt: bool,\n    show_prompt_asset: bool,\n    inverse_ranking: bool,\n    response_budget: int,\n    min_responses_per_matchup: int,\n    benchmark_id: str,\n    id: str,\n    openapi_service: OpenAPIService,\n):\n    self.__openapi_service = openapi_service\n    self.__name = name\n    self.__instruction = instruction\n    self.__show_prompt = show_prompt\n    self.__show_prompt_asset = show_prompt_asset\n    self.__inverse_ranking = inverse_ranking\n    self.__response_budget = response_budget\n    self.__min_responses_per_matchup = min_responses_per_matchup\n    self.__benchmark_id = benchmark_id\n    self.id = id\n    self.__leaderboard_page = f\"https://app.{self.__openapi_service.environment}/mri/benchmarks/{self.__benchmark_id}/leaderboard/{self.id}\"\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.level_of_detail","title":"level_of_detail  <code>property</code> <code>writable</code>","text":"<pre><code>level_of_detail: Literal[\n    \"low\", \"medium\", \"high\", \"very high\"\n]\n</code></pre> <p>Returns the level of detail of the leaderboard.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.min_responses_per_matchup","title":"min_responses_per_matchup  <code>property</code> <code>writable</code>","text":"<pre><code>min_responses_per_matchup: int\n</code></pre> <p>Returns the minimum number of responses required to be considered for the leaderboard.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.show_prompt_asset","title":"show_prompt_asset  <code>property</code>","text":"<pre><code>show_prompt_asset: bool\n</code></pre> <p>Returns whether the prompt asset is shown to the users.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.inverse_ranking","title":"inverse_ranking  <code>property</code>","text":"<pre><code>inverse_ranking: bool\n</code></pre> <p>Returns whether the ranking is inverse.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.show_prompt","title":"show_prompt  <code>property</code>","text":"<pre><code>show_prompt: bool\n</code></pre> <p>Returns whether the prompt is shown to the users.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.instruction","title":"instruction  <code>property</code>","text":"<pre><code>instruction: str\n</code></pre> <p>Returns the instruction of the leaderboard.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.name","title":"name  <code>property</code> <code>writable</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the leaderboard.</p>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.get_standings","title":"get_standings","text":"<pre><code>get_standings(\n    tags: Optional[list[str]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Returns the standings of the leaderboard.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[list[str]]</code> <p>The matchups with these tags should be used to create the standings. If tags are None, all matchups will be considered. If tags are empty, no matchups will be considered.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing the standings of the leaderboard.</p> Source code in <code>src/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard.py</code> <pre><code>def get_standings(self, tags: Optional[list[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns the standings of the leaderboard.\n\n    Args:\n        tags: The matchups with these tags should be used to create the standings.\n            If tags are None, all matchups will be considered.\n            If tags are empty, no matchups will be considered.\n\n    Returns:\n        A pandas DataFrame containing the standings of the leaderboard.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataLeaderboard.get_standings\"):\n        participants = self.__openapi_service.leaderboard_api.leaderboard_leaderboard_id_standings_get(\n            leaderboard_id=self.id, tags=tags\n        )\n\n        standings = []\n        for participant in participants.items:\n            standings.append(\n                {\n                    \"name\": participant.name,\n                    \"wins\": participant.wins,\n                    \"total_matches\": participant.total_matches,\n                    \"score\": (\n                        round(participant.score, 2)\n                        if participant.score is not None\n                        else None\n                    ),\n                }\n            )\n\n        return pd.DataFrame(standings)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard/#rapidata.rapidata_client.benchmark.leaderboard.rapidata_leaderboard.RapidataLeaderboard.view","title":"view","text":"<pre><code>view() -&gt; None\n</code></pre> <p>Views the leaderboard.</p> Source code in <code>src/rapidata/rapidata_client/benchmark/leaderboard/rapidata_leaderboard.py</code> <pre><code>def view(self) -&gt; None:\n    \"\"\"\n    Views the leaderboard.\n    \"\"\"\n    logger.info(\"Opening leaderboard page in browser...\")\n    could_open_browser = webbrowser.open(self.__leaderboard_page)\n    if not could_open_browser:\n        encoded_url = urllib.parse.quote(\n            self.__leaderboard_page, safe=\"%/:=&amp;?~#+!$,;'@()*[]\"\n        )\n        managed_print(\n            Fore.RED\n            + f\"Please open this URL in your browser: '{encoded_url}'\"\n            + Fore.RESET\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/logger/","title":"Logger","text":""},{"location":"reference/rapidata/rapidata_client/config/logger/#rapidata.rapidata_client.config.logger.LoggerProtocol","title":"LoggerProtocol","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol that defines the logger interface for type checking.</p>"},{"location":"reference/rapidata/rapidata_client/config/logger/#rapidata.rapidata_client.config.logger.RapidataLogger","title":"RapidataLogger","text":"<pre><code>RapidataLogger(name: str = 'rapidata')\n</code></pre> <p>Logger implementation that updates when the configuration changes.</p> Source code in <code>src/rapidata/rapidata_client/config/logger.py</code> <pre><code>def __init__(self, name: str = \"rapidata\"):\n    self._logger = logging.getLogger(name)\n    self._otlp_initialized = False\n    self._otlp_handler = None\n\n    # Register this logger to receive configuration updates\n    register_config_handler(self._handle_config_update)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/logging_config/","title":"Logging config","text":""},{"location":"reference/rapidata/rapidata_client/config/logging_config/#rapidata.rapidata_client.config.logging_config.LoggingConfig","title":"LoggingConfig","text":"<pre><code>LoggingConfig(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Holds the configuration for the logging process.</p> <p>Attributes:</p> Name Type Description <code>level</code> <code>str</code> <p>The logging level. Defaults to \"WARNING\".</p> <code>log_file</code> <code>str | None</code> <p>The logging file. Defaults to None.</p> <code>format</code> <code>str</code> <p>The logging format. Defaults to \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\".</p> <code>silent_mode</code> <code>bool</code> <p>Whether to disable the prints and progress bars. Does NOT affect the logging. Defaults to False.</p> <code>enable_otlp</code> <code>bool</code> <p>Whether to enable OpenTelemetry trace logs. Defaults to True.</p> Source code in <code>src/rapidata/rapidata_client/config/logging_config.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    self._notify_handlers()\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/logging_config/#rapidata.rapidata_client.config.logging_config.register_config_handler","title":"register_config_handler","text":"<pre><code>register_config_handler(\n    handler: ConfigUpdateHandler,\n) -&gt; None\n</code></pre> <p>Register a handler to be called when the logging configuration updates.</p> Source code in <code>src/rapidata/rapidata_client/config/logging_config.py</code> <pre><code>def register_config_handler(handler: ConfigUpdateHandler) -&gt; None:\n    \"\"\"Register a handler to be called when the logging configuration updates.\"\"\"\n    _config_handlers.append(handler)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/logging_config/#rapidata.rapidata_client.config.logging_config.unregister_config_handler","title":"unregister_config_handler","text":"<pre><code>unregister_config_handler(\n    handler: ConfigUpdateHandler,\n) -&gt; None\n</code></pre> <p>Unregister a previously registered handler.</p> Source code in <code>src/rapidata/rapidata_client/config/logging_config.py</code> <pre><code>def unregister_config_handler(handler: ConfigUpdateHandler) -&gt; None:\n    \"\"\"Unregister a previously registered handler.\"\"\"\n    if handler in _config_handlers:\n        _config_handlers.remove(handler)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/managed_print/","title":"Managed print","text":""},{"location":"reference/rapidata/rapidata_client/config/order_config/","title":"Order config","text":""},{"location":"reference/rapidata/rapidata_client/config/order_config/#rapidata.rapidata_client.config.order_config.OrderConfig","title":"OrderConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Holds the configuration for the order process.</p> <p>Attributes:</p> Name Type Description <code>minOrderDatapointsForValidation</code> <code>int</code> <p>The minimum number of datapoints required so that an automatic validationset gets created if no recommended was found. Defaults to 50.</p> <code>autoValidationSetSize</code> <code>int</code> <p>The maximum size of the auto-generated validation set. Defaults to 20.</p>"},{"location":"reference/rapidata/rapidata_client/config/rapidata_config/","title":"Rapidata config","text":""},{"location":"reference/rapidata/rapidata_client/config/rapidata_config/#rapidata.rapidata_client.config.rapidata_config.RapidataConfig","title":"RapidataConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Holds the configuration for the Rapidata client.</p> <p>To adjust the configurations used, you can modify the <code>rapidata_config</code> object.</p> <p>Attributes:</p> Name Type Description <code>enableBetaFeatures</code> <code>bool</code> <p>Whether to enable beta features. Defaults to False.</p> <code>upload</code> <code>UploadConfig</code> <p>The configuration for the upload process. Such as the maximum number of worker threads for processing media paths and the maximum number of retries for failed uploads.</p> <code>order</code> <code>OrderConfig</code> <p>The configuration for the order process. Such as the minimum number of datapoints required so that an automatic validationset gets created if no recommended was found.</p> <code>logging</code> <code>LoggingConfig</code> <p>The configuration for the logging process. Such as the logging level and the logging file.</p> Example <pre><code>from rapidata import rapidata_config\nrapidata_config.upload.maxUploadWorkers = 20\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/tracer/","title":"Tracer","text":""},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.TracerProtocol","title":"TracerProtocol","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol that defines the tracer interface for type checking.</p>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.NoOpSpan","title":"NoOpSpan","text":"<p>A no-op span that does nothing when tracing is disabled.</p>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.NoOpTracer","title":"NoOpTracer","text":"<p>A no-op tracer that returns no-op spans when tracing is disabled.</p>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.SpanContextManagerWrapper","title":"SpanContextManagerWrapper","text":"<pre><code>SpanContextManagerWrapper(\n    context_manager: Any, session_id: str | None\n)\n</code></pre> <p>Wrapper for span context managers to add session_id on enter.</p> Source code in <code>src/rapidata/rapidata_client/config/tracer.py</code> <pre><code>def __init__(self, context_manager: Any, session_id: str | None):\n    self._context_manager = context_manager\n    self.session_id = session_id\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.RapidataTracer","title":"RapidataTracer","text":"<pre><code>RapidataTracer(name: str = __name__)\n</code></pre> <p>Tracer implementation that updates when the configuration changes.</p> Source code in <code>src/rapidata/rapidata_client/config/tracer.py</code> <pre><code>def __init__(self, name: str = __name__):\n    self._name = name\n    self._otlp_initialized = False\n    self._tracer_provider = None\n    self._real_tracer = None\n    self._no_op_tracer = NoOpTracer()\n    self._enabled = True  # Default to enabled\n    self.session_id: str | None = None\n\n    # Register this tracer to receive configuration updates\n    register_config_handler(self._handle_config_update)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.RapidataTracer.start_span","title":"start_span","text":"<pre><code>start_span(name: str, *args, **kwargs) -&gt; Any\n</code></pre> <p>Start a span, or return a no-op span if tracing is disabled.</p> Source code in <code>src/rapidata/rapidata_client/config/tracer.py</code> <pre><code>def start_span(self, name: str, *args, **kwargs) -&gt; Any:\n    \"\"\"Start a span, or return a no-op span if tracing is disabled.\"\"\"\n    if self._enabled and self._real_tracer:\n        span = self._real_tracer.start_span(name, *args, **kwargs)\n        return self._add_session_id_to_span(span)\n    return self._no_op_tracer.start_span(name, *args, **kwargs)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/tracer/#rapidata.rapidata_client.config.tracer.RapidataTracer.start_as_current_span","title":"start_as_current_span","text":"<pre><code>start_as_current_span(name: str, *args, **kwargs) -&gt; Any\n</code></pre> <p>Start a span as current, or return a no-op span if tracing is disabled.</p> Source code in <code>src/rapidata/rapidata_client/config/tracer.py</code> <pre><code>def start_as_current_span(self, name: str, *args, **kwargs) -&gt; Any:\n    \"\"\"Start a span as current, or return a no-op span if tracing is disabled.\"\"\"\n    if self._enabled and self._real_tracer:\n        context_manager = self._real_tracer.start_as_current_span(\n            name, *args, **kwargs\n        )\n        return SpanContextManagerWrapper(context_manager, self.session_id)\n    return self._no_op_tracer.start_as_current_span(name, *args, **kwargs)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/config/upload_config/","title":"Upload config","text":""},{"location":"reference/rapidata/rapidata_client/config/upload_config/#rapidata.rapidata_client.config.upload_config.UploadConfig","title":"UploadConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Holds the configuration for the upload process.</p> <p>Attributes:</p> Name Type Description <code>maxWorkers</code> <code>int</code> <p>The maximum number of worker threads for processing media paths. Defaults to 10.</p> <code>maxRetries</code> <code>int</code> <p>The maximum number of retries for failed uploads. Defaults to 3.</p>"},{"location":"reference/rapidata/rapidata_client/datapoints/assets/constants/","title":"Constants","text":""},{"location":"reference/rapidata/rapidata_client/demographic/demographic_manager/","title":"Demographic manager","text":""},{"location":"reference/rapidata/rapidata_client/exceptions/failed_upload_exception/","title":"Failed upload exception","text":""},{"location":"reference/rapidata/rapidata_client/exceptions/failed_upload_exception/#rapidata.rapidata_client.exceptions.failed_upload_exception.FailedUploadException","title":"FailedUploadException","text":"<pre><code>FailedUploadException(\n    dataset: RapidataDataset,\n    order: RapidataOrder,\n    failed_uploads: list[Datapoint],\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Custom error class for Failed Uploads to the Rapidata order.</p> Source code in <code>src/rapidata/rapidata_client/exceptions/failed_upload_exception.py</code> <pre><code>def __init__(\n    self,\n    dataset: RapidataDataset,\n    order: RapidataOrder,\n    failed_uploads: list[Datapoint],\n):\n    self.dataset = dataset\n    self.order = order\n    self.failed_uploads = failed_uploads\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/age_filter/","title":"Age filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/age_filter/#rapidata.rapidata_client.filter.age_filter.AgeFilter","title":"AgeFilter","text":"<pre><code>AgeFilter(age_groups: list[AgeGroup])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>AgeFilter Class</p> <p>Can be used to filter who to target based on age groups.</p> <p>Parameters:</p> Name Type Description Default <code>age_groups</code> <code>list[AgeGroup]</code> <p>List of age groups to filter by.</p> required Source code in <code>src/rapidata/rapidata_client/filter/age_filter.py</code> <pre><code>def __init__(self, age_groups: list[AgeGroup]):\n    super().__init__(age_groups=age_groups)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/and_filter/","title":"And filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/and_filter/#rapidata.rapidata_client.filter.and_filter.AndFilter","title":"AndFilter","text":"<pre><code>AndFilter(filters: list[RapidataFilter])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>A filter that combines multiple filters with a logical AND operation. This class implements a logical AND operation on a list of filters, where the condition is met if all of the filters' conditions are met.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>list[RapidataFilter]</code> <p>A list of filters to be combined with AND.</p> required Example <pre><code>from rapidata import AndFilter, LanguageFilter, CountryFilter\n\nAndFilter([LanguageFilter([\"en\"]), CountryFilter([\"US\"])])\n</code></pre> <p>This will match users who have their phone set to English AND are located in the United States.</p> Source code in <code>src/rapidata/rapidata_client/filter/and_filter.py</code> <pre><code>def __init__(self, filters: list[RapidataFilter]):\n    super().__init__(filters=filters)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/campaign_filter/","title":"Campaign filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/campaign_filter/#rapidata.rapidata_client.filter.campaign_filter.CampaignFilter","title":"CampaignFilter","text":"<pre><code>CampaignFilter(campaign_ids: list[str])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>CampaignFilter Class</p> <p>Can be used to filter who to target based on campaign IDs.</p> <p>This filter can only be used when directly in contact with Rapidata.</p> <p>Parameters:</p> Name Type Description Default <code>campaign_ids</code> <code>list[str]</code> <p>List of campaign IDs to filter by.</p> required Source code in <code>src/rapidata/rapidata_client/filter/campaign_filter.py</code> <pre><code>def __init__(self, campaign_ids: list[str]):\n    super().__init__(campaign_ids=campaign_ids)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/country_filter/","title":"Country filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/country_filter/#rapidata.rapidata_client.filter.country_filter.CountryFilter","title":"CountryFilter","text":"<pre><code>CountryFilter(country_codes: list[str])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>CountryFilter Class</p> <p>Can be used to filter who to target based on country codes.</p> <p>Parameters:</p> Name Type Description Default <code>country_codes</code> <code>list[str]</code> <p>List of country codes (capitalized) to filter by.</p> required Source code in <code>src/rapidata/rapidata_client/filter/country_filter.py</code> <pre><code>def __init__(self, country_codes: list[str]):\n    super().__init__(country_codes=country_codes)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/custom_filter/","title":"Custom filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/custom_filter/#rapidata.rapidata_client.filter.custom_filter.CustomFilter","title":"CustomFilter","text":"<pre><code>CustomFilter(identifier: str, values: list[str])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>CustomFilter Class</p> <p>Can be used to filter who to target based on custom filters.</p> <p>Ought to be used with contact to Rapidata.</p> <p>Warning: If identifier does not exist, order will not get any responses.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Identifier of the custom filter.</p> required <code>values</code> <code>list[str]</code> <p>List of values to filter by.</p> required Source code in <code>src/rapidata/rapidata_client/filter/custom_filter.py</code> <pre><code>def __init__(self, identifier: str, values: list[str]):\n    super().__init__(identifier=identifier, values=values)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/gender_filter/","title":"Gender filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/gender_filter/#rapidata.rapidata_client.filter.gender_filter.GenderFilter","title":"GenderFilter","text":"<pre><code>GenderFilter(genders: list[Gender])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>GenderFilter Class</p> <p>Can be used to filter who to target based on their gender.</p> <p>Parameters:</p> Name Type Description Default <code>genders</code> <code>list[Gender]</code> <p>List of genders to filter by.</p> required Source code in <code>src/rapidata/rapidata_client/filter/gender_filter.py</code> <pre><code>def __init__(self, genders: list[Gender]):\n    super().__init__(genders=genders)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/language_filter/","title":"Language filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/language_filter/#rapidata.rapidata_client.filter.language_filter.LanguageFilter","title":"LanguageFilter","text":"<pre><code>LanguageFilter(language_codes: list[str])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>LanguageFilter Class</p> <p>Can be used to filter who to target based on language codes.</p> <p>Parameters:</p> Name Type Description Default <code>language_codes</code> <code>list[str]</code> <p>List of language codes to filter by.</p> required Example <p><pre><code>LanguageFilter([\"en\", \"de\"])\n</code></pre> This will limit the order to be shown to only people who have their phone set to english or german</p> Source code in <code>src/rapidata/rapidata_client/filter/language_filter.py</code> <pre><code>def __init__(self, language_codes: list[str]):\n    super().__init__(language_codes=language_codes)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/new_user_filter/","title":"New user filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/new_user_filter/#rapidata.rapidata_client.filter.new_user_filter.NewUserFilter","title":"NewUserFilter","text":"<p>               Bases: <code>RapidataFilter</code></p> <p>NewUserFilter Class</p> <p>Can be used to filter new users.</p>"},{"location":"reference/rapidata/rapidata_client/filter/not_filter/","title":"Not filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/not_filter/#rapidata.rapidata_client.filter.not_filter.NotFilter","title":"NotFilter","text":"<pre><code>NotFilter(filter: RapidataFilter)\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>A filter that negates another filter's condition. This class implements a logical NOT operation on a given filter, inverting its results.</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>RapidataFilter</code> <p>The filter whose condition should be negated.</p> required Example <pre><code>from rapidata import NotFilter, LanguageFilter\n\nNotFilter(LanguageFilter([\"en\"]))\n</code></pre> <p>This will limit the order to be shown to only people who have their phone set to a language other than English.</p> Source code in <code>src/rapidata/rapidata_client/filter/not_filter.py</code> <pre><code>def __init__(self, filter: RapidataFilter):\n    super().__init__(filter=filter)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/or_filter/","title":"Or filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/or_filter/#rapidata.rapidata_client.filter.or_filter.OrFilter","title":"OrFilter","text":"<pre><code>OrFilter(filters: list[RapidataFilter])\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>A filter that combines multiple filters with a logical OR operation. This class implements a logical OR operation on a list of filters, where the condition is met if any of the filters' conditions are met.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>list[RapidataFilter]</code> <p>A list of filters to be combined with OR.</p> required Example <pre><code>from rapidata import OrFilter, LanguageFilter, CountryFilter\n\nOrFilter([LanguageFilter([\"en\"]), CountryFilter([\"US\"])])\n</code></pre> <p>This will match users who either have their phone set to English OR are located in the United States.</p> Source code in <code>src/rapidata/rapidata_client/filter/or_filter.py</code> <pre><code>def __init__(self, filters: list[RapidataFilter]):\n    super().__init__(filters=filters)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/rapidata_filters/","title":"\ud83d\udd0d Filters","text":""},{"location":"reference/rapidata/rapidata_client/filter/rapidata_filters/#rapidata.rapidata_client.filter.rapidata_filters.RapidataFilters","title":"RapidataFilters","text":"<p>RapidataFilters Classes</p> <p>These filters can be added to the order to specifically target a certain group of users.</p> <p>Note that adding multiple filters to the same order will result in a logical AND operation between the filters.</p> Warning <p>This might significantly slow down the number of responses you receive.</p> <p>Attributes:</p> Name Type Description <code>user_score</code> <code>UserScoreFilter</code> <p>Filters for users with a specific user score.</p> <code>age</code> <code>AgeFilter</code> <p>Filters for users with a specific age.</p> <code>country</code> <code>CountryFilter</code> <p>Filters for users with a specific country.</p> <code>gender</code> <code>GenderFilter</code> <p>Filters for users with a specific gender.</p> <code>language</code> <code>LanguageFilter</code> <p>Filters for users with a specific language.</p> <code>not_filter</code> <code>NotFilter</code> <p>Inverts the filter.</p> <code>or_filter</code> <code>OrFilter</code> <p>Combines multiple filters with a logical OR operation.</p> <code>and_filter</code> <code>AndFilter</code> <p>Combines multiple filters with a logical AND operation.</p> Example <pre><code>from rapidata import CountryFilter, LanguageFilter\nfilters=[CountryFilter([\"US\", \"DE\"]), LanguageFilter([\"en\"])]\n</code></pre> <p>This ensures the order is only shown to users in the US and Germany whose phones are set to English.</p> Info <p>The OR, AND and NOT filter support the |, &amp; and ~ operators respectively. The AND is additionally given by the elements in the list.</p> <pre><code>from rapidata import AgeFilter, LanguageFilter, CountryFilter\nfilters=[~AgeFilter([AgeGroup.UNDER_18]), CountryFilter([\"US\"]) | (CountryFilter([\"CA\"]) &amp; LanguageFilter([\"en\"]))]\n</code></pre> <p>This would return users who are not under 18 years old and are from the US or who are from Canada and whose phones are set to English.</p>"},{"location":"reference/rapidata/rapidata_client/filter/response_count_filter/","title":"Response count filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/response_count_filter/#rapidata.rapidata_client.filter.response_count_filter.ResponseCountFilter","title":"ResponseCountFilter","text":"<pre><code>ResponseCountFilter(\n    response_count: int,\n    dimension: str,\n    operator: ComparisonOperator,\n)\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>ResponseCountFilter Class Can be used to filter users based on the number of responses they have given on validation tasks with the specified dimension.</p> <pre><code>response_count (int): The number of user responses to filter by.\ndimension (str): The dimension to apply the filter on (e.g. \"default\", \"electrical\", etc.).\noperator (str): The comparison operator to use. Must be one of:\n    - ComparisonOperator.EQUAL\n    - ComparisonOperator.NOTEQUAL\n    - ComparisonOperator.LESSTHAN\n    - ComparisonOperator.LESSTHANOREQUAL\n    - ComparisonOperator.GREATERTHAN\n    - ComparisonOperator.GREATERTHANOREQUAL\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>response_count</code> is not an integer.</p> <code>ValueError</code> <p>If <code>dimension</code> is not a string.</p> <code>ValueError</code> <p>If <code>operator</code> is not a string or not one of the allowed values.</p> Example <p><pre><code>from rapidata import ResponseCountFilter\n\nfilter = ResponseCountFilter(response_count=10, dimension=\"electrical\", operator=ComparisonOperator.GREATERTHAN)\n</code></pre> This will filter users who have a response count greater than 10 for the \"electrical\" dimension.</p> Source code in <code>src/rapidata/rapidata_client/filter/response_count_filter.py</code> <pre><code>def __init__(\n    self, response_count: int, dimension: str, operator: ComparisonOperator\n):\n    super().__init__(\n        response_count=response_count, dimension=dimension, operator=operator\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/user_score_filter/","title":"User score filter","text":""},{"location":"reference/rapidata/rapidata_client/filter/user_score_filter/#rapidata.rapidata_client.filter.user_score_filter.UserScoreFilter","title":"UserScoreFilter","text":"<pre><code>UserScoreFilter(\n    lower_bound: float,\n    upper_bound: float,\n    dimension: str | None = None,\n)\n</code></pre> <p>               Bases: <code>RapidataFilter</code>, <code>BaseModel</code></p> <p>UserScoreFilter Class</p> <p>Can be used to filter who to target based on their user score.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bound</code> <code>float</code> <p>The lower bound of the user score.</p> required <code>upper_bound</code> <code>float</code> <p>The upper bound of the user score.</p> required <code>dimension</code> <code>str</code> <p>The dimension of the userScore to be considerd for the filter.</p> <code>None</code> Example <p><pre><code>UserScoreFilter(0.5, 0.9)\n</code></pre> This will only show the order to users that have a UserScore of &gt;=0.5 and &lt;=0.9</p> Source code in <code>src/rapidata/rapidata_client/filter/user_score_filter.py</code> <pre><code>def __init__(\n    self, lower_bound: float, upper_bound: float, dimension: str | None = None\n):\n    super().__init__(\n        lower_bound=lower_bound, upper_bound=upper_bound, dimension=dimension\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/filter/models/age_group/","title":"Age group","text":""},{"location":"reference/rapidata/rapidata_client/filter/models/age_group/#rapidata.rapidata_client.filter.models.age_group.AgeGroup","title":"AgeGroup","text":"<p>               Bases: <code>Enum</code></p> <p>AgeGroup Enum</p> <p>Represents the age group of a user. Used to filter who to target based on age groups.</p> <p>Attributes:</p> Name Type Description <code>UNDER_18</code> <code>AgeGroup</code> <p>Represents the age group of users under 18.</p> <code>BETWEEN_18_29</code> <code>AgeGroup</code> <p>Represents the age group of users between 18 and 29.</p> <code>BETWEEN_30_39</code> <code>AgeGroup</code> <p>Represents the age group of users between 30 and 39.</p> <code>BETWEEN_40_49</code> <code>AgeGroup</code> <p>Represents the age group of users between 40 and 49.</p> <code>BETWEEN_50_64</code> <code>AgeGroup</code> <p>Represents the age group of users between 50 and 64.</p> <code>OVER_65</code> <code>AgeGroup</code> <p>Represents the age group of users over 65.</p>"},{"location":"reference/rapidata/rapidata_client/filter/models/gender/","title":"Gender","text":""},{"location":"reference/rapidata/rapidata_client/filter/models/gender/#rapidata.rapidata_client.filter.models.gender.Gender","title":"Gender","text":"<p>               Bases: <code>Enum</code></p> <p>Gender Enum</p> <p>Represents the gender of a user. Used to filter who to target based on genders.</p> <p>Attributes:</p> Name Type Description <code>MALE</code> <code>Gender</code> <p>Represents the Male gender.</p> <code>FEMALE</code> <code>Gender</code> <p>Represents the Female gender.</p> <code>OTHER</code> <code>Gender</code> <p>Represents any other gender.</p>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/","title":"Rapidata order","text":""},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder","title":"RapidataOrder","text":"<pre><code>RapidataOrder(\n    name: str,\n    order_id: str,\n    openapi_service: OpenAPIService,\n)\n</code></pre> <p>An instance of a Rapidata order.</p> <p>Used to interact with a specific order in the Rapidata system, such as starting, pausing, and retrieving results.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>order_id</code> <code>str</code> <p>The ID of the order.</p> required <code>openapi_service</code> <code>OpenAPIService</code> <p>The OpenAPIService instance for API interaction.</p> required Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    order_id: str,\n    openapi_service: OpenAPIService,\n):\n    self.id = order_id\n    self.name = name\n    self.__created_at: datetime | None = None\n    self._openapi_service = openapi_service\n    self.__workflow_id: str = \"\"\n    self.__campaign_id: str = \"\"\n    self.__pipeline_id: str = \"\"\n    self._max_retries = 10\n    self._retry_delay = 2\n    self.order_details_page = (\n        f\"https://app.{self._openapi_service.environment}/order/detail/{self.id}\"\n    )\n    logger.debug(\"RapidataOrder initialized\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.created_at","title":"created_at  <code>property</code>","text":"<pre><code>created_at: datetime\n</code></pre> <p>Returns the creation date of the order.</p>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.run","title":"run","text":"<pre><code>run() -&gt; RapidataOrder\n</code></pre> <p>Runs the order to start collecting responses.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def run(self) -&gt; \"RapidataOrder\":\n    \"\"\"Runs the order to start collecting responses.\"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.run\"):\n        logger.info(\"Starting order '%s'\", self)\n        self._openapi_service.order_api.order_order_id_submit_post(\n            self.id, SubmitOrderModel(ignoreFailedDatapoints=True)\n        )\n        logger.debug(\"Order '%s' has been started.\", self)\n        managed_print(\n            f\"Order '{self.name}' is now viewable under: {self.order_details_page}\"\n        )\n        return self\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.pause","title":"pause","text":"<pre><code>pause() -&gt; None\n</code></pre> <p>Pauses the order.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def pause(self) -&gt; None:\n    \"\"\"Pauses the order.\"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.pause\"):\n        logger.info(\"Pausing order '%s'\", self)\n        self._openapi_service.order_api.order_order_id_pause_post(self.id)\n        logger.debug(\"Order '%s' has been paused.\", self)\n        managed_print(f\"Order '{self}' has been paused.\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.unpause","title":"unpause","text":"<pre><code>unpause() -&gt; None\n</code></pre> <p>Unpauses/resumes the order.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def unpause(self) -&gt; None:\n    \"\"\"Unpauses/resumes the order.\"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.unpause\"):\n        logger.info(\"Unpausing order '%s'\", self)\n        self._openapi_service.order_api.order_order_id_resume_post(self.id)\n        logger.debug(\"Order '%s' has been unpaused.\", self)\n        managed_print(f\"Order '{self}' has been unpaused.\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the order.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Deletes the order.\"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.delete\"):\n        logger.info(\"Deleting order '%s'\", self)\n        self._openapi_service.order_api.order_order_id_delete(self.id)\n        logger.debug(\"Order '%s' has been deleted.\", self)\n        managed_print(f\"Order '{self}' has been deleted.\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.get_status","title":"get_status","text":"<pre><code>get_status() -&gt; str\n</code></pre> <p>Gets the status of the order.</p> States <p>Created: The order has been created but not started yet.</p> <p>Preview: The order has been set up and ready but not collecting responses yet.</p> <p>Submitted: The order has been submitted and is being reviewed.</p> <p>ManualReview: The order is in manual review - something went wrong with the automatic approval.</p> <p>Processing: The order is actively being processed.</p> <p>Paused: The order has been paused.</p> <p>Completed: The order has been completed.</p> <p>Failed: The order has failed.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def get_status(self) -&gt; str:\n    \"\"\"\n    Gets the status of the order.\n\n    States:\n        Created: The order has been created but not started yet.\\n\n        Preview: The order has been set up and ready but not collecting responses yet.\\n\n        Submitted: The order has been submitted and is being reviewed.\\n\n        ManualReview: The order is in manual review - something went wrong with the automatic approval.\\n\n        Processing: The order is actively being processed.\\n\n        Paused: The order has been paused.\\n\n        Completed: The order has been completed.\\n\n        Failed: The order has failed.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.get_status\"):\n        return self._openapi_service.order_api.order_order_id_get(self.id).state\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.display_progress_bar","title":"display_progress_bar","text":"<pre><code>display_progress_bar(refresh_rate: int = 5) -&gt; None\n</code></pre> <p>Displays a progress bar for the order processing using tqdm.</p> <p>Parameters:</p> Name Type Description Default <code>refresh_rate</code> <code>int</code> <p>How often to refresh the progress bar, in seconds.</p> <code>5</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def display_progress_bar(self, refresh_rate: int = 5) -&gt; None:\n    \"\"\"\n    Displays a progress bar for the order processing using tqdm.\n\n    Args:\n        refresh_rate: How often to refresh the progress bar, in seconds.\n    \"\"\"\n    if refresh_rate &lt; 1:\n        raise ValueError(\"refresh_rate must be at least 1\")\n\n    if self.get_status() == OrderState.CREATED:\n        raise Exception(\"Order has not been started yet. Please start it first.\")\n\n    while self.get_status() == OrderState.SUBMITTED:\n        managed_print(\n            f\"Order '{self}' is submitted and being reviewed. Standby...\", end=\"\\r\"\n        )\n        sleep(1)\n\n    if self.get_status() == OrderState.MANUALREVIEW:\n        raise Exception(\n            f\"Order '{self}' is in manual review. It might take some time to start. To speed up the process, contact support (info@rapidata.ai).\\nOnce started, run this method again to display the progress bar.\"\n        )\n\n    with tqdm(\n        total=100,\n        desc=\"Processing order\",\n        unit=\"%\",\n        bar_format=\"{desc}: {percentage:3.0f}%|{bar}| completed [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        disable=rapidata_config.logging.silent_mode,\n    ) as pbar:\n        last_percentage = 0\n        while True:\n            current_percentage = self._workflow_progress.completion_percentage\n            if current_percentage &gt; last_percentage:\n                pbar.update(current_percentage - last_percentage)\n                last_percentage = current_percentage\n\n            if current_percentage &gt;= 100:\n                break\n\n            sleep(refresh_rate)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.get_results","title":"get_results","text":"<pre><code>get_results(\n    preliminary_results: bool = False,\n) -&gt; RapidataResults\n</code></pre> <p>Gets the results of the order. If the order is still processing, this method will block until the order is completed and then return the results.</p> <p>Parameters:</p> Name Type Description Default <code>preliminary_results</code> <code>bool</code> <p>If True, returns the preliminary results of the order. Defaults to False. Note that preliminary results are not final and may not contain all the datapoints &amp; responses. Only the onese that are already available.</p> <code>False</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def get_results(self, preliminary_results: bool = False) -&gt; RapidataResults:\n    \"\"\"\n    Gets the results of the order.\n    If the order is still processing, this method will block until the order is completed and then return the results.\n\n    Args:\n        preliminary_results: If True, returns the preliminary results of the order. Defaults to False.\n            Note that preliminary results are not final and may not contain all the datapoints &amp; responses. Only the onese that are already available.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrder.get_results\"):\n        logger.info(\"Getting results for order '%s'...\", self)\n        if preliminary_results and self.get_status() not in [OrderState.COMPLETED]:\n            return self.__get_preliminary_results()\n\n        elif preliminary_results and self.get_status() in [OrderState.COMPLETED]:\n            managed_print(\"Order is already completed. Returning final results.\")\n\n        while (state := self.get_status()) not in [\n            OrderState.COMPLETED,\n            OrderState.PAUSED,\n            OrderState.MANUALREVIEW,\n            OrderState.FAILED,\n        ]:\n            sleep(5)\n            logger.debug(\n                \"Order '%s' is in state %s not yet completed. Waiting...\",\n                self,\n                state,\n            )\n\n        try:\n            return RapidataResults(\n                json.loads(\n                    self._openapi_service.order_api.order_order_id_download_results_get(\n                        order_id=self.id\n                    )\n                )\n            )\n        except (ApiException, json.JSONDecodeError) as e:\n            raise Exception(f\"Failed to get order results: {str(e)}\") from e\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.view","title":"view","text":"<pre><code>view() -&gt; None\n</code></pre> <p>Opens the order details page in the browser.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the order is not in processing state.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def view(self) -&gt; None:\n    \"\"\"\n    Opens the order details page in the browser.\n\n    Raises:\n        Exception: If the order is not in processing state.\n    \"\"\"\n    logger.info(\"Opening order details page in browser...\")\n    could_open_browser = webbrowser.open(self.order_details_page)\n    if not could_open_browser:\n        encoded_url = urllib.parse.quote(\n            self.order_details_page, safe=\"%/:=&amp;?~#+!$,;'@()*[]\"\n        )\n        managed_print(\n            Fore.RED\n            + f\"Please open this URL in your browser: '{encoded_url}'\"\n            + Fore.RESET\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order/#rapidata.rapidata_client.order.rapidata_order.RapidataOrder.preview","title":"preview","text":"<pre><code>preview() -&gt; None\n</code></pre> <p>Opens a preview of the order in the browser.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the order is not in processing state.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order.py</code> <pre><code>def preview(self) -&gt; None:\n    \"\"\"\n    Opens a preview of the order in the browser.\n\n    Raises:\n        Exception: If the order is not in processing state.\n    \"\"\"\n    logger.info(\"Opening order preview in browser...\")\n    if self.get_status() == OrderState.CREATED:\n        logger.info(\"Order is still in state created. Setting it to preview.\")\n        self._openapi_service.order_api.order_order_id_preview_post(\n            self.id, PreviewOrderModel(ignoreFailedDatapoints=True)\n        )\n        logger.info(\"Order is now in preview state.\")\n\n    campaign_id = self.__get_campaign_id()\n    auth_url = f\"https://app.{self._openapi_service.environment}/order/detail/{self.id}/preview?campaignId={campaign_id}\"\n    could_open_browser = webbrowser.open(auth_url)\n    if not could_open_browser:\n        encoded_url = urllib.parse.quote(auth_url, safe=\"%/:=&amp;?~#+!$,;'@()*[]\")\n        managed_print(\n            Fore.RED\n            + f\"Please open this URL in your browser: '{encoded_url}'\"\n            + Fore.RESET\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/","title":"Rapidata order manager","text":""},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager","title":"RapidataOrderManager","text":"<pre><code>RapidataOrderManager(openapi_service: OpenAPIService)\n</code></pre> <p>Handels everything regarding the orders from creation to retrieval.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>RapidataFilters</code> <p>The RapidataFilters instance.</p> <code>settings</code> <code>RapidataSettings</code> <p>The RapidataSettings instance.</p> <code>selections</code> <code>RapidataSelections</code> <p>The RapidataSelections instance.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def __init__(self, openapi_service: OpenAPIService):\n    self.__openapi_service = openapi_service\n    self.filters = RapidataFilters\n    self.settings = RapidataSettings\n    self.selections = RapidataSelections\n    self.__priority: int | None = None\n    self.__sticky_state: StickyStateLiteral | None = None\n    self.__asset_uploader = AssetUploader(openapi_service)\n    logger.debug(\"RapidataOrderManager initialized\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_classification_order","title":"create_classification_order","text":"<pre><code>create_classification_order(\n    name: str,\n    instruction: str,\n    answer_options: list[str],\n    datapoints: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    confidence_threshold: float | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a classification order.</p> <p>With this order you can have a datapoint (image, text, video, audio) be classified into one of the answer options. Each response will be exactly one of the answer options.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order. (Will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction for how the data should be classified.</p> required <code>answer_options</code> <code>list[str]</code> <p>The list of options for the classification.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the classification - each datapoint will be labeled.</p> required <code>data_type</code> <code>str</code> <p>The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). </p> <p>Other option: \"text\".</p> <code>'media'</code> <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the classification. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction and options. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts for the classification i.e links to the images / videos. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction and options. (Therefore will be different for each datapoint)</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>The probability threshold for the classification. Defaults to None.</p> <p>If provided, the classification datapoint will stop after the threshold is reached or at the number of responses, whatever happens first.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the classification. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the classification. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the classification. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the classification. Defaults to None. If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_classification_order(\n    self,\n    name: str,\n    instruction: str,\n    answer_options: list[str],\n    datapoints: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    confidence_threshold: float | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a classification order.\n\n    With this order you can have a datapoint (image, text, video, audio) be classified into one of the answer options.\n    Each response will be exactly one of the answer options.\n\n    Args:\n        name (str): The name of the order. (Will not be shown to the labeler)\n        instruction (str): The instruction for how the data should be classified.\n        answer_options (list[str]): The list of options for the classification.\n        datapoints (list[str]): The list of datapoints for the classification - each datapoint will be labeled.\n        data_type (str, optional): The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). \\n\n            Other option: \"text\".\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the classification. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction and options. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts for the classification i.e links to the images / videos. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction and options. (Therefore will be different for each datapoint)\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        confidence_threshold (float, optional): The probability threshold for the classification. Defaults to None.\\n\n            If provided, the classification datapoint will stop after the threshold is reached or at the number of responses, whatever happens first.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the classification. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the classification. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the classification. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the classification. Defaults to None.\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"RapidataOrderManager.create_classification_order\"\n    ):\n        if not isinstance(datapoints, list) or not all(\n            isinstance(datapoint, str) for datapoint in datapoints\n        ):\n            raise ValueError(\"Datapoints must be a list of strings\")\n\n        return self._create_general_order(\n            name=name,\n            workflow=ClassifyWorkflow(\n                instruction=instruction, answer_options=answer_options\n            ),\n            assets=datapoints,\n            data_type=data_type,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            validation_set_id=validation_set_id,\n            confidence_threshold=confidence_threshold,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_compare_order","title":"create_compare_order","text":"<pre><code>create_compare_order(\n    name: str,\n    instruction: str,\n    datapoints: list[list[str]],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    a_b_names: list[str] | None = None,\n    validation_set_id: str | None = None,\n    confidence_threshold: float | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a compare order.</p> <p>With this order you compare two datapoints (image, text, video, audio) and the annotators will choose one of the two based on the instruction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order. (Will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction for the comparison. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[list[str]]</code> <p>Outher list is the datapoints, inner list is the options for the comparison - each datapoint will be labeled.</p> required <code>data_type</code> <code>str</code> <p>The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). </p> <p>Other option: \"text\".</p> <code>'media'</code> <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>a_b_names</code> <code>list[str]</code> <p>Custom naming for the two opposing models defined by the index in the datapoints list. Defaults to None.</p> <p>If provided has to be a list of exactly two strings. example: <pre><code>datapoints = [[\"path_to_image_A\", \"path_to_image_B\"], [\"path_to_text_A\", \"path_to_text_B\"]]\na_b_naming = [\"Model A\", \"Model B\"]\n</code></pre> The results will then correctly show \"Model A\" and \"Model B\". If not provided, the results will be shown as \"A\" and \"B\".</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>The probability threshold for the comparison. Defaults to None.</p> <p>If provided, the comparison datapoint will stop after the threshold is reached or at the number of responses, whatever happens first.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the comparison. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the comparison. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the comparison. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_compare_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[list[str]],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    a_b_names: list[str] | None = None,\n    validation_set_id: str | None = None,\n    confidence_threshold: float | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a compare order.\n\n    With this order you compare two datapoints (image, text, video, audio) and the annotators will choose one of the two based on the instruction.\n\n    Args:\n        name (str): The name of the order. (Will not be shown to the labeler)\n        instruction (str): The instruction for the comparison. Will be shown along side each datapoint.\n        datapoints (list[list[str]]): Outher list is the datapoints, inner list is the options for the comparison - each datapoint will be labeled.\n        data_type (str, optional): The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). \\n\n            Other option: \"text\".\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        a_b_names (list[str], optional): Custom naming for the two opposing models defined by the index in the datapoints list. Defaults to None.\\n\n            If provided has to be a list of exactly two strings.\n            example:\n            ```python\n            datapoints = [[\"path_to_image_A\", \"path_to_image_B\"], [\"path_to_text_A\", \"path_to_text_B\"]]\n            a_b_naming = [\"Model A\", \"Model B\"]\n            ```\n            The results will then correctly show \"Model A\" and \"Model B\".\n            If not provided, the results will be shown as \"A\" and \"B\".\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        confidence_threshold (float, optional): The probability threshold for the comparison. Defaults to None.\\n\n            If provided, the comparison datapoint will stop after the threshold is reached or at the number of responses, whatever happens first.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the comparison. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the comparison. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the comparison. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrderManager.create_compare_order\"):\n        if any(not isinstance(datapoint, list) for datapoint in datapoints):\n            raise ValueError(\"Each datapoint must be a list of 2 paths/texts\")\n\n        if any(len(set(datapoint)) != 2 for datapoint in datapoints):\n            raise ValueError(\n                \"Each datapoint must contain exactly two unique options\"\n            )\n\n        if a_b_names is not None and len(a_b_names) != 2:\n            raise ValueError(\n                \"A_B_naming must be a list of exactly two strings or None\"\n            )\n\n        return self._create_general_order(\n            name=name,\n            workflow=CompareWorkflow(instruction=instruction, a_b_names=a_b_names),\n            assets=datapoints,\n            data_type=data_type,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            validation_set_id=validation_set_id,\n            confidence_threshold=confidence_threshold,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_ranking_order","title":"create_ranking_order","text":"<pre><code>create_ranking_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    total_comparison_budget: int,\n    responses_per_comparison: int = 1,\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    random_comparisons_ratio: float = 0.5,\n    context: Optional[str] = None,\n    media_context: Optional[str] = None,\n    validation_set_id: Optional[str] = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n) -&gt; RapidataOrder\n</code></pre> <p>Create a ranking order.</p> <p>With this order you can rank a list of datapoints (image, text, video, audio) based on the instruction. The annotators will be shown two datapoints at a time. The ranking happens in terms of an elo system based on the matchup results.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The question asked from People when They see two datapoints.</p> required <code>datapoints</code> <code>list[str]</code> <p>A list of datapoints that will participate in the ranking.</p> required <code>total_comparison_budget</code> <code>int</code> <p>The total number of (pairwise-)comparisons that can be made.</p> required <code>responses_per_comparison</code> <code>int</code> <p>The number of responses collected per comparison. Defaults to 1.</p> <code>1</code> <code>data_type</code> <code>str</code> <p>The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). </p> <p>Other option: \"text\".</p> <code>'media'</code> <code>random_comparisons_ratio</code> <code>float</code> <p>The fraction of random comparisons in the ranking process. The rest will focus on pairing similarly ranked datapoints. Defaults to 0.5 and can be left untouched.</p> <code>0.5</code> <code>context</code> <code>str</code> <p>The context for all the comparison. Defaults to None.</p> <p>If provided will be shown in addition to the instruction for all the matchups.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context for all the comparison. Defaults to None.</p> <p>If provided will be shown in addition to the instruction for all the matchups.</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the order. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the order. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the order. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_ranking_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    total_comparison_budget: int,\n    responses_per_comparison: int = 1,\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    random_comparisons_ratio: float = 0.5,\n    context: Optional[str] = None,\n    media_context: Optional[str] = None,\n    validation_set_id: Optional[str] = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n) -&gt; RapidataOrder:\n    \"\"\"\n    Create a ranking order.\n\n    With this order you can rank a list of datapoints (image, text, video, audio) based on the instruction.\n    The annotators will be shown two datapoints at a time. The ranking happens in terms of an elo system based on the matchup results.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The question asked from People when They see two datapoints.\n        datapoints (list[str]): A list of datapoints that will participate in the ranking.\n        total_comparison_budget (int): The total number of (pairwise-)comparisons that can be made.\n        responses_per_comparison (int, optional): The number of responses collected per comparison. Defaults to 1.\n        data_type (str, optional): The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). \\n\n            Other option: \"text\".\n        random_comparisons_ratio (float, optional): The fraction of random comparisons in the ranking process.\n            The rest will focus on pairing similarly ranked datapoints. Defaults to 0.5 and can be left untouched.\n        context (str, optional): The context for all the comparison. Defaults to None.\\n\n            If provided will be shown in addition to the instruction for all the matchups.\n        media_context (str, optional): The media context for all the comparison. Defaults to None.\\n\n            If provided will be shown in addition to the instruction for all the matchups.\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the order. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the order. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the order. Defaults to []. Decides in what order the tasks should be shown.\n    \"\"\"\n\n    with tracer.start_as_current_span(\"RapidataOrderManager.create_ranking_order\"):\n        if len(datapoints) &lt; 2:\n            raise ValueError(\"At least two datapoints are required\")\n\n        if len(set(datapoints)) != len(datapoints):\n            raise ValueError(\"Datapoints must be unique\")\n\n        return self._create_general_order(\n            name=name,\n            workflow=RankingWorkflow(\n                criteria=instruction,\n                total_comparison_budget=total_comparison_budget,\n                random_comparisons_ratio=random_comparisons_ratio,\n                context=context,\n                media_context=media_context,\n                file_uploader=self.__asset_uploader,\n            ),\n            assets=datapoints,\n            data_type=data_type,\n            responses_per_datapoint=responses_per_comparison,\n            validation_set_id=validation_set_id,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_free_text_order","title":"create_free_text_order","text":"<pre><code>create_free_text_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a free text order.</p> <p>With this order you can have a datapoint (image, text, video, audio) be labeled with free text. The annotators will be shown a datapoint and will be asked to answer a question with free text.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The instruction to answer with free text. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the free text - each datapoint will be labeled.</p> required <code>data_type</code> <code>str</code> <p>The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). </p> <p>Other option: \"text\".</p> <code>'media'</code> <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the free text. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts for the free text i.e links to the images / videos. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the free text. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the free text. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the free text. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the free text. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_free_text_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a free text order.\n\n    With this order you can have a datapoint (image, text, video, audio) be labeled with free text.\n    The annotators will be shown a datapoint and will be asked to answer a question with free text.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The instruction to answer with free text. Will be shown along side each datapoint.\n        datapoints (list[str]): The list of datapoints for the free text - each datapoint will be labeled.\n        data_type (str, optional): The data type of the datapoints. Defaults to \"media\" (any form of image, video or audio). \\n\n            Other option: \"text\".\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the free text. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts for the free text i.e links to the images / videos. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the free text. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the free text. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the free text. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the free text. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"RapidataOrderManager.create_free_text_order\"\n    ):\n        return self._create_general_order(\n            name=name,\n            workflow=FreeTextWorkflow(instruction=instruction),\n            assets=datapoints,\n            data_type=data_type,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_select_words_order","title":"create_select_words_order","text":"<pre><code>create_select_words_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    sentences: list[str],\n    responses_per_datapoint: int = 10,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a select words order.</p> <p>With this order you can have a datapoint (image, text, video, audio) be labeled with a list of words. The annotators will be shown a datapoint as well as a list of sentences split up by spaces. They will then select specific words based on the instruction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The instruction for how the words should be selected. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the select words - each datapoint will be labeled.</p> required <code>sentences</code> <code>list[str]</code> <p>The list of sentences for the select words - Will be split up by spaces and shown along side each datapoint.</p> <p>Must be the same length as datapoints.</p> required <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the select words. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the select words. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the select words. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the select words. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_select_words_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    sentences: list[str],\n    responses_per_datapoint: int = 10,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a select words order.\n\n    With this order you can have a datapoint (image, text, video, audio) be labeled with a list of words.\n    The annotators will be shown a datapoint as well as a list of sentences split up by spaces.\n    They will then select specific words based on the instruction.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The instruction for how the words should be selected. Will be shown along side each datapoint.\n        datapoints (list[str]): The list of datapoints for the select words - each datapoint will be labeled.\n        sentences (list[str]): The list of sentences for the select words - Will be split up by spaces and shown along side each datapoint.\\n\n            Must be the same length as datapoints.\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the select words. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the select words. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the select words. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the select words. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"RapidataOrderManager.create_select_words_order\"\n    ):\n        return self._create_general_order(\n            name=name,\n            workflow=SelectWordsWorkflow(\n                instruction=instruction,\n            ),\n            assets=datapoints,\n            responses_per_datapoint=responses_per_datapoint,\n            validation_set_id=validation_set_id,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            sentences=sentences,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_locate_order","title":"create_locate_order","text":"<pre><code>create_locate_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a locate order.</p> <p>With this order you can have people locate specific objects in a datapoint (image, text, video, audio). The annotators will be shown a datapoint and will be asked to select locations based on the instruction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The instruction what should be located. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the locate - each datapoint will be labeled.</p> required <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the locate. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts for the locate i.e links to the images / videos. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the locate. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the locate. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the locate. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the locate. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_locate_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a locate order.\n\n    With this order you can have people locate specific objects in a datapoint (image, text, video, audio).\n    The annotators will be shown a datapoint and will be asked to select locations based on the instruction.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The instruction what should be located. Will be shown along side each datapoint.\n        datapoints (list[str]): The list of datapoints for the locate - each datapoint will be labeled.\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the locate. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts for the locate i.e links to the images / videos. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the locate. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the locate. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the locate. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the locate. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrderManager.create_locate_order\"):\n\n        return self._create_general_order(\n            name=name,\n            workflow=LocateWorkflow(target=instruction),\n            assets=datapoints,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            validation_set_id=validation_set_id,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_draw_order","title":"create_draw_order","text":"<pre><code>create_draw_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a draw order.</p> <p>With this order you can have people draw lines on a datapoint (image, text, video, audio). The annotators will be shown a datapoint and will be asked to draw lines based on the instruction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The instruction for how the lines should be drawn. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the draw lines - each datapoint will be labeled.</p> required <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts for the draw lines i.e links to the images / videos. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the draw lines. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the draw lines. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the draw lines. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the draw lines. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_draw_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a draw order.\n\n    With this order you can have people draw lines on a datapoint (image, text, video, audio).\n    The annotators will be shown a datapoint and will be asked to draw lines based on the instruction.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The instruction for how the lines should be drawn. Will be shown along side each datapoint.\n        datapoints (list[str]): The list of datapoints for the draw lines - each datapoint will be labeled.\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts for the draw lines i.e links to the images / videos. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the draw lines. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the draw lines. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the draw lines. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the draw lines. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrderManager.create_draw_order\"):\n\n        return self._create_general_order(\n            name=name,\n            workflow=DrawWorkflow(target=instruction),\n            assets=datapoints,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            validation_set_id=validation_set_id,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.create_timestamp_order","title":"create_timestamp_order","text":"<pre><code>create_timestamp_order(\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder\n</code></pre> <p>Create a timestamp order.</p> Warning <p>This order is currently not fully supported and may give unexpected results.</p> <p>With this order you can have people mark specific timestamps in a datapoint (video, audio). The annotators will be shown a datapoint and will be asked to select a timestamp based on the instruction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order.</p> required <code>instruction</code> <code>str</code> <p>The instruction for the timestamp task. Will be shown along side each datapoint.</p> required <code>datapoints</code> <code>list[str]</code> <p>The list of datapoints for the timestamp - each datapoint will be labeled.</p> required <code>responses_per_datapoint</code> <code>int</code> <p>The number of responses that will be collected per datapoint. Defaults to 10.</p> <code>10</code> <code>contexts</code> <code>list[str]</code> <p>The list of contexts for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts for the timestamp i.e links to the images / videos. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)</p> <code>None</code> <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set. Defaults to None.</p> <p>If provided, one validation task will be shown infront of the datapoints that will be labeled.</p> <code>None</code> <code>filters</code> <code>Sequence[RapidataFilter]</code> <p>The list of filters for the timestamp. Defaults to []. Decides who the tasks should be shown to.</p> <code>[]</code> <code>settings</code> <code>Sequence[RapidataSetting]</code> <p>The list of settings for the timestamp. Defaults to []. Decides how the tasks should be shown.</p> <code>[]</code> <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>The list of selections for the timestamp. Defaults to []. Decides in what order the tasks should be shown.</p> <code>[]</code> <code>private_notes</code> <code>list[str]</code> <p>The list of private notes for the timestamp. Defaults to None.</p> <p>If provided has to be the same length as datapoints.</p> <p>This will NOT be shown to the labelers but will be included in the result purely for your own reference.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def create_timestamp_order(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[str],\n    responses_per_datapoint: int = 10,\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    validation_set_id: str | None = None,\n    filters: Sequence[RapidataFilter] = [],\n    settings: Sequence[RapidataSetting] = [],\n    selections: Sequence[RapidataSelection] = [],\n    private_notes: list[str] | None = None,\n) -&gt; RapidataOrder:\n    \"\"\"Create a timestamp order.\n\n    Warning:\n        This order is currently not fully supported and may give unexpected results.\n\n    With this order you can have people mark specific timestamps in a datapoint (video, audio).\n    The annotators will be shown a datapoint and will be asked to select a timestamp based on the instruction.\n\n    Args:\n        name (str): The name of the order.\n        instruction (str): The instruction for the timestamp task. Will be shown along side each datapoint.\n        datapoints (list[str]): The list of datapoints for the timestamp - each datapoint will be labeled.\n        responses_per_datapoint (int, optional): The number of responses that will be collected per datapoint. Defaults to 10.\n        contexts (list[str], optional): The list of contexts for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts for the timestamp i.e links to the images / videos. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n        validation_set_id (str, optional): The ID of the validation set. Defaults to None.\\n\n            If provided, one validation task will be shown infront of the datapoints that will be labeled.\n        filters (Sequence[RapidataFilter], optional): The list of filters for the timestamp. Defaults to []. Decides who the tasks should be shown to.\n        settings (Sequence[RapidataSetting], optional): The list of settings for the timestamp. Defaults to []. Decides how the tasks should be shown.\n        selections (Sequence[RapidataSelection], optional): The list of selections for the timestamp. Defaults to []. Decides in what order the tasks should be shown.\n        private_notes (list[str], optional): The list of private notes for the timestamp. Defaults to None.\\n\n            If provided has to be the same length as datapoints.\\n\n            This will NOT be shown to the labelers but will be included in the result purely for your own reference.\n    \"\"\"\n\n    with tracer.start_as_current_span(\n        \"RapidataOrderManager.create_timestamp_order\"\n    ):\n        return self._create_general_order(\n            name=name,\n            workflow=TimestampWorkflow(instruction=instruction),\n            assets=datapoints,\n            responses_per_datapoint=responses_per_datapoint,\n            contexts=contexts,\n            media_contexts=media_contexts,\n            validation_set_id=validation_set_id,\n            filters=filters,\n            selections=selections,\n            settings=settings,\n            private_notes=private_notes,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.get_order_by_id","title":"get_order_by_id","text":"<pre><code>get_order_by_id(order_id: str) -&gt; RapidataOrder\n</code></pre> <p>Get an order by ID.</p> <p>Parameters:</p> Name Type Description Default <code>order_id</code> <code>str</code> <p>The ID of the order.</p> required <p>Returns:</p> Name Type Description <code>RapidataOrder</code> <code>RapidataOrder</code> <p>The Order instance.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def get_order_by_id(self, order_id: str) -&gt; RapidataOrder:\n    \"\"\"Get an order by ID.\n\n    Args:\n        order_id (str): The ID of the order.\n\n    Returns:\n        RapidataOrder: The Order instance.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrderManager.get_order_by_id\"):\n        order = self.__openapi_service.order_api.order_order_id_get(order_id)\n\n        return RapidataOrder(\n            order_id=order_id,\n            name=order.order_name,\n            openapi_service=self.__openapi_service,\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_order_manager/#rapidata.rapidata_client.order.rapidata_order_manager.RapidataOrderManager.find_orders","title":"find_orders","text":"<pre><code>find_orders(\n    name: str = \"\", amount: int = 10\n) -&gt; list[RapidataOrder]\n</code></pre> <p>Find your recent orders given criteria. If nothing is provided, it will return the most recent order.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the order - matching order will contain the name. Defaults to \"\" for any order.</p> <code>''</code> <code>amount</code> <code>int</code> <p>The amount of orders to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[RapidataOrder]</code> <p>list[RapidataOrder]: A list of RapidataOrder instances.</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_order_manager.py</code> <pre><code>def find_orders(self, name: str = \"\", amount: int = 10) -&gt; list[RapidataOrder]:\n    \"\"\"Find your recent orders given criteria. If nothing is provided, it will return the most recent order.\n\n    Args:\n        name (str, optional): The name of the order - matching order will contain the name. Defaults to \"\" for any order.\n        amount (int, optional): The amount of orders to return. Defaults to 10.\n\n    Returns:\n        list[RapidataOrder]: A list of RapidataOrder instances.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataOrderManager.find_orders\"):\n        order_page_result = self.__openapi_service.order_api.orders_get(\n            QueryModel(\n                page=PageInfo(index=1, size=amount),\n                filter=RootFilter(\n                    filters=[\n                        Filter(\n                            field=\"OrderName\",\n                            operator=FilterOperator.CONTAINS,\n                            value=name,\n                        )\n                    ]\n                ),\n                sortCriteria=[\n                    SortCriterion(\n                        direction=SortDirection.DESC, propertyName=\"OrderDate\"\n                    )\n                ],\n            )\n        )\n\n        orders = [\n            self.get_order_by_id(order.id) for order in order_page_result.items\n        ]\n        return orders\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_results/","title":"Rapidata results","text":""},{"location":"reference/rapidata/rapidata_client/order/rapidata_results/#rapidata.rapidata_client.order.rapidata_results.RapidataResults","title":"RapidataResults","text":"<p>               Bases: <code>dict</code></p> <p>A specialized dictionary class for handling Rapidata API results. Extends the built-in dict class with specialized methods.</p>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_results/#rapidata.rapidata_client.order.rapidata_results.RapidataResults.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(split_details: bool = False) -&gt; DataFrame\n</code></pre> Warning <p>This method is currently under development. The structure of the results may change in the future.</p> <p>Converts the results to a pandas DataFrame.</p> <p>For Compare results, creates standardized A/B columns for metrics. For regular results, flattens nested dictionaries into columns with underscore-separated names.</p> <p>Parameters:</p> Name Type Description Default <code>split_details</code> <code>bool</code> <p>If True, splits each datapoint by its detailed results,           creating a row for each response with global metrics copied.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the processed results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If split_details is True but no detailed results are found</p> Source code in <code>src/rapidata/rapidata_client/order/rapidata_results.py</code> <pre><code>def to_pandas(self, split_details: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Warning:\n        This method is currently under development. The structure of the results may change in the future.\n\n    Converts the results to a pandas DataFrame.\n\n    For Compare results, creates standardized A/B columns for metrics.\n    For regular results, flattens nested dictionaries into columns with underscore-separated names.\n\n    Args:\n        split_details: If True, splits each datapoint by its detailed results,\n                      creating a row for each response with global metrics copied.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the processed results\n\n    Raises:\n        ValueError: If split_details is True but no detailed results are found\n    \"\"\"\n    if \"results\" not in self or not self[\"results\"]:\n        return pd.DataFrame()\n\n    if self[\"info\"].get(\"orderType\") is None:\n        managed_print(\n            \"Warning: Results are old and Order type is not specified. Dataframe might be wrong.\"\n        )\n\n    # Check for detailed results if split_details is True\n    if split_details:\n        if not self._has_detailed_results():\n            raise ValueError(\"No detailed results found in the data\")\n        return self._to_pandas_with_detailed_results()\n\n    if (\n        self[\"info\"].get(\"orderType\") == \"Compare\"\n        or self[\"info\"].get(\"orderType\") == \"Ranking\"\n    ):\n        return self._compare_to_pandas()\n\n    # Get the structure from first item\n    first_item = self[\"results\"][0]\n    columns = []\n    path_map = {}  # Maps flattened column names to paths to reach the values\n\n    # Build the column structure once\n    self._build_column_structure(first_item, columns, path_map)\n\n    # Extract data using the known structure\n    data = []\n    for item in self[\"results\"]:\n        row = []\n        for path in path_map.values():\n            value = self._get_value_from_path(item, path)\n            row.append(value)\n        data.append(row)\n\n    return pd.DataFrame(data, columns=Index(columns))\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/order/rapidata_results/#rapidata.rapidata_client.order.rapidata_results.RapidataResults.to_json","title":"to_json","text":"<pre><code>to_json(path: str = './results.json') -&gt; None\n</code></pre> <p>Saves the results to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path where the JSON should be saved. Defaults to \"./results.json\".</p> <code>'./results.json'</code> Source code in <code>src/rapidata/rapidata_client/order/rapidata_results.py</code> <pre><code>def to_json(self, path: str = \"./results.json\") -&gt; None:\n    \"\"\"\n    Saves the results to a JSON file.\n\n    Args:\n        path: The file path where the JSON should be saved. Defaults to \"./results.json\".\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(self, f)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/ab_test_selection/","title":"Ab test selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/ab_test_selection/#rapidata.rapidata_client.selection.ab_test_selection.AbTestSelection","title":"AbTestSelection","text":"<pre><code>AbTestSelection(\n    a_selections: Sequence[RapidataSelection],\n    b_selections: Sequence[RapidataSelection],\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>AbTestSelection Class</p> <p>Splits the userbase into two segments and serves them a different collection of rapids.</p> <p>Useful for A/B Test.</p> <p>Parameters:</p> Name Type Description Default <code>a_selections</code> <code>Sequence[RapidataSelection]</code> <p>List of selections for group A.</p> required <code>b_selections</code> <code>Sequence[RapidataSelection]</code> <p>List of selections for group B.</p> required Source code in <code>src/rapidata/rapidata_client/selection/ab_test_selection.py</code> <pre><code>def __init__(\n    self,\n    a_selections: Sequence[RapidataSelection],\n    b_selections: Sequence[RapidataSelection],\n):\n    self.a_selections = a_selections\n    self.b_selections = b_selections\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/capped_selection/","title":"Capped selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/capped_selection/#rapidata.rapidata_client.selection.capped_selection.CappedSelection","title":"CappedSelection","text":"<pre><code>CappedSelection(\n    selections: Sequence[RapidataSelection], max_rapids: int\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>CappedSelection Class</p> <p>Takes in different selections and caps the amount of rapids that can be shown.</p> <p>Useful for demographic and conditional validation selections.</p> <p>Parameters:</p> Name Type Description Default <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>List of selections to cap.</p> required <code>max_rapids</code> <code>int</code> <p>The maximum amount of rapids that can be shown for this selection.</p> required Source code in <code>src/rapidata/rapidata_client/selection/capped_selection.py</code> <pre><code>def __init__(self, selections: Sequence[RapidataSelection], max_rapids: int):\n    self.selections = selections\n    self.max_rapids = max_rapids\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/conditional_validation_selection/","title":"Conditional validation selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/conditional_validation_selection/#rapidata.rapidata_client.selection.conditional_validation_selection.ConditionalValidationSelection","title":"ConditionalValidationSelection","text":"<pre><code>ConditionalValidationSelection(\n    validation_set_id: str,\n    thresholds: list[float],\n    chances: list[float],\n    rapid_counts: list[int],\n    dimension: Optional[str] = None,\n    dimensions: Optional[list[str]] = None,\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>Conditional validation selection class.</p> <p>Probabilistically decides how many validation rapids you want to show per session based on the user score.</p> <p>Parameters:</p> Name Type Description Default <code>validation_set_id</code> <code>str</code> <p>The id of the validation set to be used.</p> required <code>thresholds</code> <code>list[float]</code> <p>The thresholds to use for the user score.</p> required <code>chances</code> <code>list[float]</code> <p>The chances of showing a validation rapid for each threshold.</p> required <code>rapid_counts</code> <code>list[int]</code> <p>The amount of validation rapids that will be shown per session of this validation set for each threshold if selected by probability. (all or nothing)</p> required <code>dimensions</code> <code>Optional[list[str]]</code> <p>The dimensions of the userScore that will be used in the thresholds. Defaults to None.</p> <code>None</code> Example <p><pre><code>ConditionalValidationSelection(\n    validation_set_id=\"validation_set_id\",\n    thresholds=[0, 0.7], # (0 must be the first threshold)\n    chances=[1, 0.2],\n    rapid_counts=[1, 1]\n)\n</code></pre> This means that there's a 100% chance of showing a validation rapid if the user score is between 0 and 0.7, and a 20% chance of showing a validation rapid if the user score is between 0.7 and 1.</p> Source code in <code>src/rapidata/rapidata_client/selection/conditional_validation_selection.py</code> <pre><code>def __init__(\n    self,\n    validation_set_id: str,\n    thresholds: list[float],\n    chances: list[float],\n    rapid_counts: list[int],\n    dimension: Optional[str] = None,\n    dimensions: Optional[list[str]] = None,\n):\n    if len(thresholds) != len(chances) or len(thresholds) != len(rapid_counts):\n        raise ValueError(\n            \"The lengths of thresholds, chances and rapid_counts must be equal.\"\n        )\n\n    if dimension:\n        logger.warning(\"dimension is deprecated, use dimensions instead\")\n        dimensions = (dimensions or []) + [dimension]\n\n    self.validation_set_id = validation_set_id\n    self.thresholds = thresholds\n    self.chances = chances\n    self.rapid_counts = rapid_counts\n    self.dimensions = dimensions\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/demographic_selection/","title":"Demographic selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/demographic_selection/#rapidata.rapidata_client.selection.demographic_selection.DemographicSelection","title":"DemographicSelection","text":"<pre><code>DemographicSelection(keys: list[str], max_rapids: int)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>Demographic selection class.</p> <p>This is used to ask demographic questions in an order.</p> <p>The keys will select the rapids based on the confidence we already saved for each user.</p> <p>If the confidence is high, the users will be selected to solve the rapids with lower probability.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>List of keys for the demographic rapids to be shown. As an example: \"age\"</p> required <code>max_rapids</code> <code>int</code> <p>The maximum number of rapids to run.</p> <p>Allows to provide more keys, in case some of the earlier ones are not selected because of high confidence.</p> required Example <p><pre><code>DemographicSelection([\"age\", \"gender\"], 1)\n</code></pre> This will try to ask the user about their age, if that is not selected due to an already high confidence, it will try asking about their gender. The gender question may also be skipped if the confidence is high enough.</p> Source code in <code>src/rapidata/rapidata_client/selection/demographic_selection.py</code> <pre><code>def __init__(self, keys: list[str], max_rapids: int):\n    self.keys = keys\n    self.max_rapids = max_rapids\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/effort_selection/","title":"Effort selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/effort_selection/#rapidata.rapidata_client.selection.effort_selection.EffortSelection","title":"EffortSelection","text":"<pre><code>EffortSelection(\n    effort_budget: int,\n    retrieval_mode: RapidataRetrievalMode = Shuffled,\n    max_iterations: int | None = None,\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>With this selection you can define the effort budget you have for a task. As an example, you have a task that takes 10 seconds to complete. The effort budget would be 10.</p> <p>Parameters:</p> Name Type Description Default <code>effort_budget</code> <code>int</code> <p>The effort budget for the task.</p> required <code>retrieval_mode</code> <code>RetrievalMode</code> <p>The retrieval mode for the task.</p> <code>Shuffled</code> <code>max_iterations</code> <code>int | None</code> <p>The maximum number of iterations for the task.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/selection/effort_selection.py</code> <pre><code>def __init__(\n    self,\n    effort_budget: int,\n    retrieval_mode: RapidataRetrievalMode = RapidataRetrievalMode.Shuffled,\n    max_iterations: int | None = None,\n):\n    self.effort_budget = effort_budget\n    self.retrieval_mode = retrieval_mode\n    self.max_iterations = max_iterations\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/labeling_selection/","title":"Labeling selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/labeling_selection/#rapidata.rapidata_client.selection.labeling_selection.LabelingSelection","title":"LabelingSelection","text":"<pre><code>LabelingSelection(\n    amount: int,\n    retrieval_mode: RapidataRetrievalMode = Shuffled,\n    max_iterations: int | None = None,\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>Labeling selection class.</p> <p>Decides how many actual datapoints you want to show per session.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>int</code> <p>The amount of labeling rapids that will be shown per session.</p> required <code>retrieval_mode</code> <code>RetrievalMode</code> <p>The retrieval mode to use. Defaults to \"Shuffled\".</p> <code>Shuffled</code> <code>max_iterations</code> <code>int | None</code> <p>An annotator can answer the same task only once if the retrieval_mode is \"Shuffled\" or \"Sequential\". max_iterations can increase the amount of responses an annotator can do to the same task (datapoint).</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/selection/labeling_selection.py</code> <pre><code>def __init__(\n    self,\n    amount: int,\n    retrieval_mode: RapidataRetrievalMode = RapidataRetrievalMode.Shuffled,\n    max_iterations: int | None = None,\n):\n    self.amount = amount\n    self.retrieval_mode = retrieval_mode\n    self.max_iterations = max_iterations\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/rapidata_retrieval_modes/","title":"Rapidata retrieval modes","text":""},{"location":"reference/rapidata/rapidata_client/selection/rapidata_retrieval_modes/#rapidata.rapidata_client.selection.rapidata_retrieval_modes.RapidataRetrievalMode","title":"RapidataRetrievalMode","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for defining retrieval modes for datapoints.</p>"},{"location":"reference/rapidata/rapidata_client/selection/rapidata_retrieval_modes/#rapidata.rapidata_client.selection.rapidata_retrieval_modes.RapidataRetrievalMode.Shuffled","title":"Shuffled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Shuffled = SHUFFLED\n</code></pre> <p>Will shuffle the datapoints randomly for each user. The user will then see the datapoints in that order. This will take into account the \"max_iterations\" parameter.</p>"},{"location":"reference/rapidata/rapidata_client/selection/rapidata_retrieval_modes/#rapidata.rapidata_client.selection.rapidata_retrieval_modes.RapidataRetrievalMode.Sequential","title":"Sequential  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Sequential = SEQUENTIAL\n</code></pre> <p>Will show the datapoints in the order they are in the dataset. This will take into account the \"max_iterations\" parameter.</p>"},{"location":"reference/rapidata/rapidata_client/selection/rapidata_retrieval_modes/#rapidata.rapidata_client.selection.rapidata_retrieval_modes.RapidataRetrievalMode.Random","title":"Random  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Random = RANDOM\n</code></pre> <p>Will just randomly feed the datapoints to the annotators. This will NOT take into account the \"max_iterations\" parameter.</p>"},{"location":"reference/rapidata/rapidata_client/selection/rapidata_selections/","title":"\ud83d\udce4 Selections","text":""},{"location":"reference/rapidata/rapidata_client/selection/rapidata_selections/#rapidata.rapidata_client.selection.rapidata_selections.RapidataSelections","title":"RapidataSelections","text":"<p>RapidataSelections Classes</p> <p>Selections are used to define what type of tasks and in what order they are shown to the user. All selections combined are called a \"Session\". A session can contain multiple tasks of different types of tasks. As an example, a session might be 1 validation task, 2 labeling tasks.</p> <p>Attributes:</p> Name Type Description <code>labeling</code> <code>LabelingSelection</code> <p>Decides how many actual datapoints you want to show per session.</p> <code>validation</code> <code>ValidationSelection</code> <p>Decides how many validation rapids you want to show per session.</p> <code>conditional_validation</code> <code>ConditionalValidationSelection</code> <p>Probabilistically decides how many validation rapids you want to show per session based on the user score.</p> <code>demographic</code> <code>DemographicSelection</code> <p>Decides if and how many demographic questions you want to show per session.</p> <code>capped</code> <code>CappedSelection</code> <p>Takes in different selections and caps the amount of rapids that can be shown.</p> <code>shuffling</code> <code>ShufflingSelection</code> <p>Shuffles the selections provided in the list.</p> Example <pre><code>from rapidata import LabelingSelection, ValidationSelection\nselections=[ValidationSelection(\"your-validation-set-id\", 1),\n            LabelingSelection(2)]\n</code></pre> <p>This will require annotators to complete one validation task followed by two labeling tasks.</p>"},{"location":"reference/rapidata/rapidata_client/selection/shuffling_selection/","title":"Shuffling selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/shuffling_selection/#rapidata.rapidata_client.selection.shuffling_selection.ShufflingSelection","title":"ShufflingSelection","text":"<pre><code>ShufflingSelection(selections: Sequence[RapidataSelection])\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>ShufflingSelection Class</p> <p>Shuffles the selections provided in the list.</p> <p>Parameters:</p> Name Type Description Default <code>selections</code> <code>Sequence[RapidataSelection]</code> <p>List of selections to shuffle.</p> required Example <p><pre><code>selection = ShufflingSelection(\n            [ValidSelections(\"validation_id\", 1), LabelingSelection(2)])\n</code></pre> This means that the users will get 1 validation task and 2 labeling tasks in a shuffled order.</p> Source code in <code>src/rapidata/rapidata_client/selection/shuffling_selection.py</code> <pre><code>def __init__(self, selections: Sequence[RapidataSelection]):\n    self.selections = selections\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/static_selection/","title":"Static selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/static_selection/#rapidata.rapidata_client.selection.static_selection.StaticSelection","title":"StaticSelection","text":"<pre><code>StaticSelection(rapid_ids: list[str])\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>StaticSelection Class</p> <p>Given a list of RapidIds, theses specific rapids will be shown in order for every session.</p> <p>Parameters:</p> Name Type Description Default <code>rapid_ids</code> <code>list[str]</code> <p>List of rapid ids to show.</p> required Source code in <code>src/rapidata/rapidata_client/selection/static_selection.py</code> <pre><code>def __init__(self, rapid_ids: list[str]):\n    self.rapid_ids = rapid_ids\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/selection/validation_selection/","title":"Validation selection","text":""},{"location":"reference/rapidata/rapidata_client/selection/validation_selection/#rapidata.rapidata_client.selection.validation_selection.ValidationSelection","title":"ValidationSelection","text":"<pre><code>ValidationSelection(\n    validation_set_id: str, amount: int = 1\n)\n</code></pre> <p>               Bases: <code>RapidataSelection</code></p> <p>Validation selection class.</p> <p>Decides how many validation rapids you want to show per session.</p> <p>Parameters:</p> Name Type Description Default <code>validation_set_id</code> <code>str</code> <p>The id of the validation set to be used.</p> required <code>amount</code> <code>int</code> <p>The amount of validation rapids that will be shown per session of this validation set.</p> <code>1</code> Source code in <code>src/rapidata/rapidata_client/selection/validation_selection.py</code> <pre><code>def __init__(self, validation_set_id: str, amount: int = 1):\n    self.validation_set_id = validation_set_id\n    self.amount = amount\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/alert_on_fast_response/","title":"Alert on fast response","text":""},{"location":"reference/rapidata/rapidata_client/settings/alert_on_fast_response/#rapidata.rapidata_client.settings.alert_on_fast_response.AlertOnFastResponse","title":"AlertOnFastResponse","text":"<pre><code>AlertOnFastResponse(threshold: int)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Gives an alert as a pop up on the UI when the response time is less than the milliseconds.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>if the user responds in less than this time, an alert will be shown.</p> required Source code in <code>src/rapidata/rapidata_client/settings/alert_on_fast_response.py</code> <pre><code>def __init__(self, threshold: int):\n    if not isinstance(threshold, int):\n        raise ValueError(\"The alert must be an integer.\")\n    if threshold &lt; 10:\n        managed_print(\n            f\"Warning: Are you sure you want to set the threshold so low ({threshold} milliseconds)?\"\n        )\n    if threshold &gt; 25000:\n        raise ValueError(\"The alert must be less than 25000 milliseconds.\")\n    if threshold &lt; 0:\n        raise ValueError(\"The alert must be greater than or equal to 0.\")\n\n    super().__init__(key=\"alert_on_fast_response\", value=threshold)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/allow_neither_both/","title":"Allow neither both","text":""},{"location":"reference/rapidata/rapidata_client/settings/allow_neither_both/#rapidata.rapidata_client.settings.allow_neither_both.AllowNeitherBoth","title":"AllowNeitherBoth","text":"<pre><code>AllowNeitherBoth(value: bool = True)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Set whether to allow neither or both options. This setting only works for compare orders.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>Whether to allow neither or both options. Defaults to True. If this setting is not added to an order, the users won't be able to select neither or both.</p> <code>True</code> Source code in <code>src/rapidata/rapidata_client/settings/allow_neither_both.py</code> <pre><code>def __init__(self, value: bool = True):\n    super().__init__(key=\"compare_unsure\", value=value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/custom_setting/","title":"Custom setting","text":""},{"location":"reference/rapidata/rapidata_client/settings/custom_setting/#rapidata.rapidata_client.settings.custom_setting.CustomSetting","title":"CustomSetting","text":"<pre><code>CustomSetting(key: str, value: str)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Set a custom setting with the given key and value. Use this to enable features that do not have a dedicated method (yet)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key for the custom setting.</p> required <code>value</code> <code>str</code> <p>The value for the custom setting.</p> required Source code in <code>src/rapidata/rapidata_client/settings/custom_setting.py</code> <pre><code>def __init__(self, key: str, value: str):\n    if not isinstance(key, str):\n        raise ValueError(\"The key must be a string.\")\n\n    super().__init__(key=key, value=value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/free_text_minimum_characters/","title":"Free text minimum characters","text":""},{"location":"reference/rapidata/rapidata_client/settings/free_text_minimum_characters/#rapidata.rapidata_client.settings.free_text_minimum_characters.FreeTextMinimumCharacters","title":"FreeTextMinimumCharacters","text":"<pre><code>FreeTextMinimumCharacters(value: int)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Set the minimum number of characters a user has to type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The minimum number of characters for free text.</p> required Source code in <code>src/rapidata/rapidata_client/settings/free_text_minimum_characters.py</code> <pre><code>def __init__(self, value: int):\n    if value &lt; 1:\n        raise ValueError(\n            \"The minimum number of characters must be greater than or equal to 1.\"\n        )\n    if value &gt; 40:\n        managed_print(\n            f\"Warning: Are you sure you want to set the minimum number of characters at {value}?\"\n        )\n    super().__init__(key=\"free_text_minimum_characters\", value=value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/no_shuffle/","title":"No shuffle","text":""},{"location":"reference/rapidata/rapidata_client/settings/no_shuffle/#rapidata.rapidata_client.settings.no_shuffle.NoShuffle","title":"NoShuffle","text":"<pre><code>NoShuffle(value: bool = True)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Only for classification and compare tasks. If true, the order of the categories / images will not be shuffled and presented in the same order as specified.</p> <p>If this is not added to the order, the shuffling will be active.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>Whether to disable shuffling. Defaults to True for function call.</p> <code>True</code> Source code in <code>src/rapidata/rapidata_client/settings/no_shuffle.py</code> <pre><code>def __init__(self, value: bool = True):\n    if not isinstance(value, bool):\n        raise ValueError(\"The value must be a boolean.\")\n\n    super().__init__(key=\"no_shuffle\", value=value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/play_video_until_the_end/","title":"Play video until the end","text":""},{"location":"reference/rapidata/rapidata_client/settings/play_video_until_the_end/#rapidata.rapidata_client.settings.play_video_until_the_end.PlayVideoUntilTheEnd","title":"PlayVideoUntilTheEnd","text":"<pre><code>PlayVideoUntilTheEnd(additional_time: int = 0)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Allows users to only answer once the video has finished playing. The additional time gets added on top of the video duration. Can be negative to allow answers before the video ends.</p> <p>Parameters:</p> Name Type Description Default <code>additional_time</code> <code>int</code> <p>Additional time in milliseconds. Defaults to 0.</p> <code>0</code> Source code in <code>src/rapidata/rapidata_client/settings/play_video_until_the_end.py</code> <pre><code>def __init__(self, additional_time: int = 0):\n    if additional_time &lt; -25000 or additional_time &gt; 25000:\n        raise ValueError(\"The additional time must be between -25000 and 25000.\")\n\n    super().__init__(\n        key=\"alert_on_fast_response_add_media_duration\", value=additional_time\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/rapidata_settings/","title":"\u2699\ufe0f Settings","text":""},{"location":"reference/rapidata/rapidata_client/settings/rapidata_settings/#rapidata.rapidata_client.settings.rapidata_settings.RapidataSettings","title":"RapidataSettings","text":"<p>Container class for all setting factory functions</p> <p>Settings can be added to an order to determine the behaviour of the task.</p> <p>Attributes:</p> Name Type Description <code>alert_on_fast_response</code> <code>AlertOnFastResponse</code> <p>Gives an alert as a pop up on the UI when the response time is less than the milliseconds.</p> <code>translation_behaviour</code> <code>TranslationBehaviour</code> <p>Defines what's the behaviour of the translation in the UI.</p> <code>free_text_minimum_characters</code> <code>FreeTextMinimumCharacters</code> <p>Only for free text tasks. Set the minimum number of characters a user has to type.</p> <code>no_shuffle</code> <code>NoShuffle</code> <p>Only for classification and compare tasks. If true, the order of the categories / images will not be shuffled and presented in the same order as specified.</p> <code>play_video_until_the_end</code> <code>PlayVideoUntilTheEnd</code> <p>Allows users to only answer once the video has finished playing.</p> <code>allow_neither_both</code> <code>AllowNeitherBoth</code> <p>Only for compare tasks. If true, the users will be able to select neither or both instead of exclusively one of the options.</p> <code>swap_context_instruction</code> <code>SwapContextInstruction</code> <p>Swap the place of the context and instruction.</p> Example <pre><code>from rapidata import FreeTextMinimumCharacters\nsettings=[FreeTextMinimumCharacters(10)]\n</code></pre> <p>This can be used in a free text order to set the minimum number of characters required to submit the task.</p>"},{"location":"reference/rapidata/rapidata_client/settings/swap_context_instruction/","title":"Swap context instruction","text":""},{"location":"reference/rapidata/rapidata_client/settings/swap_context_instruction/#rapidata.rapidata_client.settings.swap_context_instruction.SwapContextInstruction","title":"SwapContextInstruction","text":"<pre><code>SwapContextInstruction(value: bool = True)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Swap the place of the context and instruction.</p> <p>If set to true, the instruction will be shown on top and the context below. if collapsed, only the instruction will be shown.</p> <p>By default, the context will be shown on top and the instruction below.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bool</code> <p>Whether to swap the place of the context and instruction.</p> <code>True</code> Source code in <code>src/rapidata/rapidata_client/settings/swap_context_instruction.py</code> <pre><code>def __init__(self, value: bool = True):\n    if not isinstance(value, bool):\n        raise ValueError(\"The value must be a boolean.\")\n\n    super().__init__(key=\"swap_question_and_prompt\", value=value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/translation_behaviour/","title":"Translation behaviour","text":""},{"location":"reference/rapidata/rapidata_client/settings/translation_behaviour/#rapidata.rapidata_client.settings.translation_behaviour.TranslationBehaviour","title":"TranslationBehaviour","text":"<pre><code>TranslationBehaviour(value: TranslationBehaviourOptions)\n</code></pre> <p>               Bases: <code>RapidataSetting</code></p> <p>Defines what's the behaviour of the translation in the UI. Will not translate text datapoints or sentences.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>TranslationBehaviourOptions</code> <p>The translation behaviour.</p> required Source code in <code>src/rapidata/rapidata_client/settings/translation_behaviour.py</code> <pre><code>def __init__(self, value: TranslationBehaviourOptions):\n    if not isinstance(value, TranslationBehaviourOptions):\n        raise ValueError(\"The value must be a TranslationBehaviourOptions.\")\n\n    super().__init__(key=\"translation_behaviour\", value=value.value)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/settings/models/translation_behaviour_options/","title":"Translation behaviour options","text":""},{"location":"reference/rapidata/rapidata_client/settings/models/translation_behaviour_options/#rapidata.rapidata_client.settings.models.translation_behaviour_options.TranslationBehaviourOptions","title":"TranslationBehaviourOptions","text":"<p>               Bases: <code>Enum</code></p> <p>The options for the translation behaviour setting.</p> <p>Attributes:</p> Name Type Description <code>BOTH</code> <p>Show both the original and the translated text. May clutter the screen if the options are too long.</p> <code>ONLY_ORIGINAL</code> <p>Show only the original text.</p> <code>ONLY_TRANSLATED</code> <p>Show only the translated text.</p>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/","title":"Rapidata validation set","text":""},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet","title":"RapidataValidationSet","text":"<pre><code>RapidataValidationSet(\n    validation_set_id,\n    name: str,\n    openapi_service: OpenAPIService,\n)\n</code></pre> <p>A class for interacting with a Rapidata validation set.</p> <p>Represents a set of all the validation tasks that can be added to an order.</p> <p>When added to an order, the tasks will be selected randomly from the set.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The ID of the validation set.</p> <code>name</code> <code>str</code> <p>The name of the validation set.</p> Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def __init__(self, validation_set_id, name: str, openapi_service: OpenAPIService):\n    self.id = validation_set_id\n    self.name = name\n    self.validation_set_details_page = (\n        f\"https://app.{openapi_service.environment}/validation-set/detail/{self.id}\"\n    )\n    self._openapi_service = openapi_service\n    self.validation_rapid_uploader = ValidationRapidUploader(openapi_service)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet.add_rapid","title":"add_rapid","text":"<pre><code>add_rapid(rapid: Rapid)\n</code></pre> <p>Add a Rapid to the validation set.</p> <p>Parameters:</p> Name Type Description Default <code>rapid</code> <code>Rapid</code> <p>The Rapid to add to the validation set.</p> required Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def add_rapid(self, rapid: Rapid):\n    \"\"\"Add a Rapid to the validation set.\n\n    Args:\n        rapid (Rapid): The Rapid to add to the validation set.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataValidationSet.add_rapid\"):\n        logger.debug(\"Adding rapid %s to validation set %s\", rapid, self.id)\n        self.validation_rapid_uploader.upload_rapid(rapid, self.id)\n    return self\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet.update_dimensions","title":"update_dimensions","text":"<pre><code>update_dimensions(dimensions: list[str])\n</code></pre> <p>Update the dimensions of the validation set.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>list[str]</code> <p>The new dimensions of the validation set.</p> required Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def update_dimensions(self, dimensions: list[str]):\n    \"\"\"Update the dimensions of the validation set.\n\n    Args:\n        dimensions (list[str]): The new dimensions of the validation set.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataValidationSet.update_dimensions\"):\n        logger.debug(\n            \"Updating dimensions for validation set %s to %s\", self.id, dimensions\n        )\n        self._openapi_service.validation_api.validation_set_validation_set_id_patch(\n            self.id, UpdateValidationSetModel(dimensions=dimensions)\n        )\n        return self\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet.update_should_alert","title":"update_should_alert","text":"<pre><code>update_should_alert(should_alert: bool)\n</code></pre> <p>Determines whether users should be alerted if they answer incorrectly.</p> <p>Parameters:</p> Name Type Description Default <code>should_alert</code> <code>bool</code> <p>Specifies whether users should be alerted for incorrect answers. Defaults to True if not specifically overridden by this method.</p> required Note <p>The userScore dimensions which are updated when a user answers a validation task are updated regardless of the value of <code>should_alert</code>.</p> Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def update_should_alert(self, should_alert: bool):\n    \"\"\"Determines whether users should be alerted if they answer incorrectly.\n\n    Args:\n        should_alert (bool): Specifies whether users should be alerted for incorrect answers. Defaults to True if not specifically overridden by this method.\n\n    Note:\n        The userScore dimensions which are updated when a user answers a validation task are updated regardless of the value of `should_alert`.\n    \"\"\"\n    with tracer.start_as_current_span(\"RapidataValidationSet.update_should_alert\"):\n        logger.debug(\n            \"Setting shouldAlert for validation set %s to %s\", self.id, should_alert\n        )\n        self._openapi_service.validation_api.validation_set_validation_set_id_patch(\n            self.id, UpdateValidationSetModel(shouldAlert=should_alert)\n        )\n        return self\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet.view","title":"view","text":"<pre><code>view() -&gt; None\n</code></pre> <p>Opens the validation set details page in the browser.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the order is not in processing state.</p> Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def view(self) -&gt; None:\n    \"\"\"\n    Opens the validation set details page in the browser.\n\n    Raises:\n        Exception: If the order is not in processing state.\n    \"\"\"\n    logger.info(\"Opening validation set details page in browser...\")\n    could_open_browser = webbrowser.open(self.validation_set_details_page)\n    if not could_open_browser:\n        encoded_url = urllib.parse.quote(\n            self.validation_set_details_page, safe=\"%/:=&amp;?~#+!$,;'@()*[]\"\n        )\n        managed_print(\n            Fore.RED\n            + f\"Please open this URL in your browser: '{encoded_url}'\"\n            + Fore.RESET\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapidata_validation_set/#rapidata.rapidata_client.validation.rapidata_validation_set.RapidataValidationSet.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the validation set</p> Source code in <code>src/rapidata/rapidata_client/validation/rapidata_validation_set.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"Deletes the validation set\"\"\"\n    with tracer.start_as_current_span(\"RapidataValidationSet.delete\"):\n        logger.info(\"Deleting ValidationSet '%s'\", self)\n        self._openapi_service.validation_api.validation_set_validation_set_id_delete(\n            self.id\n        )\n        logger.debug(\"ValidationSet '%s' has been deleted.\", self)\n        managed_print(f\"ValidationSet '{self}' has been deleted.\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/","title":"Validation set manager","text":""},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager","title":"ValidationSetManager","text":"<pre><code>ValidationSetManager(openapi_service: OpenAPIService)\n</code></pre> <p>Responsible for everything related to validation sets. From creation to retrieval.</p> <p>Attributes:</p> Name Type Description <code>rapid</code> <code>RapidsManager</code> <p>The RapidsManager instance.</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def __init__(self, openapi_service: OpenAPIService) -&gt; None:\n    self.__openapi_service = openapi_service\n    self.rapid = RapidsManager(openapi_service)\n    logger.debug(\"ValidationSetManager initialized\")\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_classification_set","title":"create_classification_set","text":"<pre><code>create_classification_set(\n    name: str,\n    instruction: str,\n    answer_options: list[str],\n    datapoints: list[str],\n    truths: list[list[str]],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanations: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a classification validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction by which the labeler will answer.</p> required <code>answer_options</code> <code>list[str]</code> <p>The options to choose from when answering.</p> required <code>datapoints</code> <code>list[str]</code> <p>The datapoints that will be used for validation.</p> required <code>truths</code> <code>list[list[str]]</code> <p>The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.</p> <p>example:     options: [\"yes\", \"no\", \"maybe\"]     datapoints: [\"datapoint1\", \"datapoint2\"]     truths: [[\"yes\"], [\"no\", \"maybe\"]] -&gt; first datapoint correct answer is \"yes\", second datapoint is \"no\" or \"maybe\"</p> required <code>data_type</code> <code>str</code> <p>The type of data. Defaults to \"media\" (any form of image, video or audio). Other option: \"text\".</p> <code>'media'</code> <code>contexts</code> <code>list[str]</code> <p>The contexts for each datapoint. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction and answer options. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>explanations</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>options: [\"yes\", \"no\", \"maybe\"]\ndatapoints: [\"datapoint1\", \"datapoint2\"]\ntruths: [[\"yes\"], [\"no\", \"maybe\"]]\n</code></pre> This would mean: first datapoint correct answer is \"yes\", second datapoint is \"no\" or \"maybe\"</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_classification_set(\n    self,\n    name: str,\n    instruction: str,\n    answer_options: list[str],\n    datapoints: list[str],\n    truths: list[list[str]],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanations: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a classification validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction by which the labeler will answer.\n        answer_options (list[str]): The options to choose from when answering.\n        datapoints (list[str]): The datapoints that will be used for validation.\n        truths (list[list[str]]): The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.\\n\n            example:\n                options: [\"yes\", \"no\", \"maybe\"]\n                datapoints: [\"datapoint1\", \"datapoint2\"]\n                truths: [[\"yes\"], [\"no\", \"maybe\"]] -&gt; first datapoint correct answer is \"yes\", second datapoint is \"no\" or \"maybe\"\n        data_type (str, optional): The type of data. Defaults to \"media\" (any form of image, video or audio). Other option: \"text\".\n        contexts (list[str], optional): The contexts for each datapoint. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction and answer options. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        explanations (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        options: [\"yes\", \"no\", \"maybe\"]\n        datapoints: [\"datapoint1\", \"datapoint2\"]\n        truths: [[\"yes\"], [\"no\", \"maybe\"]]\n        ```\n        This would mean: first datapoint correct answer is \"yes\", second datapoint is \"no\" or \"maybe\"\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"ValidationSetManager.create_classification_set\"\n    ):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if len(datapoints) != len(truths):\n            raise ValueError(\"The number of datapoints and truths must be equal\")\n\n        if not all([isinstance(truth, (list, tuple)) for truth in truths]):\n            raise ValueError(\"Truths must be a list of lists or tuples\")\n\n        if contexts and len(contexts) != len(datapoints):\n            raise ValueError(\"The number of contexts and datapoints must be equal\")\n\n        if media_contexts and len(media_contexts) != len(datapoints):\n            raise ValueError(\n                \"The number of media contexts and datapoints must be equal\"\n            )\n\n        if explanations and len(explanations) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating classification rapids\")\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.classification_rapid(\n                    instruction=instruction,\n                    answer_options=answer_options,\n                    datapoint=datapoints[i],\n                    truths=truths[i],\n                    data_type=data_type,\n                    context=contexts[i] if contexts != None else None,\n                    media_context=(\n                        media_contexts[i] if media_contexts != None else None\n                    ),\n                    explanation=explanations[i] if explanations != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting classification rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_compare_set","title":"create_compare_set","text":"<pre><code>create_compare_set(\n    name: str,\n    instruction: str,\n    datapoints: list[list[str]],\n    truths: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a comparison validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction to compare against.</p> required <code>truths</code> <code>list[str]</code> <p>The truths for each comparison. List is for each comparison.</p> <p>example:     instruction: \"Which image has a cat?\"     datapoints = [[\"image1.jpg\", \"image2.jpg\"], [\"image3.jpg\", \"image4.jpg\"]]     truths: [\"image1.jpg\", \"image4.jpg\"] -&gt; first comparison image1.jpg has a cat, second comparison image4.jpg has a cat</p> required <code>datapoints</code> <code>list[list[str]]</code> <p>The compare datapoints to create the validation set with. Outer list is for each comparison, inner list the two images/texts that will be compared.</p> required <code>data_type</code> <code>str</code> <p>The type of data. Defaults to \"media\" (any form of image, video or audio). Other option: \"text\".</p> <code>'media'</code> <code>contexts</code> <code>list[str]</code> <p>The contexts for each datapoint. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction and truth. (Therefore will be different for each datapoint) Will be match up with the datapoints using the list index.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>explanation</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>instruction: \"Which image has a cat?\"\ndatapoints = [[\"image1.jpg\", \"image2.jpg\"], [\"image3.jpg\", \"image4.jpg\"]]\ntruths: [\"image1.jpg\", \"image4.jpg\"]\n</code></pre> This would mean: first comparison image1.jpg has a cat, second comparison image4.jpg has a cat</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_compare_set(\n    self,\n    name: str,\n    instruction: str,\n    datapoints: list[list[str]],\n    truths: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a comparison validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction to compare against.\n        truths (list[str]): The truths for each comparison. List is for each comparison.\\n\n            example:\n                instruction: \"Which image has a cat?\"\n                datapoints = [[\"image1.jpg\", \"image2.jpg\"], [\"image3.jpg\", \"image4.jpg\"]]\n                truths: [\"image1.jpg\", \"image4.jpg\"] -&gt; first comparison image1.jpg has a cat, second comparison image4.jpg has a cat\n        datapoints (list[list[str]]): The compare datapoints to create the validation set with.\n            Outer list is for each comparison, inner list the two images/texts that will be compared.\n        data_type (str, optional): The type of data. Defaults to \"media\" (any form of image, video or audio). Other option: \"text\".\n        contexts (list[str], optional): The contexts for each datapoint. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction and truth. (Therefore will be different for each datapoint)\n            Will be match up with the datapoints using the list index.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        explanation (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        instruction: \"Which image has a cat?\"\n        datapoints = [[\"image1.jpg\", \"image2.jpg\"], [\"image3.jpg\", \"image4.jpg\"]]\n        truths: [\"image1.jpg\", \"image4.jpg\"]\n        ```\n        This would mean: first comparison image1.jpg has a cat, second comparison image4.jpg has a cat\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.create_compare_set\"):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if len(datapoints) != len(truths):\n            raise ValueError(\"The number of datapoints and truths must be equal\")\n\n        if not all([isinstance(truth, str) for truth in truths]):\n            raise ValueError(\"Truths must be a list of strings\")\n\n        if contexts and len(contexts) != len(datapoints):\n            raise ValueError(\"The number of contexts and datapoints must be equal\")\n\n        if media_contexts and len(media_contexts) != len(datapoints):\n            raise ValueError(\n                \"The number of media contexts and datapoints must be equal\"\n            )\n\n        if explanation and len(explanation) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating comparison rapids\")\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.compare_rapid(\n                    instruction=instruction,\n                    truth=truths[i],\n                    datapoint=datapoints[i],\n                    data_type=data_type,\n                    context=contexts[i] if contexts != None else None,\n                    media_context=(\n                        media_contexts[i] if media_contexts != None else None\n                    ),\n                    explanation=explanation[i] if explanation != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting comparison rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_select_words_set","title":"create_select_words_set","text":"<pre><code>create_select_words_set(\n    name: str,\n    instruction: str,\n    truths: list[list[int]],\n    datapoints: list[str],\n    sentences: list[str],\n    required_precision: float = 1.0,\n    required_completeness: float = 1.0,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a select words validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction to show to the labeler.</p> required <code>truths</code> <code>list[list[int]]</code> <p>The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.</p> <p>example:     datapoints: [\"datapoint1\", \"datapoint2\"]     sentences: [\"this example 1\", \"this example with another text\"]     truths: [[0, 1], [2]] -&gt; first datapoint correct words are \"this\" and \"example\", second datapoint is \"with\"</p> required <code>datapoints</code> <code>list[str]</code> <p>The datapoints that will be used for validation.</p> required <code>sentences</code> <code>list[str]</code> <p>The sentences that will be used for validation. The sentece will be split up by spaces to be selected by the labeler. Must be the same length as datapoints.</p> required <code>required_precision</code> <code>float</code> <p>The required precision for the labeler to get the rapid correct (minimum ratio of the words selected that need to be correct). Defaults to 1.0 (no wrong word can be selected).</p> <code>1.0</code> <code>required_completeness</code> <code>float</code> <p>The required completeness for the labeler to get the rapid correct (miminum ratio of total correct words selected). Defaults to 1.0 (all correct words need to be selected).</p> <code>1.0</code> <code>explanation</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>datapoints: [\"datapoint1\", \"datapoint2\"]\nsentences: [\"this example 1\", \"this example with another text\"]\ntruths: [[0, 1], [2]]\n</code></pre> This would mean: first datapoint the correct words are \"this\" and \"example\", second datapoint is \"with\"</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_select_words_set(\n    self,\n    name: str,\n    instruction: str,\n    truths: list[list[int]],\n    datapoints: list[str],\n    sentences: list[str],\n    required_precision: float = 1.0,\n    required_completeness: float = 1.0,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a select words validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction to show to the labeler.\n        truths (list[list[int]]): The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.\\n\n            example:\n                datapoints: [\"datapoint1\", \"datapoint2\"]\n                sentences: [\"this example 1\", \"this example with another text\"]\n                truths: [[0, 1], [2]] -&gt; first datapoint correct words are \"this\" and \"example\", second datapoint is \"with\"\n        datapoints (list[str]): The datapoints that will be used for validation.\n        sentences (list[str]): The sentences that will be used for validation. The sentece will be split up by spaces to be selected by the labeler.\n            Must be the same length as datapoints.\n        required_precision (float, optional): The required precision for the labeler to get the rapid correct (minimum ratio of the words selected that need to be correct). Defaults to 1.0 (no wrong word can be selected).\n        required_completeness (float, optional): The required completeness for the labeler to get the rapid correct (miminum ratio of total correct words selected). Defaults to 1.0 (all correct words need to be selected).\n        explanation (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        datapoints: [\"datapoint1\", \"datapoint2\"]\n        sentences: [\"this example 1\", \"this example with another text\"]\n        truths: [[0, 1], [2]]\n        ```\n        This would mean: first datapoint the correct words are \"this\" and \"example\", second datapoint is \"with\"\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"ValidationSetManager.create_select_words_set\"\n    ):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if not all([isinstance(truth, (list, tuple)) for truth in truths]):\n            raise ValueError(\"Truths must be a list of lists or tuples\")\n\n        if len(datapoints) != len(truths) or len(datapoints) != len(sentences):\n            raise ValueError(\n                \"The number of datapoints, truths, and sentences must be equal\"\n            )\n\n        if explanation and len(explanation) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating select words rapids\")\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.select_words_rapid(\n                    instruction=instruction,\n                    truths=truths[i],\n                    datapoint=datapoints[i],\n                    sentence=sentences[i],\n                    required_precision=required_precision,\n                    required_completeness=required_completeness,\n                    explanation=explanation[i] if explanation != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting select words rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_locate_set","title":"create_locate_set","text":"<pre><code>create_locate_set(\n    name: str,\n    instruction: str,\n    truths: list[list[Box]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a locate validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction to show to the labeler.</p> required <code>truths</code> <code>list[list[Box]]</code> <p>The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.</p> <p>example:     datapoints: [\"datapoint1\", \"datapoint2\"]     truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]] -&gt; first datapoint the object is in the top left corner, second datapoint the object is in the center</p> required <code>datapoints</code> <code>list[str]</code> <p>The datapoints that will be used for validation.</p> required <code>contexts</code> <code>list[str]</code> <p>The contexts for each datapoint. Defaults to None.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>explanation</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>datapoints: [\"datapoint1\", \"datapoint2\"]\ntruths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]]\n</code></pre> This would mean: first datapoint the object is in the top left corner, second datapoint the object is in the center</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_locate_set(\n    self,\n    name: str,\n    instruction: str,\n    truths: list[list[Box]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a locate validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction to show to the labeler.\n        truths (list[list[Box]]): The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.\\n\n            example:\n                datapoints: [\"datapoint1\", \"datapoint2\"]\n                truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]] -&gt; first datapoint the object is in the top left corner, second datapoint the object is in the center\n        datapoints (list[str]): The datapoints that will be used for validation.\n        contexts (list[str], optional): The contexts for each datapoint. Defaults to None.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        explanation (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        datapoints: [\"datapoint1\", \"datapoint2\"]\n        truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]]\n        ```\n        This would mean: first datapoint the object is in the top left corner, second datapoint the object is in the center\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.create_locate_set\"):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if len(datapoints) != len(truths):\n            raise ValueError(\"The number of datapoints and truths must be equal\")\n\n        if not all([isinstance(truth, (list, tuple)) for truth in truths]):\n            raise ValueError(\"Truths must be a list of lists or tuples\")\n\n        if contexts and len(contexts) != len(datapoints):\n            raise ValueError(\"The number of contexts and datapoints must be equal\")\n\n        if media_contexts and len(media_contexts) != len(datapoints):\n            raise ValueError(\n                \"The number of media contexts and datapoints must be equal\"\n            )\n\n        if explanation and len(explanation) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating locate rapids\")\n        rapids = []\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.locate_rapid(\n                    instruction=instruction,\n                    truths=truths[i],\n                    datapoint=datapoints[i],\n                    context=contexts[i] if contexts != None else None,\n                    media_context=(\n                        media_contexts[i] if media_contexts != None else None\n                    ),\n                    explanation=explanation[i] if explanation != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting locate rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_draw_set","title":"create_draw_set","text":"<pre><code>create_draw_set(\n    name: str,\n    instruction: str,\n    truths: list[list[Box]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a draw validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction to show to the labeler.</p> required <code>truths</code> <code>list[list[Box]]</code> <p>The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.</p> <p>example:     datapoints: [\"datapoint1\", \"datapoint2\"]     truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]] -&gt; first datapoint the object is in the top left corner, second datapoint the object is in the center</p> required <code>datapoints</code> <code>list[str]</code> <p>The datapoints that will be used for validation.</p> required <code>contexts</code> <code>list[str]</code> <p>The contexts for each datapoint. Defaults to None.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>explanation</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>datapoints: [\"datapoint1\", \"datapoint2\"]\ntruths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]]\n</code></pre> This would mean: first datapoint the object is in the top left corner, second datapoint the object is in the center</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_draw_set(\n    self,\n    name: str,\n    instruction: str,\n    truths: list[list[Box]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a draw validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction to show to the labeler.\n        truths (list[list[Box]]): The truths for each datapoint. Outer list is for each datapoint, inner list is for each truth.\\n\n            example:\n                datapoints: [\"datapoint1\", \"datapoint2\"]\n                truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]] -&gt; first datapoint the object is in the top left corner, second datapoint the object is in the center\n        datapoints (list[str]): The datapoints that will be used for validation.\n        contexts (list[str], optional): The contexts for each datapoint. Defaults to None.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        explanation (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        datapoints: [\"datapoint1\", \"datapoint2\"]\n        truths: [[Box(0, 0, 100, 100)], [Box(50, 50, 150, 150)]]\n        ```\n        This would mean: first datapoint the object is in the top left corner, second datapoint the object is in the center\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.create_draw_set\"):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if len(datapoints) != len(truths):\n            raise ValueError(\"The number of datapoints and truths must be equal\")\n\n        if not all([isinstance(truth, (list, tuple)) for truth in truths]):\n            raise ValueError(\"Truths must be a list of lists or tuples\")\n\n        if contexts and len(contexts) != len(datapoints):\n            raise ValueError(\"The number of contexts and datapoints must be equal\")\n\n        if media_contexts and len(media_contexts) != len(datapoints):\n            raise ValueError(\n                \"The number of media contexts and datapoints must be equal\"\n            )\n\n        if explanation and len(explanation) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating draw rapids\")\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.draw_rapid(\n                    instruction=instruction,\n                    truths=truths[i],\n                    datapoint=datapoints[i],\n                    context=contexts[i] if contexts != None else None,\n                    media_context=(\n                        media_contexts[i] if media_contexts != None else None\n                    ),\n                    explanation=explanation[i] if explanation != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting draw rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_timestamp_set","title":"create_timestamp_set","text":"<pre><code>create_timestamp_set(\n    name: str,\n    instruction: str,\n    truths: list[list[tuple[int, int]]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a timestamp validation set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>instruction</code> <code>str</code> <p>The instruction to show to the labeler.</p> required <code>truths</code> <code>list[list[tuple[int, int]]]</code> <p>The truths for each datapoint defined as start and endpoint based on miliseconds. Outer list is for each datapoint, inner list is for each truth.</p> <p>example:     datapoints: [\"datapoint1\", \"datapoint2\"]     truths: [[(0, 10)], [(20, 30)]] -&gt; first datapoint the correct interval is from 0 to 10, second datapoint the correct interval is from 20 to 30</p> required <code>datapoints</code> <code>list[str]</code> <p>The datapoints that will be used for validation.</p> required <code>contexts</code> <code>list[str]</code> <p>The contexts for each datapoint. Defaults to None.</p> <code>None</code> <code>media_contexts</code> <code>list[str]</code> <p>The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.</p> <p>If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint) Will be matched up with the datapoints using the list index.</p> <code>None</code> <code>explanation</code> <code>list[str | None]</code> <p>The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.</p> <code>None</code> <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Example <p><pre><code>datapoints: [\"datapoint1\", \"datapoint2\"]\ntruths: [[(0, 10)], [(20, 30)]]\n</code></pre> This would mean: first datapoint the correct interval is from 0 to 10, second datapoint the correct interval is from 20 to 30</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_timestamp_set(\n    self,\n    name: str,\n    instruction: str,\n    truths: list[list[tuple[int, int]]],\n    datapoints: list[str],\n    contexts: list[str] | None = None,\n    media_contexts: list[str] | None = None,\n    explanation: list[str | None] | None = None,\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a timestamp validation set.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        instruction (str): The instruction to show to the labeler.\n        truths (list[list[tuple[int, int]]]): The truths for each datapoint defined as start and endpoint based on miliseconds.\n            Outer list is for each datapoint, inner list is for each truth.\\n\n            example:\n                datapoints: [\"datapoint1\", \"datapoint2\"]\n                truths: [[(0, 10)], [(20, 30)]] -&gt; first datapoint the correct interval is from 0 to 10, second datapoint the correct interval is from 20 to 30\n        datapoints (list[str]): The datapoints that will be used for validation.\n        contexts (list[str], optional): The contexts for each datapoint. Defaults to None.\n        media_contexts (list[str], optional): The list of media contexts i.e. links to the images / videos for the comparison. Defaults to None.\\n\n            If provided has to be the same length as datapoints and will be shown in addition to the instruction. (Therefore will be different for each datapoint)\n            Will be matched up with the datapoints using the list index.\n        explanation (list[str | None], optional): The explanations for each datapoint. Will be given to the annotators in case the answer is wrong. Defaults to None.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n\n    Example:\n        ```python\n        datapoints: [\"datapoint1\", \"datapoint2\"]\n        truths: [[(0, 10)], [(20, 30)]]\n        ```\n        This would mean: first datapoint the correct interval is from 0 to 10, second datapoint the correct interval is from 20 to 30\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.create_timestamp_set\"):\n        if not datapoints:\n            raise ValueError(\"Datapoints cannot be empty\")\n\n        if len(datapoints) != len(truths):\n            raise ValueError(\"The number of datapoints and truths must be equal\")\n\n        if not all([isinstance(truth, (list, tuple)) for truth in truths]):\n            raise ValueError(\"Truths must be a list of lists or tuples\")\n\n        if contexts and len(contexts) != len(datapoints):\n            raise ValueError(\"The number of contexts and datapoints must be equal\")\n\n        if media_contexts and len(media_contexts) != len(datapoints):\n            raise ValueError(\n                \"The number of media contexts and datapoints must be equal\"\n            )\n\n        if explanation and len(explanation) != len(datapoints):\n            raise ValueError(\n                \"The number of explanations and datapoints must be equal, the index must align, but can be padded with None\"\n            )\n\n        logger.debug(\"Creating timestamp rapids\")\n        rapids: list[Rapid] = []\n        for i in range(len(datapoints)):\n            rapids.append(\n                self.rapid.timestamp_rapid(\n                    instruction=instruction,\n                    truths=truths[i],\n                    datapoint=datapoints[i],\n                    context=contexts[i] if contexts != None else None,\n                    media_context=(\n                        media_contexts[i] if media_contexts != None else None\n                    ),\n                    explanation=explanation[i] if explanation != None else None,\n                )\n            )\n\n        logger.debug(\"Submitting timestamp rapids\")\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.create_mixed_set","title":"create_mixed_set","text":"<pre><code>create_mixed_set(\n    name: str,\n    rapids: list[Rapid],\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet\n</code></pre> <p>Create a validation set with a list of rapids.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the validation set. (will not be shown to the labeler)</p> required <code>rapids</code> <code>list[Rapid]</code> <p>The list of rapids to add to the validation set.</p> required <code>dimensions</code> <code>list[str]</code> <p>The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.</p> <code>[]</code> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def create_mixed_set(\n    self,\n    name: str,\n    rapids: list[Rapid],\n    dimensions: list[str] = [],\n) -&gt; RapidataValidationSet:\n    \"\"\"Create a validation set with a list of rapids.\n\n    Args:\n        name (str): The name of the validation set. (will not be shown to the labeler)\n        rapids (list[Rapid]): The list of rapids to add to the validation set.\n        dimensions (list[str], optional): The dimensions to add to the validation set accross which users will be tracked. Defaults to [] which is the default dimension.\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.create_mixed_set\"):\n        if not rapids:\n            raise ValueError(\"Rapids cannot be empty\")\n\n        return self._submit(name=name, rapids=rapids, dimensions=dimensions)\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.get_validation_set_by_id","title":"get_validation_set_by_id","text":"<pre><code>get_validation_set_by_id(\n    validation_set_id: str,\n) -&gt; RapidataValidationSet\n</code></pre> <p>Get a validation set by ID.</p> <p>Parameters:</p> Name Type Description Default <code>validation_set_id</code> <code>str</code> <p>The ID of the validation set.</p> required <p>Returns:</p> Name Type Description <code>RapidataValidationSet</code> <code>RapidataValidationSet</code> <p>The ValidationSet instance.</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def get_validation_set_by_id(self, validation_set_id: str) -&gt; RapidataValidationSet:\n    \"\"\"Get a validation set by ID.\n\n    Args:\n        validation_set_id (str): The ID of the validation set.\n\n    Returns:\n        RapidataValidationSet: The ValidationSet instance.\n    \"\"\"\n\n    with tracer.start_as_current_span(\n        \"ValidationSetManager.get_validation_set_by_id\"\n    ):\n        validation_set = self.__openapi_service.validation_api.validation_set_validation_set_id_get(\n            validation_set_id=validation_set_id\n        )\n\n        return RapidataValidationSet(\n            validation_set_id, str(validation_set.name), self.__openapi_service\n        )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/validation_set_manager/#rapidata.rapidata_client.validation.validation_set_manager.ValidationSetManager.find_validation_sets","title":"find_validation_sets","text":"<pre><code>find_validation_sets(\n    name: str = \"\", amount: int = 10\n) -&gt; list[RapidataValidationSet]\n</code></pre> <p>Find validation sets by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to search for. Defaults to \"\" to match with any set.</p> <code>''</code> <code>amount</code> <code>int</code> <p>The amount of validation sets to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[RapidataValidationSet]</code> <p>list[RapidataValidationSet]: The list of validation sets.</p> Source code in <code>src/rapidata/rapidata_client/validation/validation_set_manager.py</code> <pre><code>def find_validation_sets(\n    self, name: str = \"\", amount: int = 10\n) -&gt; list[RapidataValidationSet]:\n    \"\"\"Find validation sets by name.\n\n    Args:\n        name (str, optional): The name to search for. Defaults to \"\" to match with any set.\n        amount (int, optional): The amount of validation sets to return. Defaults to 10.\n\n    Returns:\n        list[RapidataValidationSet]: The list of validation sets.\n    \"\"\"\n    with tracer.start_as_current_span(\"ValidationSetManager.find_validation_sets\"):\n\n        validation_page_result = (\n            self.__openapi_service.validation_api.validation_sets_get(\n                QueryModel(\n                    page=PageInfo(index=1, size=amount),\n                    filter=RootFilter(\n                        filters=[\n                            Filter(\n                                field=\"Name\",\n                                operator=FilterOperator.CONTAINS,\n                                value=name,\n                            )\n                        ]\n                    ),\n                    sortCriteria=[\n                        SortCriterion(\n                            direction=SortDirection.DESC, propertyName=\"CreatedAt\"\n                        )\n                    ],\n                )\n            )\n        )\n\n        validation_sets = [\n            self.get_validation_set_by_id(str(validation_set.id))\n            for validation_set in validation_page_result.items\n        ]\n        return validation_sets\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/box/","title":"Box","text":""},{"location":"reference/rapidata/rapidata_client/validation/rapids/box/#rapidata.rapidata_client.validation.rapids.box.Box","title":"Box","text":"<p>               Bases: <code>BaseModel</code></p> <p>Used in the Locate and Draw Validation sets. All coordinates are in ratio of the image size (0.0 to 1.0).</p> <p>Parameters:</p> Name Type Description Default <code>x_min</code> <code>float</code> <p>The minimum x value of the box in ratio of the image size.</p> required <code>y_min</code> <code>float</code> <p>The minimum y value of the box in ratio of the image size.</p> required <code>x_max</code> <code>float</code> <p>The maximum x value of the box in ratio of the image size.</p> required <code>y_max</code> <code>float</code> <p>The maximum y value of the box in ratio of the image size.</p> required"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids/","title":"Rapids","text":""},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/","title":"Rapids manager","text":""},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager","title":"RapidsManager","text":"<pre><code>RapidsManager(openapi_service: OpenAPIService)\n</code></pre> <p>Can be used to build different types of rapids. That can then be added to Validation sets</p> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def __init__(self, openapi_service: OpenAPIService):\n    self._openapi_service = openapi_service\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.classification_rapid","title":"classification_rapid","text":"<pre><code>classification_rapid(\n    instruction: str,\n    answer_options: list[str],\n    datapoint: str,\n    truths: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a classification rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction/question to be shown to the labeler.</p> required <code>answer_options</code> <code>list[str]</code> <p>The options that the labeler can choose from to answer the question.</p> required <code>datapoint</code> <code>str</code> <p>The datapoint that the labeler will be labeling.</p> required <code>truths</code> <code>list[str]</code> <p>The correct answers to the question.</p> required <code>data_type</code> <code>str</code> <p>The type of the datapoint. Defaults to \"media\" (any form of image, video or audio).</p> <code>'media'</code> <code>context</code> <code>str</code> <p>The context is text that will be shown in addition to the instruction. Defaults to None.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.</p> <code>None</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def classification_rapid(\n    self,\n    instruction: str,\n    answer_options: list[str],\n    datapoint: str,\n    truths: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a classification rapid\n\n    Args:\n        instruction (str): The instruction/question to be shown to the labeler.\n        answer_options (list[str]): The options that the labeler can choose from to answer the question.\n        datapoint (str): The datapoint that the labeler will be labeling.\n        truths (list[str]): The correct answers to the question.\n        data_type (str, optional): The type of the datapoint. Defaults to \"media\" (any form of image, video or audio).\n        context (str, optional): The context is text that will be shown in addition to the instruction. Defaults to None.\n        media_context (str, optional): The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n    if not isinstance(truths, list):\n        raise ValueError(\"Truths must be a list of strings\")\n\n    if not all(truth in answer_options for truth in truths):\n        raise ValueError(\"Truths must be part of the answer options\")\n\n    payload = ClassifyPayload(\n        _t=\"ClassifyPayload\", possibleCategories=answer_options, title=instruction\n    )\n    model_truth = AttachCategoryTruth(\n        correctCategories=truths, _t=\"AttachCategoryTruth\"\n    )\n\n    return Rapid(\n        asset=datapoint,\n        data_type=data_type,\n        context=context,\n        media_context=media_context,\n        explanation=explanation,\n        payload=payload,\n        truth=model_truth,\n        random_correct_probability=len(truths) / len(answer_options),\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.compare_rapid","title":"compare_rapid","text":"<pre><code>compare_rapid(\n    instruction: str,\n    truth: str,\n    datapoint: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a compare rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction that the labeler will be comparing the assets on.</p> required <code>truth</code> <code>str</code> <p>The correct answer to the comparison. (has to be one of the assets)</p> required <code>datapoint</code> <code>list[str]</code> <p>The two assets that the labeler will be comparing.</p> required <code>data_type</code> <code>str</code> <p>The type of the datapoint. Defaults to \"media\" (any form of image, video or audio).</p> <code>'media'</code> <code>context</code> <code>str</code> <p>The context is text that will be shown in addition to the instruction. Defaults to None.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.</p> <code>None</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def compare_rapid(\n    self,\n    instruction: str,\n    truth: str,\n    datapoint: list[str],\n    data_type: Literal[\"media\", \"text\"] = \"media\",\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a compare rapid\n\n    Args:\n        instruction (str): The instruction that the labeler will be comparing the assets on.\n        truth (str): The correct answer to the comparison. (has to be one of the assets)\n        datapoint (list[str]): The two assets that the labeler will be comparing.\n        data_type (str, optional): The type of the datapoint. Defaults to \"media\" (any form of image, video or audio).\n        context (str, optional): The context is text that will be shown in addition to the instruction. Defaults to None.\n        media_context (str, optional): The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n\n    payload = ComparePayload(_t=\"ComparePayload\", criteria=instruction)\n    truth = os.path.basename(truth)\n    model_truth = CompareTruth(_t=\"CompareTruth\", winnerId=truth)\n\n    if len(datapoint) != 2:\n        raise ValueError(\"Compare rapid requires exactly two media paths\")\n\n    return Rapid(\n        asset=datapoint,\n        data_type=data_type,\n        truth=model_truth,\n        context=context,\n        media_context=media_context,\n        payload=payload,\n        explanation=explanation,\n        random_correct_probability=0.5,\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.select_words_rapid","title":"select_words_rapid","text":"<pre><code>select_words_rapid(\n    instruction: str,\n    truths: list[int],\n    datapoint: str,\n    sentence: str,\n    required_precision: float = 1,\n    required_completeness: float = 1,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a select words rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction for the labeler.</p> required <code>truths</code> <code>list[int]</code> <p>The indices of the words that are the correct answers.</p> required <code>datapoint</code> <code>str</code> <p>The asset that the labeler will be selecting words from.</p> required <code>sentence</code> <code>str</code> <p>The sentence that the labeler will be selecting words from. (split up by spaces)</p> required <code>required_precision</code> <code>float</code> <p>The required precision for the labeler to get the rapid correct (minimum ratio of the words selected that need to be correct). defaults to 1. (no wrong words can be selected)</p> <code>1</code> <code>required_completeness</code> <code>float</code> <p>The required completeness for the labeler to get the rapid correct (miminum ratio of total correct words selected). defaults to 1. (all correct words need to be selected)</p> <code>1</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def select_words_rapid(\n    self,\n    instruction: str,\n    truths: list[int],\n    datapoint: str,\n    sentence: str,\n    required_precision: float = 1,\n    required_completeness: float = 1,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a select words rapid\n\n    Args:\n        instruction (str): The instruction for the labeler.\n        truths (list[int]): The indices of the words that are the correct answers.\n        datapoint (str): The asset that the labeler will be selecting words from.\n        sentence (str): The sentence that the labeler will be selecting words from. (split up by spaces)\n        required_precision (float): The required precision for the labeler to get the rapid correct (minimum ratio of the words selected that need to be correct). defaults to 1. (no wrong words can be selected)\n        required_completeness (float): The required completeness for the labeler to get the rapid correct (miminum ratio of total correct words selected). defaults to 1. (all correct words need to be selected)\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n\n    transcription_words = [\n        TranscriptionWord(word=word, wordIndex=i)\n        for i, word in enumerate(sentence.split(\" \"))\n    ]\n\n    correct_transcription_words: list[TranscriptionWord] = []\n    for index in truths:\n        correct_transcription_words.append(\n            TranscriptionWord(word=transcription_words[index].word, wordIndex=index)\n        )\n\n    payload = TranscriptionPayload(\n        _t=\"TranscriptionPayload\",\n        title=instruction,\n        transcription=transcription_words,\n    )\n\n    model_truth = TranscriptionTruth(\n        _t=\"TranscriptionTruth\",\n        correctWords=correct_transcription_words,\n        requiredPrecision=required_precision,\n        requiredCompleteness=required_completeness,\n    )\n\n    return Rapid(\n        payload=payload,\n        truth=model_truth,\n        asset=datapoint,\n        sentence=sentence,\n        explanation=explanation,\n        random_correct_probability=len(correct_transcription_words)\n        / len(transcription_words),\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.locate_rapid","title":"locate_rapid","text":"<pre><code>locate_rapid(\n    instruction: str,\n    truths: list[Box],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a locate rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction on what the labeler should do.</p> required <code>truths</code> <code>list[Box]</code> <p>The bounding boxes of the object that the labeler ought to be locating.</p> required <code>datapoint</code> <code>str</code> <p>The asset that the labeler will be locating the object in.</p> required <code>context</code> <code>str</code> <p>The context is text that will be shown in addition to the instruction. Defaults to None.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.</p> <code>None</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def locate_rapid(\n    self,\n    instruction: str,\n    truths: list[Box],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a locate rapid\n\n    Args:\n        instruction (str): The instruction on what the labeler should do.\n        truths (list[Box]): The bounding boxes of the object that the labeler ought to be locating.\n        datapoint (str): The asset that the labeler will be locating the object in.\n        context (str, optional): The context is text that will be shown in addition to the instruction. Defaults to None.\n        media_context (str, optional): The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n\n    payload = LocatePayload(_t=\"LocatePayload\", target=instruction)\n\n    model_truth = LocateBoxTruth(\n        _t=\"LocateBoxTruth\",\n        boundingBoxes=[truth.to_model() for truth in truths],\n    )\n\n    coverage = self._calculate_boxes_coverage(\n        truths,\n    )\n\n    return Rapid(\n        payload=payload,\n        truth=model_truth,\n        asset=datapoint,\n        context=context,\n        media_context=media_context,\n        explanation=explanation,\n        random_correct_probability=coverage,\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.draw_rapid","title":"draw_rapid","text":"<pre><code>draw_rapid(\n    instruction: str,\n    truths: list[Box],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a draw rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instructions on what the labeler</p> required <code>truths</code> <code>list[Box]</code> <p>The bounding boxes of the object that the labeler ought to be drawing.</p> required <code>datapoint</code> <code>str</code> <p>The asset that the labeler will be drawing the object in.</p> required <code>context</code> <code>str</code> <p>The context is text that will be shown in addition to the instruction. Defaults to None.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.</p> <code>None</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def draw_rapid(\n    self,\n    instruction: str,\n    truths: list[Box],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a draw rapid\n\n    Args:\n        instruction (str): The instructions on what the labeler\n        truths (list[Box]): The bounding boxes of the object that the labeler ought to be drawing.\n        datapoint (str): The asset that the labeler will be drawing the object in.\n        context (str, optional): The context is text that will be shown in addition to the instruction. Defaults to None.\n        media_context (str, optional): The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n\n    payload = LinePayload(_t=\"LinePayload\", target=instruction)\n\n    model_truth = BoundingBoxTruth(\n        _t=\"BoundingBoxTruth\",\n        xMax=truths[0].x_max * 100,\n        xMin=truths[0].x_min * 100,\n        yMax=truths[0].y_max * 100,\n        yMin=truths[0].y_min * 100,\n    )\n\n    coverage = self._calculate_boxes_coverage(\n        truths,\n    )\n\n    return Rapid(\n        payload=payload,\n        truth=model_truth,\n        asset=datapoint,\n        context=context,\n        media_context=media_context,\n        explanation=explanation,\n        random_correct_probability=coverage,\n    )\n</code></pre>"},{"location":"reference/rapidata/rapidata_client/validation/rapids/rapids_manager/#rapidata.rapidata_client.validation.rapids.rapids_manager.RapidsManager.timestamp_rapid","title":"timestamp_rapid","text":"<pre><code>timestamp_rapid(\n    instruction: str,\n    truths: list[tuple[int, int]],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid\n</code></pre> <p>Build a timestamp rapid</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction for the labeler.</p> required <code>truths</code> <code>list[tuple[int, int]]</code> <p>The possible accepted timestamps intervals for the labeler (in miliseconds). The first element of the tuple is the start of the interval and the second element is the end of the interval.</p> required <code>datapoint</code> <code>str</code> <p>The asset that the labeler will be timestamping.</p> required <code>context</code> <code>str</code> <p>The context is text that will be shown in addition to the instruction. Defaults to None.</p> <code>None</code> <code>media_context</code> <code>str</code> <p>The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.</p> <code>None</code> <code>explanation</code> <code>str</code> <p>The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.</p> <code>None</code> Source code in <code>src/rapidata/rapidata_client/validation/rapids/rapids_manager.py</code> <pre><code>def timestamp_rapid(\n    self,\n    instruction: str,\n    truths: list[tuple[int, int]],\n    datapoint: str,\n    context: str | None = None,\n    media_context: str | None = None,\n    explanation: str | None = None,\n) -&gt; Rapid:\n    \"\"\"Build a timestamp rapid\n\n    Args:\n        instruction (str): The instruction for the labeler.\n        truths (list[tuple[int, int]]): The possible accepted timestamps intervals for the labeler (in miliseconds).\n            The first element of the tuple is the start of the interval and the second element is the end of the interval.\n        datapoint (str): The asset that the labeler will be timestamping.\n        context (str, optional): The context is text that will be shown in addition to the instruction. Defaults to None.\n        media_context (str, optional): The media context is a link to an image / video that will be shown in addition to the instruction (can be combined with context). Defaults to None.\n        explanation (str, optional): The explanation that will be shown to the labeler if the answer is wrong. Defaults to None.\n    \"\"\"\n\n    for truth in truths:\n        if len(truth) != 2:\n            raise ValueError(\n                \"The truths per datapoint must be a tuple of exactly two integers.\"\n            )\n        if truth[0] &gt; truth[1]:\n            raise ValueError(\n                \"The start of the interval must be smaller than the end of the interval.\"\n            )\n\n    payload = ScrubPayload(_t=\"ScrubPayload\", target=instruction)\n\n    model_truth = ScrubTruth(\n        _t=\"ScrubTruth\",\n        validRanges=[ScrubRange(start=truth[0], end=truth[1]) for truth in truths],\n    )\n\n    return Rapid(\n        payload=payload,\n        truth=model_truth,\n        asset=datapoint,\n        context=context,\n        media_context=media_context,\n        explanation=explanation,\n        random_correct_probability=0.5,  # TODO: implement coverage ratio\n    )\n</code></pre>"}]}